[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Cheng-Yuan Wu",
    "section": "",
    "text": "Hello, I’m Cheng-Yuan Wu.\nWelcome to my personal website! I’m currently studying at UC San Diego and working on cool projects in data, analytics, and business strategy.\nYou can find my resume here or connect with me on LinkedIn."
  },
  {
    "objectID": "index.html#introduction",
    "href": "index.html#introduction",
    "title": "Cheng-Yuan Wu",
    "section": "",
    "text": "Hello, I’m Cheng-Yuan Wu.\nWelcome to my personal website! I’m currently studying at UC San Diego and working on cool projects in data, analytics, and business strategy.\nYou can find my resume here or connect with me on LinkedIn."
  },
  {
    "objectID": "Resume.html",
    "href": "Resume.html",
    "title": "Resume",
    "section": "",
    "text": "Download PDF file."
  },
  {
    "objectID": "hw1_questions.html",
    "href": "hw1_questions.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nThe study found that announcing a matching grant significantly increased both the likelihood of donation and the average amount given. However, increasing the match ratio—such as from 1:1 to 3:1—did not yield any additional gains in giving. Interestingly, the effectiveness of matching grants also varied by political geography: donors in Republican-leaning (“red”) states were more responsive than those in Democratic-leaning (“blue”) states. These results challenge conventional fundraising wisdom and offer practical insights for designing more effective donation campaigns.\nThis project aims to replicate their findings."
  },
  {
    "objectID": "hw1_questions.html#introduction",
    "href": "hw1_questions.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nThe study found that announcing a matching grant significantly increased both the likelihood of donation and the average amount given. However, increasing the match ratio—such as from 1:1 to 3:1—did not yield any additional gains in giving. Interestingly, the effectiveness of matching grants also varied by political geography: donors in Republican-leaning (“red”) states were more responsive than those in Democratic-leaning (“blue”) states. These results challenge conventional fundraising wisdom and offer practical insights for designing more effective donation campaigns.\nThis project aims to replicate their findings."
  },
  {
    "objectID": "hw1_questions.html#data",
    "href": "hw1_questions.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nData Description\nThis dataset originates from the field experiment conducted by Karlan and List (2007), in which 50,083 prior donors were sent fundraising letters randomly assigned to different treatments. Each row represents one individual who received a solicitation letter.\n\n\nDataset Overview\n\nTotal observations: 50,083\nUnit of observation: Individual donor\nDesign: Randomized field experiment with control and treatment groups\n\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\nKey Variables Summary\n\n\n\n\n\n\n\n\nVariable\nMean\nDescription\n\n\n\n\ntreatment\n0.667\nReceived a treatment letter (matching/challenge)\n\n\ncontrol\n0.333\nReceived a standard control letter\n\n\ngave\n0.021\nDonated (binary: 1 if donated, 0 otherwise)\n\n\namount\n0.916\nDonation amount (including 0s for non-donors)\n\n\namountchange\n-52.67\nChange in amount given compared to previous donations\n\n\nhpa\n59.38\nHighest previous contribution\n\n\nltmedmra\n0.494\nIndicator for small prior donors (last gift &lt; $35)\n\n\nyear5\n0.509\nDonor for at least 5 years\n\n\ndormant\n0.523\nAlready donated in 2005\n\n\nfemale\n0.278\nFemale donor\n\n\ncouple\n0.092\nCouple donor\n\n\n\n\n\nTreatment Group Assignment\n\n\n\nVariable\nMean\nDescription\n\n\n\n\nratio2\n0.222\nReceived 2:1 matching treatment\n\n\nratio3\n0.222\nReceived 3:1 matching treatment\n\n\nsize25\n0.167\n$25,000 match threshold\n\n\nsize50\n0.167\n$50,000 match threshold\n\n\nsize100\n0.167\n$100,000 match threshold\n\n\nsizeno\n0.167\nUnstated match threshold\n\n\naskd1\n0.222\nSuggested amount = last donation\n\n\naskd2\n0.222\nSuggested amount = 1.25x last donation\n\n\naskd3\n0.222\nSuggested amount = 1.5x last donation\n\n\n\n\n\nPolitical and Geographic Context\n\n\n\n\n\n\n\n\nVariable\nMean\nDescription\n\n\n\n\nred0\n0.404\nLives in a Republican-leaning (red) state\n\n\nblue0\n0.596\nLives in a Democratic-leaning (blue) state\n\n\nredcty\n0.510\nLives in a red county\n\n\nperbush\n0.489\nState-level vote share for Bush (2004)\n\n\n\n\n\nZip Code Demographics\n\n\n\n\n\n\n\n\nVariable\nMean\nDescription\n\n\n\n\npwhite\n0.820\nProportion of white residents\n\n\npblack\n0.087\nProportion of Black residents\n\n\npage18_39\n0.322\nProportion aged 18–39\n\n\nmedian_hhincome\n$54,816\nMedian household income\n\n\npowner\n0.669\nProportion of homeowners\n\n\npsch_atlstba\n0.392\nProportion with at least a bachelor’s degree\n\n\n\n\n\n\n\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\n\n\nT-test vs. Linear Regression\nTo verify whether the random assignment was successful, we tested a few non-outcome variables to see if treatment and control groups differ significantly at the 95% confidence level. This helps us confirm whether any observed treatment effects later can be attributed to the intervention itself, not to pre-existing group differences.\nWe tested four variables: - mrm2 (months since last donation) - freq (number of prior donations) - amountchange (change in donation amount) - female (gender)\n\n\n\nHypothesis\nFor each variable, we test:\n\nNull hypothesis (H₀): There is no difference in means between treatment and control groups.\nAlternative hypothesis (H₁): There is a difference in means between the groups.\n\n\n\n\nT-test Results\n\n\n\n\n\n\n\n\n\n\n\nFeature\nMean\nStd\nT-statistic\nP-value\n95% Confidence Interval\n\n\n\n\nmrm2\n13.007\n12.081\n0.119\n0.905\n(-0.211, 0.238)\n\n\nfreq\n8.039\n11.394\n-0.111\n0.912\n(-0.224, 0.200)\n\n\namountchange\n-52.672\n1267.239\n0.527\n0.598\n(-17.216, 29.877)\n\n\nfemale\n0.278\n0.448\n-1.758\n0.079\n(-0.016, 0.001)\n\n\n\n\nAll p-values are above 0.05, so we fail to reject the null hypothesis. There is no statistically significant difference between the treatment and control groups for any of these variables, indicating that random assignment was successful.\n\n\n\n\nT-test Code\n# For 'mrm2' as an example\nimport pandas as pd\nimport numpy as np\nfrom scipy import stats\n\ndata = pd.read_stata(\"karlan_list_2007.dta\")\ndf_treatment = data[data['treatment'] == 1]['mrm2'].dropna()\ndf_control = data[data['treatment'] == 0]['mrm2'].dropna()\n\nt_stat, p_val = stats.ttest_ind(df_treatment, df_control, equal_var=True)\n\nprint(f\"T-statistic: {t_stat:.3f}\")\nprint(f\"P-value: {p_val:.4f}\")\n\nmean_diff = df_treatment.mean() - df_control.mean()\nn1, n2 = len(df_treatment), len(df_control)\ns1, s2 = df_treatment.std(ddof=1), df_control.std(ddof=1)\n\nsp = np.sqrt(((n1 - 1)*s1**2 + (n2 - 1)*s2**2) / (n1 + n2 - 2))\nse = sp * np.sqrt(1/n1 + 1/n2)\n\nt_crit = stats.t.ppf(0.975, df=n1 + n2 - 2)\nci_low = mean_diff - t_crit * se\nci_high = mean_diff + t_crit * se\n\nprint(f\"\\nMean Difference: {mean_diff:.3f}\")\nprint(f\"95% Confidence Interval: ({ci_low:.3f}, {ci_high:.3f})\")\n\n\nLinear Regression Validation\nTo confirm the t-test results, we also ran simple linear regressions for each variable using the treatment indicator as the independent variable. This allows us to check whether the coefficient on treatment (i.e., the mean difference between groups) matches the value from the t-test.\n\n\n\nRegression Model\nFor each variable (e.g., mrm2), we estimate the model: \\[\nY_i = \\beta_0 + \\beta_1 \\cdot \\text{treatment}_i + \\epsilon_i\n\\]\nWhere:\n\n\\(Y_i\\) is the outcome variable (e.g., mrm2, freq, etc.)\n\\(\\text{treatment}_i = 1\\) if the observation is in the treatment group, and \\(0\\) otherwise\n\n\\(\\beta_1\\) represents the difference in means between the treatment and control groups\n\n\nIf the coefficient of treatment is not statistically significant, it means there is no difference between the groups, which supports the randomization.\nThe p-value and 95% confidence interval should match the t-test results exactly.\n\n\n\nRegression Code\nimport statsmodels.api as sm\n\n# Prepare data\ndata = pd.read_stata(\"karlan_list_2007.dta\")\ndf_clean = data[['mrm2', 'treatment']].dropna()\n\n# Define X and y\nX = sm.add_constant(df_clean['treatment'])  # Adds intercept\ny = df_clean['mrm2']\n\n# Fit the model\nmodel = sm.OLS(y, X).fit()\n\n# Print regression result\nprint(model.summary())\nFor mrm2: The coefficient for treatment is 0.009, with a p-value of 0.940, which is exactly consistent with the t-test result. The 95% confidence interval also matches.\nSo the treatment group and the control group prove to be randomly selected and have no group differences."
  },
  {
    "objectID": "hw1_questions.html#experimental-results",
    "href": "hw1_questions.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\nThe bar chart below shows the proportion of people who donated in each group.\n\n\n\nDonation Rate by Group\n\n\n\n\nImpact of Treatment on Donation Behavior\nWe tested whether individuals who received a treatment letter (with a matching grant offer) were more likely to make a charitable donation compared to those who received a standard control letter.\n\n\nT-test Code\ndf_treatment = data[data['treatment'] == 1]['gave'].dropna()\ndf_control = data[data['treatment'] == 0]['gave'].dropna()\n\nt_stat, p_val = stats.ttest_ind(df_treatment, df_control, equal_var=True)\n\nprint(f\"T-statistic: {t_stat:.3f}\")\nprint(f\"P-value: {p_val:.4f}\")\n\nmean_diff = df_treatment.mean() - df_control.mean()\nn1, n2 = len(df_treatment), len(df_control)\ns1, s2 = df_treatment.std(ddof=1), df_control.std(ddof=1)\n\nsp = np.sqrt(((n1 - 1)*s1**2 + (n2 - 1)*s2**2) / (n1 + n2 - 2))\nse = sp * np.sqrt(1/n1 + 1/n2)\n\nt_crit = stats.t.ppf(0.975, df=n1 + n2 - 2)\nci_low = mean_diff - t_crit * se\nci_high = mean_diff + t_crit * se\n\nprint(f\"\\nMean Difference: {mean_diff:.3f}\")\nprint(f\"95% Confidence Interval: ({ci_low:.3f}, {ci_high:.3f})\")\nA t-test comparing the proportion of donors between the two groups revealed a statistically significant difference: individuals in the treatment group were more likely to donate. The mean difference in donation rates was 0.004, with a t-statistic of 3.101 and a p-value of 0.0019. The 95% confidence interval for the difference was (0.002, 0.007), which does not include zero — providing strong evidence that the treatment had a real effect.\nTo confirm this result, we ran a bivariate linear regression where the outcome variable was a binary indicator of whether a donation was made.\n\n\nRegression Code\ndf_clean = data[['gave', 'treatment']].dropna()\nX = sm.add_constant(df_clean['treatment']) \ny = df_clean['gave']\nmodel = sm.OLS(y, X).fit()\n\nprint(model.summary())\nThe coefficient on the treatment variable was 0.0042, with a p-value of 0.002, and the 95% confidence interval was also (0.002, 0.007). This perfectly aligns with the t-test results and confirms that the treatment increased the likelihood of giving.\nThis result supports the idea that including a matching grant offer in a donation appeal can meaningfully influence behavior. Even though the financial amount offered as a match was the same, simply presenting the opportunity to have one’s donation matched significantly increased the chance that someone would donate at all.\nIn behavioral terms, this suggests that people respond to cues of increased impact — like knowing their donation will be matched. It may enhance the perceived effectiveness or social validation of their gift. This finding highlights how small changes in how we ask for donations can significantly affect participation rates.\n\n\nProbit Regression Analysis\nTo better understand the impact of the treatment on donation behavior, we estimated a probit regression where the binary outcome variable was whether or not a donation was made (gave), and the explanatory variable was the assignment to the treatment group.\n\n\nCode\ndf_clean = data[['gave', 'treatment']].dropna()\n\nX = sm.add_constant(df_clean['treatment'])\ny = df_clean['gave']\n\nprobit_model = sm.Probit(y, X).fit()\nprint(probit_model.summary())\nThe estimated coefficient on the treatment variable is 0.0868, with a z-statistic of 3.113 and a p-value of 0.002, indicating that the effect is statistically significant at the 1% level. The 95% confidence interval for the coefficient is (0.032, 0.141), which excludes zero.\nThis result replicates Table 3, Column 1 of Karlan & List (2007), confirming that individuals in the treatment group were significantly more likely to make a charitable donation than those in the control group.\nThe positive and significant coefficient on treatment suggests that receiving a fundraising letter with a matching grant offer makes people more likely to donate, even after controlling for random assignment through a nonlinear probability model.\nIn simpler terms, this shows that human behavior is sensitive to perceived impact. When people are told that their donation will be matched by someone else, they are more motivated to act. This finding reinforces the behavioral insight that the way a request is framed — even without changing the actual cost — can meaningfully affect decision-making.\nThis has real-world implications for how nonprofits design their appeals: adding a matching offer not only increases donation amounts, but also boosts the likelihood that people will give at all.\n\n\nDifferences between Match Rates\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\nWe test whether higher match ratios (2:1, 3:1) increase the probability of donation compared to a baseline 1:1 match. The following T-tests compare donation rates between these match conditions.\n\n\nT-test Code\n\n# Filter for treatment group with valid ratio and donation outcome\ndf_ratio = data[(data['treatment'] == 1) & (data['ratio'].notnull()) & (data['gave'].notnull())]\n\n# Split by match ratio\ngave_11 = df_ratio[df_ratio['ratio'] == 1]['gave'].dropna()\ngave_21 = df_ratio[df_ratio['ratio'] == 2]['gave'].dropna()\ngave_31 = df_ratio[df_ratio['ratio'] == 3]['gave'].dropna()\n\n# Donation rates\nmean_11 = gave_11.mean()\nmean_21 = gave_21.mean()\nmean_31 = gave_31.mean()\n\nprint(\"Donation Rates:\")\nprint(f\"1:1  = {mean_11:.4f}\")\nprint(f\"2:1  = {mean_21:.4f}\")\nprint(f\"3:1  = {mean_31:.4f}\")\n\n# T-tests\nprint(\"\\nT-test Results:\")\nt_21, p_21 = stats.ttest_ind(gave_21, gave_11, equal_var=False)\nt_31, p_31 = stats.ttest_ind(gave_31, gave_11, equal_var=False)\nt_32, p_32 = stats.ttest_ind(gave_31, gave_21, equal_var=False)\n\nprint(f\"2:1 vs 1:1 → T = {t_21:.3f}, p = {p_21:.4f}\")\nprint(f\"3:1 vs 1:1 → T = {t_31:.3f}, p = {p_31:.4f}\")\nprint(f\"3:1 vs 2:1 → T = {t_32:.3f}, p = {p_32:.4f}\")\n\nT-test Results:\n\n2:1 vs 1:1 → T = 0.965, p = 0.3345\n3:1 vs 1:1 → T = 1.015, p = 0.3101\n3:1 vs 2:1 → T = 0.050, p = 0.9600\n\n\nAlthough the donation rates appear slightly higher for the 2:1 and 3:1 match offers (2.26% and 2.27%) compared to the 1:1 match (2.07%), the differences are not statistically significant. All p-values are far above the 0.05 threshold, meaning we cannot reject the null hypothesis of equal donation probabilities between match ratio groups.\nWe now assess the effect of different match ratios on donation behavior using a linear regression model. We create dummy variables for 2:1 and 3:1 match ratios, using 1:1 as the reference group.\n\n\nRegression Code\n\n# Filter treatment group and prepare ratio dummies\ndf_ratio = data[(data['treatment'] == 1) & (data['ratio'].isin([1,2,3])) & (data['gave'].notnull())]\ndf_ratio['ratio2'] = (df_ratio['ratio'] == 2).astype(int)\ndf_ratio['ratio3'] = (df_ratio['ratio'] == 3).astype(int)\n\n# Run regression\nX = sm.add_constant(df_ratio[['ratio2', 'ratio3']])\ny = df_ratio['gave']\nmodel = sm.OLS(y, X).fit()\nprint(model.summary())\n\nRegression Results Summary:\n\nIntercept (baseline 1:1 match): 0.0207\nCoefficient for 2:1 match: +0.0019, p = 0.338\nCoefficient for 3:1 match: +0.0020, p = 0.313\n\n\nThis regression confirms the earlier t-test findings: although donation rates appear slightly higher for 2:1 and 3:1 match ratios compared to 1:1, the differences are not statistically significant. Both p-values are above 0.3, and the confidence intervals include zero, suggesting no meaningful increase in donation probability from increasing the match ratio.\nThis matches the earlier t-test results and supports the paper’s finding.\nWe also compare donation rates across match ratios in two ways:\n\nDirectly from the data — by calculating group means\n\nFrom the regression model — by examining the estimated coefficients\n\nBoth approaches provide estimates of how 2:1 and 3:1 match ratios compare to the 1:1 baseline.\n\n\nCode\n# Directly from data\ngave_11 = df_ratio[df_ratio['ratio'] == 1]['gave'].mean()\ngave_21 = df_ratio[df_ratio['ratio'] == 2]['gave'].mean()\ngave_31 = df_ratio[df_ratio['ratio'] == 3]['gave'].mean()\n\nprint(\"Direct from data:\")\nprint(f\"2:1 - 1:1 = {gave_21 - gave_11:.4f}\")\nprint(f\"3:1 - 1:1 = {gave_31 - gave_11:.4f}\")\nprint(f\"3:1 - 2:1 = {gave_31 - gave_21:.4f}\")\n\n# From regression coefficients\nb2 = model.params['ratio2']\nb3 = model.params['ratio3']\n\nprint(\"\\nFrom regression coefficients:\")\nprint(f\"2:1 - 1:1 = {b2:.4f}\")\nprint(f\"3:1 - 1:1 = {b3:.4f}\")\nprint(f\"3:1 - 2:1 = {b3 - b2:.4f}\")\n\nDirect from data: Response rate difference (2:1 - 1:1): 0.0019\nDirect from data: Response rate difference (3:1 - 1:1): 0.0020\nDirect from data: Response rate difference (3:1 - 2:1): 0.0001\nFrom regression: Response rate difference (2:1 - 1:1): 0.0019\nFrom regression: Response rate difference (3:1 - 1:1): 0.0020\nFrom regression: Response rate difference (3:1 - 2:1): 0.0001\n\nThese results are consistent whether calculated directly from the raw data or from the estimated regression coefficients.\nSuch small differences, combined with the non-significant p-values seen earlier, indicate that increasing the match ratio does not lead to a meaningful increase in the likelihood of donation.\n\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\nIn addition to increasing the likelihood of giving, does the treatment also lead to larger donation amounts among donors?\nWe conduct a t-test comparing the donation amounts (amount) between the treatment and control groups.\n\n\nT-test Code\n# T-test comparing donation amounts\namount_t = data[data['treatment'] == 1]['amount'].dropna()\namount_c = data[data['treatment'] == 0]['amount'].dropna()\n\nfrom scipy import stats\nt_stat, p_val = stats.ttest_ind(amount_t, amount_c, equal_var=False)\n\nprint(f\"T-statistic: {t_stat:.3f}\")\nprint(f\"P-value: {p_val:.4f}\")\nprint(f\"Mean difference: {amount_t.mean() - amount_c.mean():.3f}\")\n\nResults\n\nT-statistic: 1.918\nP-value: 0.0551\nMean difference: $0.154\n\n\nWhile the treatment group donated an average of $0.154 more than the control group, the difference is not statistically significant at the 5% level. The p-value of 0.0551 is just above the standard cutoff of 0.05.\nThis suggests a potential positive effect of treatment on donation amount, but the evidence is not strong enough to make a definitive conclusion.\nNow we restrict the sample to only those individuals who made a donation (amount &gt; 0) and examine whether assignment to the treatment group influenced the amount donated, conditional on giving.\nThis analysis estimates the conditional average treatment effect (CATE) on donation size.\n\n\nRegression Code\n# Filter: only donors\ndf_positive = data[(data['amount'] &gt; 0) & data['treatment'].notnull()]\n\n# Run regression\nX = sm.add_constant(df_positive['treatment'])\ny = df_positive['amount']\nmodel = sm.OLS(y, X).fit()\n\nprint(model.summary())\n\nResults\n\nIntercept (control group mean donation): $45.54\nTreatment coefficient: -1.668, p = 0.561\n\n\nAmong those who donated, individuals in the treatment group gave $1.67 less on average than those in the control group. However, this difference is not statistically significant (p = 0.561), and the confidence interval includes both negative and positive values (-7.31 to +3.97). This means that while the direction of the estimate is slightly negative, we have no evidence to conclude that the treatment caused people to give more or less, once they had already decided to donate.\nSo we do two bar chart to show the distribution directly. The histograms below display the distribution of donation amounts among individuals who made a donation, separately for the treatment and control groups. Each plot includes a red dashed line indicating the sample mean.\n\n\n\nDonation Amounts\n\n\nFrom the plots, we observe that:\n\nBoth groups are highly right-skewed: most donors give between $10–$50, but a small number give substantially more (some over $200).\nThe control group has a slightly higher mean donation ($45.54) compared to the treatment group ($43.87).\nThe two distributions are fairly similar in shape, with the treatment group having a marginally heavier tail but not substantially so.\n\nThis observation is consistent with our previous regression analysis (run on donors only), where the treatment group donated $1.67 less on average, but the difference was not statistically significant (p = 0.561).\nWhile the treatment had a positive effect on donation likelihood, it did not lead to higher donation amounts among those who chose to give. In fact, the treatment group donated slightly less on average.\nThis reinforces the interpretation that the matching grant offer may influence whether someone donates, but does not significantly affect how much they donate, once they decide to give.\n\n\nSimulation Experiment\nAs a reminder of how the t-statistic “works,” in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\n\nLaw of Large Numbers\nNext we want to simulate the cumulative average of differences in donation amounts between treatment and control groups.\nThis helps us visualize how a sample average stabilizes as the number of samples increases.\n\n\n\nCumulative Average of Donation Differences\n\n\n\n\n\nSimulation Code\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Filter for positive donations\ncontrol = data[(data['treatment'] == 0) & (data['amount'] &gt; 0)]['amount'].values\ntreatment = data[(data['treatment'] == 1) & (data['amount'] &gt; 0)]['amount'].values\n\n# Simulate draws\nnp.random.seed(42)\ndraws_control = np.random.choice(control, 100000, replace=True)\ndraws_treatment = np.random.choice(treatment, 10000, replace=True)\n\n# Calculate differences\ndifferences = draws_treatment - draws_control[:10000]\n\n# Compute cumulative average\ncumulative_avg = np.cumsum(differences) / np.arange(1, len(differences) + 1)\n\n# Plot\nplt.figure(figsize=(10, 5))\nplt.plot(cumulative_avg, label='Cumulative Average of Differences', color='steelblue')\nplt.axhline(np.mean(treatment) - np.mean(control), color='red', linestyle='--', label='True Mean Difference')\nplt.title('Cumulative Average of Donation Differences (Treatment - Control)')\nplt.xlabel('Number of Simulated Pairs')\nplt.ylabel('Cumulative Average Difference ($)')\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\nThis simulation illustrates how sample averages behave as we increase the number of observations. Initially, the cumulative average of differences is highly unstable, with large fluctuations. But as more pairs are sampled, the line converges toward the true mean difference, indicated by the red dashed line.\nThis is a direct demonstration of the Law of Large Numbers: as the number of samples increases, the sample mean gets closer to the population mean.\nIn this case, the simulation confirms that although there is considerable variation with small samples, the overall average difference in donations between the treatment and control groups stabilizes close to the true effect — which in our case is slightly negative.\n\n\nCentral Limit Theorem\nTo understand how sample size affects the precision and stability of estimated treatment effects, we simulate four sets of experiments with different sample sizes.\nFor each sample size (50, 200, 500, 1000), we:\n\nDraw n random samples from both the treatment and control distributions\nCompute the average donation difference (treatment - control)\nRepeat this process 1000 times\nPlot the distribution (histogram) of the 1000 average differences\n\n\n\n\nDifferent Sample Size Simulation\n\n\n\n\nSimulation Code\n\n# Filter: only positive donation amounts\ncontrol = data[(data['treatment'] == 0) & (data['amount'] &gt; 0)]['amount'].values\ntreatment = data[(data['treatment'] == 1) & (data['amount'] &gt; 0)]['amount'].values\n\nsample_sizes = [50, 200, 500, 1000]\nn_simulations = 1000\n\nplt.figure(figsize=(20, 4))\nfor i, size in enumerate(sample_sizes):\n    diffs = []\n    for _ in range(n_simulations):\n        c = np.random.choice(control, size, replace=True)\n        t = np.random.choice(treatment, size, replace=True)\n        diffs.append(np.mean(t) - np.mean(c))\n\n    plt.subplot(1, 4, i + 1)\n    plt.hist(diffs, bins=30, color='skyblue', edgecolor='black')\n    plt.axvline(x=0, color='red', linestyle='--', label='Zero Line')\n    plt.title(f'Sample size = {size}')\n    plt.xlabel('Mean Difference ($)')\n    plt.ylabel('Frequency')\n    plt.legend()\n\nplt.tight_layout()\nplt.show()\nAs sample size increases: - The distribution of estimated treatment effects becomes narrower and more concentrated - At small sample sizes (like 50), the distribution is wide, and zero is near the center, indicating a high degree of uncertainty - At larger sample sizes (like 1000), the distribution is tighter, and zero lies closer to the edge of the distribution, suggesting a more stable and possibly significant treatment effect\nThis simulation highlights how larger sample sizes reduce variance and help us better detect true effects.\nLarger samples lead to more precise and stable estimates of the treatment effect"
  },
  {
    "objectID": "hw1_questions.html#simulation-experiment",
    "href": "hw1_questions.html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic “works,” in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\nLaw of Large Numbers\nNext we want to simulate the cumulative average of differences in donation amounts between treatment and control groups.\nThis helps us visualize how a sample average stabilizes as the number of samples increases.\n\n\n\nSimulation Code\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Filter for positive donations\ncontrol = data[(data['treatment'] == 0) & (data['amount'] &gt; 0)]['amount'].values\ntreatment = data[(data['treatment'] == 1) & (data['amount'] &gt; 0)]['amount'].values\n\n# Simulate draws\nnp.random.seed(42)\ndraws_control = np.random.choice(control, 100000, replace=True)\ndraws_treatment = np.random.choice(treatment, 10000, replace=True)\n\n# Calculate differences\ndifferences = draws_treatment - draws_control[:10000]\n\n# Compute cumulative average\ncumulative_avg = np.cumsum(differences) / np.arange(1, len(differences) + 1)\n\n# Plot\nplt.figure(figsize=(10, 5))\nplt.plot(cumulative_avg, label='Cumulative Average of Differences', color='steelblue')\nplt.axhline(np.mean(treatment) - np.mean(control), color='red', linestyle='--', label='True Mean Difference')\nplt.title('Cumulative Average of Donation Differences (Treatment - Control)')\nplt.xlabel('Number of Simulated Pairs')\nplt.ylabel('Cumulative Average Difference ($)')\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\nThis simulation illustrates how sample averages behave as we increase the number of observations. Initially, the cumulative average of differences is highly unstable, with large fluctuations. But as more pairs are sampled, the line converges toward the true mean difference, indicated by the red dashed line.\nThis is a direct demonstration of the Law of Large Numbers: as the number of samples increases, the sample mean gets closer to the population mean.\nIn this case, the simulation confirms that although there is considerable variation with small samples, the overall average difference in donations between the treatment and control groups stabilizes close to the true effect — which in our case is slightly negative.\n\n\nCentral Limit Theorem\nto do: Make 4 histograms like those on slide 44 from our first class at sample sizes 50, 200, 500, and 1000 and explain these plots to the reader. To do this for a sample size of e.g. 50, take 50 draws from each of the control and treatment distributions, and calculate the average difference between those draws. Then repeat that process 999 more times so that you have 1000 averages. Plot the histogram of those averages. Comment on whether zero is in the “middle” of the distribution or whether it’s in the “tail.”"
  },
  {
    "objectID": "data_hw1.html",
    "href": "data_hw1.html",
    "title": "Cheng-Yuan Wu's Website",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\n\n\ndata = pd.read_stata('karlan_list_2007.dta')\ndata.head()\n\n\n\n\n\n\n\n\ntreatment\ncontrol\nratio\nratio2\nratio3\nsize\nsize25\nsize50\nsize100\nsizeno\n...\nredcty\nbluecty\npwhite\npblack\npage18_39\nave_hh_sz\nmedian_hhincome\npowner\npsch_atlstba\npop_propurban\n\n\n\n\n0\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n0.0\n1.0\n0.446493\n0.527769\n0.317591\n2.10\n28517.0\n0.499807\n0.324528\n1.0\n\n\n1\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n1.0\n0.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2\n1\n0\n1\n0\n0\n$100,000\n0\n0\n1\n0\n...\n0.0\n1.0\n0.935706\n0.011948\n0.276128\n2.48\n51175.0\n0.721941\n0.192668\n1.0\n\n\n3\n1\n0\n1\n0\n0\nUnstated\n0\n0\n0\n1\n...\n1.0\n0.0\n0.888331\n0.010760\n0.279412\n2.65\n79269.0\n0.920431\n0.412142\n1.0\n\n\n4\n1\n0\n1\n0\n0\n$50,000\n0\n1\n0\n0\n...\n0.0\n1.0\n0.759014\n0.127421\n0.442389\n1.85\n40908.0\n0.416072\n0.439965\n1.0\n\n\n\n\n5 rows × 51 columns\n\n\n\n\ndf = data.describe(include='all')\ndf\n\n\n\n\n\n\n\n\ntreatment\ncontrol\nratio\nratio2\nratio3\nsize\nsize25\nsize50\nsize100\nsizeno\n...\nredcty\nbluecty\npwhite\npblack\npage18_39\nave_hh_sz\nmedian_hhincome\npowner\npsch_atlstba\npop_propurban\n\n\n\n\ncount\n50083.000000\n50083.000000\n50083\n50083.000000\n50083.000000\n50083\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n...\n49978.000000\n49978.000000\n48217.000000\n48047.000000\n48217.000000\n48221.000000\n48209.000000\n48214.000000\n48215.000000\n48217.000000\n\n\nunique\nNaN\nNaN\n4\nNaN\nNaN\n5\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\ntop\nNaN\nNaN\nControl\nNaN\nNaN\nControl\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nfreq\nNaN\nNaN\n16687\nNaN\nNaN\n16687\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nmean\n0.666813\n0.333187\nNaN\n0.222311\n0.222211\nNaN\n0.166723\n0.166623\n0.166723\n0.166743\n...\n0.510245\n0.488715\n0.819599\n0.086710\n0.321694\n2.429012\n54815.700533\n0.669418\n0.391661\n0.871968\n\n\nstd\n0.471357\n0.471357\nNaN\n0.415803\n0.415736\nNaN\n0.372732\n0.372643\n0.372732\n0.372750\n...\n0.499900\n0.499878\n0.168561\n0.135868\n0.103039\n0.378115\n22027.316665\n0.193405\n0.186599\n0.258654\n\n\nmin\n0.000000\n0.000000\nNaN\n0.000000\n0.000000\nNaN\n0.000000\n0.000000\n0.000000\n0.000000\n...\n0.000000\n0.000000\n0.009418\n0.000000\n0.000000\n0.000000\n5000.000000\n0.000000\n0.000000\n0.000000\n\n\n25%\n0.000000\n0.000000\nNaN\n0.000000\n0.000000\nNaN\n0.000000\n0.000000\n0.000000\n0.000000\n...\n0.000000\n0.000000\n0.755845\n0.014729\n0.258311\n2.210000\n39181.000000\n0.560222\n0.235647\n0.884929\n\n\n50%\n1.000000\n0.000000\nNaN\n0.000000\n0.000000\nNaN\n0.000000\n0.000000\n0.000000\n0.000000\n...\n1.000000\n0.000000\n0.872797\n0.036554\n0.305534\n2.440000\n50673.000000\n0.712296\n0.373744\n1.000000\n\n\n75%\n1.000000\n1.000000\nNaN\n0.000000\n0.000000\nNaN\n0.000000\n0.000000\n0.000000\n0.000000\n...\n1.000000\n1.000000\n0.938827\n0.090882\n0.369132\n2.660000\n66005.000000\n0.816798\n0.530036\n1.000000\n\n\nmax\n1.000000\n1.000000\nNaN\n1.000000\n1.000000\nNaN\n1.000000\n1.000000\n1.000000\n1.000000\n...\n1.000000\n1.000000\n1.000000\n0.989622\n0.997544\n5.270000\n200001.000000\n1.000000\n1.000000\n1.000000\n\n\n\n\n11 rows × 51 columns\n\n\n\n\ndf['amount']\n\ncount     50083.000000\nunique             NaN\ntop                NaN\nfreq               NaN\nmean          0.915694\nstd           8.709199\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           0.000000\nmax         400.000000\nName: amount, dtype: float64\n\n\n\nfrom scipy import stats\n# for mrm2\ndf_treatment = data[data['treatment'] == 1]['mrm2'].dropna()\ndf_control = data[data['treatment'] == 0]['mrm2'].dropna()\n\nt_stat, p_val = stats.ttest_ind(df_treatment, df_control, equal_var=True)\n\nprint(f\"T-statistic: {t_stat:.3f}\")\nprint(f\"P-value: {p_val:.4f}\")\n\nT-statistic: 0.119\nP-value: 0.9049\n\n\n\nimport numpy as np\n\nmean_diff = df_treatment.mean() - df_control.mean()\nn1, n2 = len(df_treatment), len(df_control)\ns1, s2 = df_treatment.std(ddof=1), df_control.std(ddof=1)\n\nsp = np.sqrt(((n1 - 1)*s1**2 + (n2 - 1)*s2**2) / (n1 + n2 - 2))\nse = sp * np.sqrt(1/n1 + 1/n2)\n\nt_crit = stats.t.ppf(0.975, df=n1 + n2 - 2)\nci_low = mean_diff - t_crit * se\nci_high = mean_diff + t_crit * se\n\nprint(f\"\\nMean Difference: {mean_diff:.3f}\")\nprint(f\"95% Confidence Interval: ({ci_low:.3f}, {ci_high:.3f})\")\n\n\nMean Difference: 0.014\n95% Confidence Interval: (-0.211, 0.238)\n\n\n\nimport statsmodels.api as sm\ndf_clean = data[['mrm2', 'treatment']].dropna()\nX = sm.add_constant(df_clean['treatment']) \ny = df_clean['mrm2']\nmodel = sm.OLS(y, X).fit()\n\nprint(model.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   mrm2   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                 -0.000\nMethod:                 Least Squares   F-statistic:                   0.01428\nDate:                Tue, 22 Apr 2025   Prob (F-statistic):              0.905\nTime:                        15:11:15   Log-Likelihood:            -1.9585e+05\nNo. Observations:               50082   AIC:                         3.917e+05\nDf Residuals:                   50080   BIC:                         3.917e+05\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         12.9981      0.094    138.979      0.000      12.815      13.181\ntreatment      0.0137      0.115      0.119      0.905      -0.211       0.238\n==============================================================================\nOmnibus:                     8031.352   Durbin-Watson:                   2.004\nProb(Omnibus):                  0.000   Jarque-Bera (JB):            12471.135\nSkew:                           1.163   Prob(JB):                         0.00\nKurtosis:                       3.751   Cond. No.                         3.23\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n# for freq\ndf_treatment = data[data['treatment'] == 1]['freq'].dropna()\ndf_control = data[data['treatment'] == 0]['freq'].dropna()\n\nt_stat, p_val = stats.ttest_ind(df_treatment, df_control, equal_var=True)\n\nprint(f\"T-statistic: {t_stat:.3f}\")\nprint(f\"P-value: {p_val:.4f}\")\n\nT-statistic: -0.111\nP-value: 0.9117\n\n\n\ndf['freq']\n\ncount     50083.000000\nunique             NaN\ntop                NaN\nfreq               NaN\nmean          8.039355\nstd          11.394454\nmin           0.000000\n25%           2.000000\n50%           4.000000\n75%          10.000000\nmax         218.000000\nName: freq, dtype: float64\n\n\n\nimport numpy as np\n\n# 計算平均差\nmean_diff = df_treatment.mean() - df_control.mean()\n\n# 樣本數與標準差\nn1, n2 = len(df_treatment), len(df_control)\ns1, s2 = df_treatment.std(ddof=1), df_control.std(ddof=1)\n\n# 合併標準差（pooled standard deviation）\nsp = np.sqrt(((n1 - 1)*s1**2 + (n2 - 1)*s2**2) / (n1 + n2 - 2))\n\n# 標準誤\nse = sp * np.sqrt(1/n1 + 1/n2)\n\n# 95% 信賴區間\nt_crit = stats.t.ppf(0.975, df=n1 + n2 - 2)\nci_low = mean_diff - t_crit * se\nci_high = mean_diff + t_crit * se\n\nprint(f\"\\nMean Difference: {mean_diff:.3f}\")\nprint(f\"95% Confidence Interval: ({ci_low:.3f}, {ci_high:.3f})\")\n\n\nMean Difference: -0.012\n95% Confidence Interval: (-0.224, 0.200)\n\n\n\ndf_clean = data[['freq', 'treatment']].dropna()\n# 加入常數項（intercept）\nX = sm.add_constant(df_clean['treatment'])  # 會產生兩個欄位：常數 + treatment\ny = df_clean['freq']\n# 執行 OLS 線性回歸\nmodel = sm.OLS(y, X).fit()\n\n# 印出完整結果\nprint(model.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   freq   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                 -0.000\nMethod:                 Least Squares   F-statistic:                   0.01230\nDate:                Tue, 22 Apr 2025   Prob (F-statistic):              0.912\nTime:                        17:58:28   Log-Likelihood:            -1.9292e+05\nNo. Observations:               50083   AIC:                         3.858e+05\nDf Residuals:                   50081   BIC:                         3.859e+05\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          8.0473      0.088     91.231      0.000       7.874       8.220\ntreatment     -0.0120      0.108     -0.111      0.912      -0.224       0.200\n==============================================================================\nOmnibus:                    49107.114   Durbin-Watson:                   2.016\nProb(Omnibus):                  0.000   Jarque-Bera (JB):          3644795.393\nSkew:                           4.707   Prob(JB):                         0.00\nKurtosis:                      43.718   Cond. No.                         3.23\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n# for amountchange\ndf_treatment = data[data['treatment'] == 1]['amountchange'].dropna()\ndf_control = data[data['treatment'] == 0]['amountchange'].dropna()\n\nt_stat, p_val = stats.ttest_ind(df_treatment, df_control, equal_var=True)\n\nprint(f\"T-statistic: {t_stat:.3f}\")\nprint(f\"P-value: {p_val:.4f}\")\n\nT-statistic: 0.527\nP-value: 0.5982\n\n\n\nimport numpy as np\n\n# 計算平均差\nmean_diff = df_treatment.mean() - df_control.mean()\n\n# 樣本數與標準差\nn1, n2 = len(df_treatment), len(df_control)\ns1, s2 = df_treatment.std(ddof=1), df_control.std(ddof=1)\n\n# 合併標準差（pooled standard deviation）\nsp = np.sqrt(((n1 - 1)*s1**2 + (n2 - 1)*s2**2) / (n1 + n2 - 2))\n\n# 標準誤\nse = sp * np.sqrt(1/n1 + 1/n2)\n\n# 95% 信賴區間\nt_crit = stats.t.ppf(0.975, df=n1 + n2 - 2)\nci_low = mean_diff - t_crit * se\nci_high = mean_diff + t_crit * se\n\nprint(f\"\\nMean Difference: {mean_diff:.3f}\")\nprint(f\"95% Confidence Interval: ({ci_low:.3f}, {ci_high:.3f})\")\n\n\nMean Difference: 6.331\n95% Confidence Interval: (-17.216, 29.877)\n\n\n\ndf['amountchange']\n\ncount      50083.000000\nunique              NaN\ntop                 NaN\nfreq                NaN\nmean         -52.672016\nstd         1267.238647\nmin      -200412.125000\n25%          -50.000000\n50%          -30.000000\n75%          -25.000000\nmax          275.000000\nName: amountchange, dtype: float64\n\n\n\n# for female\nimport pandas as pd\nimport numpy as np\n\ndf_treatment = data[data['treatment'] == 1]['female'].dropna()\ndf_control = data[data['treatment'] == 0]['female'].dropna()\n\nt_stat, p_val = stats.ttest_ind(df_treatment, df_control, equal_var=True)\n\nprint(f\"T-statistic: {t_stat:.3f}\")\nprint(f\"P-value: {p_val:.4f}\")\n\nmean_diff = df_treatment.mean() - df_control.mean()\nn1, n2 = len(df_treatment), len(df_control)\ns1, s2 = df_treatment.std(ddof=1), df_control.std(ddof=1)\n\nsp = np.sqrt(((n1 - 1)*s1**2 + (n2 - 1)*s2**2) / (n1 + n2 - 2))\nse = sp * np.sqrt(1/n1 + 1/n2)\n\nt_crit = stats.t.ppf(0.975, df=n1 + n2 - 2)\nci_low = mean_diff - t_crit * se\nci_high = mean_diff + t_crit * se\n\nprint(f\"\\nMean Difference: {mean_diff:.3f}\")\nprint(f\"95% Confidence Interval: ({ci_low:.3f}, {ci_high:.3f})\")\n\nT-statistic: -1.758\nP-value: 0.0787\n\nMean Difference: -0.008\n95% Confidence Interval: (-0.016, 0.001)\n\n\n\ndf['female']\n\ncount     48972.000000\nunique             NaN\ntop                NaN\nfreq               NaN\nmean          0.277669\nstd           0.447854\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           1.000000\nmax           1.000000\nName: female, dtype: float64\n\n\n\nimport matplotlib.pyplot as plt\n\ngrouped = data.groupby('treatment')['gave'].mean()\n\ngrouped.index = ['Control', 'Treatment']\n\nplt.figure(figsize=(6, 4))\nplt.bar(grouped.index, grouped.values, color=[\"gray\", \"steelblue\"])\nplt.ylabel('Proportion Donated')\nplt.title('Donation Rate by Group')\nplt.ylim(0, 0.03)  # 因為捐款率很低，設定 y 軸比較清楚\nplt.grid(axis='y', linestyle='--', alpha=0.5)\nplt.show()\n\n\n\n\n\n\n\n\n\n# for whether they donate or not\nimport pandas as pd\nimport numpy as np\n\ndf_treatment = data[data['treatment'] == 1]['gave'].dropna()\ndf_control = data[data['treatment'] == 0]['gave'].dropna()\n\nt_stat, p_val = stats.ttest_ind(df_treatment, df_control, equal_var=True)\n\nprint(f\"T-statistic: {t_stat:.3f}\")\nprint(f\"P-value: {p_val:.4f}\")\n\nmean_diff = df_treatment.mean() - df_control.mean()\nn1, n2 = len(df_treatment), len(df_control)\ns1, s2 = df_treatment.std(ddof=1), df_control.std(ddof=1)\n\nsp = np.sqrt(((n1 - 1)*s1**2 + (n2 - 1)*s2**2) / (n1 + n2 - 2))\nse = sp * np.sqrt(1/n1 + 1/n2)\n\nt_crit = stats.t.ppf(0.975, df=n1 + n2 - 2)\nci_low = mean_diff - t_crit * se\nci_high = mean_diff + t_crit * se\n\nprint(f\"\\nMean Difference: {mean_diff:.3f}\")\nprint(f\"95% Confidence Interval: ({ci_low:.3f}, {ci_high:.3f})\")\n\nT-statistic: 3.101\nP-value: 0.0019\n\nMean Difference: 0.004\n95% Confidence Interval: (0.002, 0.007)\n\n\n\ndf_clean = data[['gave', 'treatment']].dropna()\n# 加入常數項（intercept）\nX = sm.add_constant(df_clean['treatment'])  # 會產生兩個欄位：常數 + treatment\ny = df_clean['gave']\n# 執行 OLS 線性回歸\nmodel = sm.OLS(y, X).fit()\n\n# 印出完整結果\nprint(model.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   gave   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                  0.000\nMethod:                 Least Squares   F-statistic:                     9.618\nDate:                Tue, 22 Apr 2025   Prob (F-statistic):            0.00193\nTime:                        19:56:19   Log-Likelihood:                 26630.\nNo. Observations:               50083   AIC:                        -5.326e+04\nDf Residuals:                   50081   BIC:                        -5.324e+04\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          0.0179      0.001     16.225      0.000       0.016       0.020\ntreatment      0.0042      0.001      3.101      0.002       0.002       0.007\n==============================================================================\nOmnibus:                    59814.280   Durbin-Watson:                   2.005\nProb(Omnibus):                  0.000   Jarque-Bera (JB):          4317152.727\nSkew:                           6.740   Prob(JB):                         0.00\nKurtosis:                      46.440   Cond. No.                         3.23\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\nimport pandas as pd\nimport statsmodels.api as sm\n\ndf_clean = data[['gave', 'treatment']].dropna()\n\nX = sm.add_constant(df_clean['treatment'])\ny = df_clean['gave']\n\nprobit_model = sm.Probit(y, X).fit()\nprint(probit_model.summary())\n\nOptimization terminated successfully.\n         Current function value: 0.100443\n         Iterations 7\n                          Probit Regression Results                           \n==============================================================================\nDep. Variable:                   gave   No. Observations:                50083\nModel:                         Probit   Df Residuals:                    50081\nMethod:                           MLE   Df Model:                            1\nDate:                Tue, 22 Apr 2025   Pseudo R-squ.:               0.0009783\nTime:                        20:25:33   Log-Likelihood:                -5030.5\nconverged:                       True   LL-Null:                       -5035.4\nCovariance Type:            nonrobust   LLR p-value:                  0.001696\n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         -2.1001      0.023    -90.073      0.000      -2.146      -2.054\ntreatment      0.0868      0.028      3.113      0.002       0.032       0.141\n==============================================================================\n\n\n\ndata\n\n\n\n\n\n\n\n\ntreatment\ncontrol\nratio\nratio2\nratio3\nsize\nsize25\nsize50\nsize100\nsizeno\n...\nredcty\nbluecty\npwhite\npblack\npage18_39\nave_hh_sz\nmedian_hhincome\npowner\npsch_atlstba\npop_propurban\n\n\n\n\n0\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n0.0\n1.0\n0.446493\n0.527769\n0.317591\n2.10\n28517.0\n0.499807\n0.324528\n1.000000\n\n\n1\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n1.0\n0.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2\n1\n0\n1\n0\n0\n$100,000\n0\n0\n1\n0\n...\n0.0\n1.0\n0.935706\n0.011948\n0.276128\n2.48\n51175.0\n0.721941\n0.192668\n1.000000\n\n\n3\n1\n0\n1\n0\n0\nUnstated\n0\n0\n0\n1\n...\n1.0\n0.0\n0.888331\n0.010760\n0.279412\n2.65\n79269.0\n0.920431\n0.412142\n1.000000\n\n\n4\n1\n0\n1\n0\n0\n$50,000\n0\n1\n0\n0\n...\n0.0\n1.0\n0.759014\n0.127421\n0.442389\n1.85\n40908.0\n0.416072\n0.439965\n1.000000\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n50078\n1\n0\n1\n0\n0\n$25,000\n1\n0\n0\n0\n...\n0.0\n1.0\n0.872797\n0.089959\n0.257265\n2.13\n45047.0\n0.771316\n0.263744\n1.000000\n\n\n50079\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n0.0\n1.0\n0.688262\n0.108889\n0.288792\n2.67\n74655.0\n0.741931\n0.586466\n1.000000\n\n\n50080\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n1.0\n0.0\n0.900000\n0.021311\n0.178689\n2.36\n26667.0\n0.778689\n0.107930\n0.000000\n\n\n50081\n1\n0\n3\n0\n1\nUnstated\n0\n0\n0\n1\n...\n1.0\n0.0\n0.917206\n0.008257\n0.225619\n2.57\n39530.0\n0.733988\n0.184768\n0.634903\n\n\n50082\n1\n0\n3\n0\n1\n$25,000\n1\n0\n0\n0\n...\n0.0\n1.0\n0.530023\n0.074112\n0.340698\n3.70\n48744.0\n0.717843\n0.127941\n0.994181\n\n\n\n\n50083 rows × 51 columns\n\n\n\n\ndf_ratio = data[(data['treatment'] == 1) & (data['ratio'].notnull()) & (data['gave'].notnull())]\n\ngave_11 = df_ratio[df_ratio['ratio'] == 1]['gave'].dropna()\ngave_21 = df_ratio[df_ratio['ratio'] == 2]['gave'].dropna()\ngave_31 = df_ratio[df_ratio['ratio'] == 3]['gave'].dropna()\n\n# 比較各組平均捐款率\nmean_11 = gave_11.mean()\nmean_21 = gave_21.mean()\nmean_31 = gave_31.mean()\n\nprint(\"Donation Rates:\")\nprint(f\"1:1  = {mean_11:.4f}\")\nprint(f\"2:1  = {mean_21:.4f}\")\nprint(f\"3:1  = {mean_31:.4f}\")\n\n# T-tests\nprint(\"\\nT-test Results:\")\nt_21, p_21 = stats.ttest_ind(gave_21, gave_11, equal_var=False)\nt_31, p_31 = stats.ttest_ind(gave_31, gave_11, equal_var=False)\nt_32, p_32 = stats.ttest_ind(gave_31, gave_21, equal_var=False)\n\nprint(f\"2:1 vs 1:1 → T = {t_21:.3f}, p = {p_21:.4f}\")\nprint(f\"3:1 vs 1:1 → T = {t_31:.3f}, p = {p_31:.4f}\")\nprint(f\"3:1 vs 2:1 → T = {t_32:.3f}, p = {p_32:.4f}\")\n\nDonation Rates:\n1:1  = 0.0207\n2:1  = 0.0226\n3:1  = 0.0227\n\nT-test Results:\n2:1 vs 1:1 → T = 0.965, p = 0.3345\n3:1 vs 1:1 → T = 1.015, p = 0.3101\n3:1 vs 2:1 → T = 0.050, p = 0.9600\n\n\n\ndf_ratio = data[(data['treatment'] == 1) & (data['ratio'].isin([1,2,3])) & (data['gave'].notnull())]\n\ndf_ratio['ratio2'] = (df_ratio['ratio'] == 2).astype(int)\ndf_ratio['ratio3'] = (df_ratio['ratio'] == 3).astype(int)\nX = sm.add_constant(df_ratio[['ratio2', 'ratio3']])\ny = df_ratio['gave']\nmodel = sm.OLS(y, X).fit()\nprint(model.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   gave   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                 -0.000\nMethod:                 Least Squares   F-statistic:                    0.6454\nDate:                Tue, 22 Apr 2025   Prob (F-statistic):              0.524\nTime:                        23:39:57   Log-Likelihood:                 16688.\nNo. Observations:               33396   AIC:                        -3.337e+04\nDf Residuals:                   33393   BIC:                        -3.334e+04\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          0.0207      0.001     14.912      0.000       0.018       0.023\nratio2         0.0019      0.002      0.958      0.338      -0.002       0.006\nratio3         0.0020      0.002      1.008      0.313      -0.002       0.006\n==============================================================================\nOmnibus:                    38963.957   Durbin-Watson:                   1.995\nProb(Omnibus):                  0.000   Jarque-Bera (JB):          2506478.937\nSkew:                           6.511   Prob(JB):                         0.00\nKurtosis:                      43.394   Cond. No.                         3.73\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n/var/folders/4_/b_ln9nlx3j19x_2nmv9t3rd40000gn/T/ipykernel_56870/1714721735.py:3: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df_ratio['ratio2'] = (df_ratio['ratio'] == 2).astype(int)\n/var/folders/4_/b_ln9nlx3j19x_2nmv9t3rd40000gn/T/ipykernel_56870/1714721735.py:4: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df_ratio['ratio3'] = (df_ratio['ratio'] == 3).astype(int)\n\n\n\n# directly from data\ngave_11 = df_ratio[df_ratio['ratio'] == 1]['gave'].mean()\ngave_21 = df_ratio[df_ratio['ratio'] == 2]['gave'].mean()\ngave_31 = df_ratio[df_ratio['ratio'] == 3]['gave'].mean()\n\n# 算差值\nprint(\"Direct from data:\")\nprint(f\"2:1 - 1:1 = {gave_21 - gave_11:.4f}\")\nprint(f\"3:1 - 1:1 = {gave_31 - gave_11:.4f}\")\nprint(f\"3:1 - 2:1 = {gave_31 - gave_21:.4f}\")\n\nDirect from data:\n2:1 - 1:1 = 0.0019\n3:1 - 1:1 = 0.0020\n3:1 - 2:1 = 0.0001\n\n\n\n# from model\nb2 = model.params['ratio2']\nb3 = model.params['ratio3']\n\nprint(\"\\nFrom regression coefficients:\")\nprint(f\"2:1 - 1:1 = {b2:.4f}\")        # b2 就是 2:1 相對於 1:1 的差\nprint(f\"3:1 - 1:1 = {b3:.4f}\")        # b3 就是 3:1 相對於 1:1 的差\nprint(f\"3:1 - 2:1 = {b3 - b2:.4f}\")  # b3 相對於 1:1，再減去 b2\n\n\nFrom regression coefficients:\n2:1 - 1:1 = 0.0019\n3:1 - 1:1 = 0.0020\n3:1 - 2:1 = 0.0001\n\n\n\namount_t = data[data['treatment'] == 1]['amount'].dropna()\namount_c = data[data['treatment'] == 0]['amount'].dropna()\n\nt_stat, p_val = stats.ttest_ind(amount_t, amount_c, equal_var=False)\n\nprint(f\"T-statistic: {t_stat:.3f}\")\nprint(f\"P-value: {p_val:.4f}\")\nprint(f\"Mean difference: {amount_t.mean() - amount_c.mean():.3f}\")\n\nT-statistic: 1.918\nP-value: 0.0551\nMean difference: 0.154\n\n\n\ndf_positive = data[(data['amount'] &gt; 0) & data['treatment'].notnull()]\n\nX = sm.add_constant(df_positive['treatment'])\ny = df_positive['amount']\n\nmodel = sm.OLS(y, X).fit()\nprint(model.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                 amount   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                 -0.001\nMethod:                 Least Squares   F-statistic:                    0.3374\nDate:                Wed, 23 Apr 2025   Prob (F-statistic):              0.561\nTime:                        00:05:31   Log-Likelihood:                -5326.8\nNo. Observations:                1034   AIC:                         1.066e+04\nDf Residuals:                    1032   BIC:                         1.067e+04\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         45.5403      2.423     18.792      0.000      40.785      50.296\ntreatment     -1.6684      2.872     -0.581      0.561      -7.305       3.968\n==============================================================================\nOmnibus:                      587.258   Durbin-Watson:                   2.031\nProb(Omnibus):                  0.000   Jarque-Bera (JB):             5623.279\nSkew:                           2.464   Prob(JB):                         0.00\nKurtosis:                      13.307   Cond. No.                         3.49\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\ndf_positive = data[(data['amount'] &gt; 0) & data['treatment'].notnull()]\n\n# 分成 treatment 和 control\namount_t = df_positive[df_positive['treatment'] == 1]['amount']\namount_c = df_positive[df_positive['treatment'] == 0]['amount']\n\n# 計算平均\nmean_t = amount_t.mean()\nmean_c = amount_c.mean()\n\n# 畫圖：Treatment group\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\nplt.hist(amount_t, bins=30, color='skyblue', edgecolor='black')\nplt.axvline(mean_t, color='red', linestyle='--', label=f'Mean = {mean_t:.2f}')\nplt.title('Donation Amounts: Treatment Group')\nplt.xlabel('Amount ($)')\nplt.ylabel('Frequency')\nplt.legend()\n\n# 畫圖：Control group\nplt.subplot(1, 2, 2)\nplt.hist(amount_c, bins=30, color='lightgray', edgecolor='black')\nplt.axvline(mean_c, color='red', linestyle='--', label=f'Mean = {mean_c:.2f}')\nplt.title('Donation Amounts: Control Group')\nplt.xlabel('Amount ($)')\nplt.ylabel('Frequency')\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\ncontrol = data[(data['treatment'] == 0) & (data['amount'] &gt; 0)]['amount'].values\ntreatment = data[(data['treatment'] == 1) & (data['amount'] &gt; 0)]['amount'].values\n\n# Step 1: 抽樣模擬\nnp.random.seed(42)\ndraws_control = np.random.choice(control, 100000, replace=True)\ndraws_treatment = np.random.choice(treatment, 10000, replace=True)\n\n# Step 2: 算出 10,000 個差異值（對應 index）\ndifferences = draws_treatment - draws_control[:10000]\n\n# Step 3: 算累積平均\ncumulative_avg = np.cumsum(differences) / np.arange(1, len(differences) + 1)\n\n# Step 4: 畫圖\nplt.figure(figsize=(10, 5))\nplt.plot(cumulative_avg, color='steelblue', label='Cumulative Average of Differences')\nplt.axhline(np.mean(treatment) - np.mean(control), color='red', linestyle='--', label='True Mean Difference')\nplt.title('Cumulative Average of Donation Differences (Treatment - Control)')\nplt.xlabel('Number of Simulated Pairs')\nplt.ylabel('Cumulative Average Difference ($)')\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\ncontrol = data[(data['treatment'] == 0) & (data['amount'] &gt; 0)]['amount'].values\ntreatment = data[(data['treatment'] == 1) & (data['amount'] &gt; 0)]['amount'].values\n\n# 模擬參數\nsample_sizes = [50, 200, 500, 1000]\nn_simulations = 1000\n\n# 畫圖\nplt.figure(figsize=(18, 4))\n\nfor i, size in enumerate(sample_sizes):\n    diff_list = []\n    for _ in range(n_simulations):\n        draw_c = np.random.choice(control, size, replace=True)\n        draw_t = np.random.choice(treatment, size, replace=True)\n        diff = np.mean(draw_t) - np.mean(draw_c)\n        diff_list.append(diff)\n\n    # 畫圖\n    plt.subplot(1, 4, i+1)\n    plt.hist(diff_list, bins=30, color='skyblue', edgecolor='black')\n    plt.axvline(x=0, color='red', linestyle='--', label='Zero Line')\n    plt.title(f'Sample size = {size}')\n    plt.xlabel('Mean Difference ($)')\n    plt.ylabel('Frequency')\n    plt.legend()\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "hw1_questions.html#linear-regression-validation",
    "href": "hw1_questions.html#linear-regression-validation",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Linear Regression Validation",
    "text": "Linear Regression Validation\nTo confirm the t-test results, we also ran simple linear regressions for each variable using the treatment indicator as the independent variable. This allows us to check whether the coefficient on treatment (i.e., the mean difference between groups) matches the value from the t-test.\n\n\n🔁 Regression Model\nFor each variable (e.g., mrm2), we estimate the model: \\[\nY_i = \\beta_0 + \\beta_1 \\cdot \\text{treatment}_i + \\epsilon_i\n\\]\nWhere: - ( Y_i ) is the outcome variable (mrm2, freq, etc.) - ( _i = 1 ) if the observation is in the treatment group, 0 otherwise\n- ( _1 ) represents the difference in means between the treatment and control groups\n\n\n\n💡 Interpretation\n\nIf the coefficient of treatment is not statistically significant, it means there is no difference between the groups, which supports the randomization.\nThe p-value and 95% confidence interval should match the t-test results exactly.\nFor mrm2 &gt; ✅ The coefficient for treatment is 0.009, with a p-value of 0.940, which is exactly consistent with the t-test result. The 95% confidence interval also matches.\n\n\n\n\n💻 Regression Code\nimport statsmodels.api as sm\n\n# Prepare data\ndata = pd.read_stata(\"karlan_list_2007.dta\")\ndf_clean = data[['mrm2', 'treatment']].dropna()\n\n# Define X and y\nX = sm.add_constant(df_clean['treatment'])  # Adds intercept\ny = df_clean['mrm2']\n\n# Fit the model\nmodel = sm.OLS(y, X).fit()\n\n# Print regression result\nprint(model.summary())"
  },
  {
    "objectID": "blog/blog.html",
    "href": "blog/blog.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nThe study found that announcing a matching grant significantly increased both the likelihood of donation and the average amount given. However, increasing the match ratio—such as from 1:1 to 3:1—did not yield any additional gains in giving. Interestingly, the effectiveness of matching grants also varied by political geography: donors in Republican-leaning (“red”) states were more responsive than those in Democratic-leaning (“blue”) states. These results challenge conventional fundraising wisdom and offer practical insights for designing more effective donation campaigns.\nThis project aims to replicate their findings."
  },
  {
    "objectID": "blog/blog.html#introduction",
    "href": "blog/blog.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nThe study found that announcing a matching grant significantly increased both the likelihood of donation and the average amount given. However, increasing the match ratio—such as from 1:1 to 3:1—did not yield any additional gains in giving. Interestingly, the effectiveness of matching grants also varied by political geography: donors in Republican-leaning (“red”) states were more responsive than those in Democratic-leaning (“blue”) states. These results challenge conventional fundraising wisdom and offer practical insights for designing more effective donation campaigns.\nThis project aims to replicate their findings."
  },
  {
    "objectID": "blog/blog.html#data",
    "href": "blog/blog.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nData Description\nThis dataset originates from the field experiment conducted by Karlan and List (2007), in which 50,083 prior donors were sent fundraising letters randomly assigned to different treatments. Each row represents one individual who received a solicitation letter.\n\n\nDataset Overview\n\nTotal observations: 50,083\nUnit of observation: Individual donor\nDesign: Randomized field experiment with control and treatment groups\n\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\nKey Variables Summary\n\n\n\n\n\n\n\n\nVariable\nMean\nDescription\n\n\n\n\ntreatment\n0.667\nReceived a treatment letter (matching/challenge)\n\n\ncontrol\n0.333\nReceived a standard control letter\n\n\ngave\n0.021\nDonated (binary: 1 if donated, 0 otherwise)\n\n\namount\n0.916\nDonation amount (including 0s for non-donors)\n\n\namountchange\n-52.67\nChange in amount given compared to previous donations\n\n\nhpa\n59.38\nHighest previous contribution\n\n\nltmedmra\n0.494\nIndicator for small prior donors (last gift &lt; $35)\n\n\nyear5\n0.509\nDonor for at least 5 years\n\n\ndormant\n0.523\nAlready donated in 2005\n\n\nfemale\n0.278\nFemale donor\n\n\ncouple\n0.092\nCouple donor\n\n\n\n\n\nTreatment Group Assignment\n\n\n\nVariable\nMean\nDescription\n\n\n\n\nratio2\n0.222\nReceived 2:1 matching treatment\n\n\nratio3\n0.222\nReceived 3:1 matching treatment\n\n\nsize25\n0.167\n$25,000 match threshold\n\n\nsize50\n0.167\n$50,000 match threshold\n\n\nsize100\n0.167\n$100,000 match threshold\n\n\nsizeno\n0.167\nUnstated match threshold\n\n\naskd1\n0.222\nSuggested amount = last donation\n\n\naskd2\n0.222\nSuggested amount = 1.25x last donation\n\n\naskd3\n0.222\nSuggested amount = 1.5x last donation\n\n\n\n\n\nPolitical and Geographic Context\n\n\n\n\n\n\n\n\nVariable\nMean\nDescription\n\n\n\n\nred0\n0.404\nLives in a Republican-leaning (red) state\n\n\nblue0\n0.596\nLives in a Democratic-leaning (blue) state\n\n\nredcty\n0.510\nLives in a red county\n\n\nperbush\n0.489\nState-level vote share for Bush (2004)\n\n\n\n\n\nZip Code Demographics\n\n\n\n\n\n\n\n\nVariable\nMean\nDescription\n\n\n\n\npwhite\n0.820\nProportion of white residents\n\n\npblack\n0.087\nProportion of Black residents\n\n\npage18_39\n0.322\nProportion aged 18–39\n\n\nmedian_hhincome\n$54,816\nMedian household income\n\n\npowner\n0.669\nProportion of homeowners\n\n\npsch_atlstba\n0.392\nProportion with at least a bachelor’s degree\n\n\n\n\n\n\n\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\n\n\nT-test vs. Linear Regression\nTo verify whether the random assignment was successful, we tested a few non-outcome variables to see if treatment and control groups differ significantly at the 95% confidence level. This helps us confirm whether any observed treatment effects later can be attributed to the intervention itself, not to pre-existing group differences.\nWe tested four variables: - mrm2 (months since last donation) - freq (number of prior donations) - amountchange (change in donation amount) - female (gender)\n\n\n\nHypothesis\nFor each variable, we test:\n\nNull hypothesis (H₀): There is no difference in means between treatment and control groups.\nAlternative hypothesis (H₁): There is a difference in means between the groups.\n\n\n\n\nT-test Results\n\n\n\n\n\n\n\n\n\n\n\nFeature\nMean\nStd\nT-statistic\nP-value\n95% Confidence Interval\n\n\n\n\nmrm2\n13.007\n12.081\n0.119\n0.905\n(-0.211, 0.238)\n\n\nfreq\n8.039\n11.394\n-0.111\n0.912\n(-0.224, 0.200)\n\n\namountchange\n-52.672\n1267.239\n0.527\n0.598\n(-17.216, 29.877)\n\n\nfemale\n0.278\n0.448\n-1.758\n0.079\n(-0.016, 0.001)\n\n\n\n\nAll p-values are above 0.05, so we fail to reject the null hypothesis. There is no statistically significant difference between the treatment and control groups for any of these variables, indicating that random assignment was successful.\n\n\n\n\nT-test Code\n# For 'mrm2' as an example\nimport pandas as pd\nimport numpy as np\nfrom scipy import stats\n\ndata = pd.read_stata(\"karlan_list_2007.dta\")\ndf_treatment = data[data['treatment'] == 1]['mrm2'].dropna()\ndf_control = data[data['treatment'] == 0]['mrm2'].dropna()\n\nt_stat, p_val = stats.ttest_ind(df_treatment, df_control, equal_var=True)\n\nprint(f\"T-statistic: {t_stat:.3f}\")\nprint(f\"P-value: {p_val:.4f}\")\n\nmean_diff = df_treatment.mean() - df_control.mean()\nn1, n2 = len(df_treatment), len(df_control)\ns1, s2 = df_treatment.std(ddof=1), df_control.std(ddof=1)\n\nsp = np.sqrt(((n1 - 1)*s1**2 + (n2 - 1)*s2**2) / (n1 + n2 - 2))\nse = sp * np.sqrt(1/n1 + 1/n2)\n\nt_crit = stats.t.ppf(0.975, df=n1 + n2 - 2)\nci_low = mean_diff - t_crit * se\nci_high = mean_diff + t_crit * se\n\nprint(f\"\\nMean Difference: {mean_diff:.3f}\")\nprint(f\"95% Confidence Interval: ({ci_low:.3f}, {ci_high:.3f})\")\n\n\nLinear Regression Validation\nTo confirm the t-test results, we also ran simple linear regressions for each variable using the treatment indicator as the independent variable. This allows us to check whether the coefficient on treatment (i.e., the mean difference between groups) matches the value from the t-test.\n\n\n\nRegression Model\nFor each variable (e.g., mrm2), we estimate the model: \\[\nY_i = \\beta_0 + \\beta_1 \\cdot \\text{treatment}_i + \\epsilon_i\n\\]\nWhere:\n\n\\(Y_i\\) is the outcome variable (e.g., mrm2, freq, etc.)\n\\(\\text{treatment}_i = 1\\) if the observation is in the treatment group, and \\(0\\) otherwise\n\n\\(\\beta_1\\) represents the difference in means between the treatment and control groups\n\n\nIf the coefficient of treatment is not statistically significant, it means there is no difference between the groups, which supports the randomization.\nThe p-value and 95% confidence interval should match the t-test results exactly.\n\n\n\nRegression Code\nimport statsmodels.api as sm\n\n# Prepare data\ndata = pd.read_stata(\"karlan_list_2007.dta\")\ndf_clean = data[['mrm2', 'treatment']].dropna()\n\n# Define X and y\nX = sm.add_constant(df_clean['treatment'])  # Adds intercept\ny = df_clean['mrm2']\n\n# Fit the model\nmodel = sm.OLS(y, X).fit()\n\n# Print regression result\nprint(model.summary())\nFor mrm2: The coefficient for treatment is 0.009, with a p-value of 0.940, which is exactly consistent with the t-test result. The 95% confidence interval also matches.\nSo the treatment group and the control group prove to be randomly selected and have no group differences."
  },
  {
    "objectID": "blog/blog.html#experimental-results",
    "href": "blog/blog.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\nThe bar chart below shows the proportion of people who donated in each group.\n\n\n\nDonation Rate by Group\n\n\n\n\nImpact of Treatment on Donation Behavior\nWe tested whether individuals who received a treatment letter (with a matching grant offer) were more likely to make a charitable donation compared to those who received a standard control letter.\n\n\nT-test Code\ndf_treatment = data[data['treatment'] == 1]['gave'].dropna()\ndf_control = data[data['treatment'] == 0]['gave'].dropna()\n\nt_stat, p_val = stats.ttest_ind(df_treatment, df_control, equal_var=True)\n\nprint(f\"T-statistic: {t_stat:.3f}\")\nprint(f\"P-value: {p_val:.4f}\")\n\nmean_diff = df_treatment.mean() - df_control.mean()\nn1, n2 = len(df_treatment), len(df_control)\ns1, s2 = df_treatment.std(ddof=1), df_control.std(ddof=1)\n\nsp = np.sqrt(((n1 - 1)*s1**2 + (n2 - 1)*s2**2) / (n1 + n2 - 2))\nse = sp * np.sqrt(1/n1 + 1/n2)\n\nt_crit = stats.t.ppf(0.975, df=n1 + n2 - 2)\nci_low = mean_diff - t_crit * se\nci_high = mean_diff + t_crit * se\n\nprint(f\"\\nMean Difference: {mean_diff:.3f}\")\nprint(f\"95% Confidence Interval: ({ci_low:.3f}, {ci_high:.3f})\")\nA t-test comparing the proportion of donors between the two groups revealed a statistically significant difference: individuals in the treatment group were more likely to donate. The mean difference in donation rates was 0.004, with a t-statistic of 3.101 and a p-value of 0.0019. The 95% confidence interval for the difference was (0.002, 0.007), which does not include zero — providing strong evidence that the treatment had a real effect.\nTo confirm this result, we ran a bivariate linear regression where the outcome variable was a binary indicator of whether a donation was made.\n\n\nRegression Code\ndf_clean = data[['gave', 'treatment']].dropna()\nX = sm.add_constant(df_clean['treatment']) \ny = df_clean['gave']\nmodel = sm.OLS(y, X).fit()\n\nprint(model.summary())\nThe coefficient on the treatment variable was 0.0042, with a p-value of 0.002, and the 95% confidence interval was also (0.002, 0.007). This perfectly aligns with the t-test results and confirms that the treatment increased the likelihood of giving.\nThis result supports the idea that including a matching grant offer in a donation appeal can meaningfully influence behavior. Even though the financial amount offered as a match was the same, simply presenting the opportunity to have one’s donation matched significantly increased the chance that someone would donate at all.\nIn behavioral terms, this suggests that people respond to cues of increased impact — like knowing their donation will be matched. It may enhance the perceived effectiveness or social validation of their gift. This finding highlights how small changes in how we ask for donations can significantly affect participation rates.\n\n\nProbit Regression Analysis\nTo better understand the impact of the treatment on donation behavior, we estimated a probit regression where the binary outcome variable was whether or not a donation was made (gave), and the explanatory variable was the assignment to the treatment group.\n\n\nCode\ndf_clean = data[['gave', 'treatment']].dropna()\n\nX = sm.add_constant(df_clean['treatment'])\ny = df_clean['gave']\n\nprobit_model = sm.Probit(y, X).fit()\nprint(probit_model.summary())\nThe estimated coefficient on the treatment variable is 0.0868, with a z-statistic of 3.113 and a p-value of 0.002, indicating that the effect is statistically significant at the 1% level. The 95% confidence interval for the coefficient is (0.032, 0.141), which excludes zero.\nThis result replicates Table 3, Column 1 of Karlan & List (2007), confirming that individuals in the treatment group were significantly more likely to make a charitable donation than those in the control group.\nThe positive and significant coefficient on treatment suggests that receiving a fundraising letter with a matching grant offer makes people more likely to donate, even after controlling for random assignment through a nonlinear probability model.\nIn simpler terms, this shows that human behavior is sensitive to perceived impact. When people are told that their donation will be matched by someone else, they are more motivated to act. This finding reinforces the behavioral insight that the way a request is framed — even without changing the actual cost — can meaningfully affect decision-making.\nThis has real-world implications for how nonprofits design their appeals: adding a matching offer not only increases donation amounts, but also boosts the likelihood that people will give at all.\n\n\nDifferences between Match Rates\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\nWe test whether higher match ratios (2:1, 3:1) increase the probability of donation compared to a baseline 1:1 match. The following T-tests compare donation rates between these match conditions.\n\n\nT-test Code\n\n# Filter for treatment group with valid ratio and donation outcome\ndf_ratio = data[(data['treatment'] == 1) & (data['ratio'].notnull()) & (data['gave'].notnull())]\n\n# Split by match ratio\ngave_11 = df_ratio[df_ratio['ratio'] == 1]['gave'].dropna()\ngave_21 = df_ratio[df_ratio['ratio'] == 2]['gave'].dropna()\ngave_31 = df_ratio[df_ratio['ratio'] == 3]['gave'].dropna()\n\n# Donation rates\nmean_11 = gave_11.mean()\nmean_21 = gave_21.mean()\nmean_31 = gave_31.mean()\n\nprint(\"Donation Rates:\")\nprint(f\"1:1  = {mean_11:.4f}\")\nprint(f\"2:1  = {mean_21:.4f}\")\nprint(f\"3:1  = {mean_31:.4f}\")\n\n# T-tests\nprint(\"\\nT-test Results:\")\nt_21, p_21 = stats.ttest_ind(gave_21, gave_11, equal_var=False)\nt_31, p_31 = stats.ttest_ind(gave_31, gave_11, equal_var=False)\nt_32, p_32 = stats.ttest_ind(gave_31, gave_21, equal_var=False)\n\nprint(f\"2:1 vs 1:1 → T = {t_21:.3f}, p = {p_21:.4f}\")\nprint(f\"3:1 vs 1:1 → T = {t_31:.3f}, p = {p_31:.4f}\")\nprint(f\"3:1 vs 2:1 → T = {t_32:.3f}, p = {p_32:.4f}\")\n\nT-test Results:\n\n2:1 vs 1:1 → T = 0.965, p = 0.3345\n3:1 vs 1:1 → T = 1.015, p = 0.3101\n3:1 vs 2:1 → T = 0.050, p = 0.9600\n\n\nAlthough the donation rates appear slightly higher for the 2:1 and 3:1 match offers (2.26% and 2.27%) compared to the 1:1 match (2.07%), the differences are not statistically significant. All p-values are far above the 0.05 threshold, meaning we cannot reject the null hypothesis of equal donation probabilities between match ratio groups.\nWe now assess the effect of different match ratios on donation behavior using a linear regression model. We create dummy variables for 2:1 and 3:1 match ratios, using 1:1 as the reference group.\n\n\nRegression Code\n\n# Filter treatment group and prepare ratio dummies\ndf_ratio = data[(data['treatment'] == 1) & (data['ratio'].isin([1,2,3])) & (data['gave'].notnull())]\ndf_ratio['ratio2'] = (df_ratio['ratio'] == 2).astype(int)\ndf_ratio['ratio3'] = (df_ratio['ratio'] == 3).astype(int)\n\n# Run regression\nX = sm.add_constant(df_ratio[['ratio2', 'ratio3']])\ny = df_ratio['gave']\nmodel = sm.OLS(y, X).fit()\nprint(model.summary())\n\nRegression Results Summary:\n\nIntercept (baseline 1:1 match): 0.0207\nCoefficient for 2:1 match: +0.0019, p = 0.338\nCoefficient for 3:1 match: +0.0020, p = 0.313\n\n\nThis regression confirms the earlier t-test findings: although donation rates appear slightly higher for 2:1 and 3:1 match ratios compared to 1:1, the differences are not statistically significant. Both p-values are above 0.3, and the confidence intervals include zero, suggesting no meaningful increase in donation probability from increasing the match ratio.\nThis matches the earlier t-test results and supports the paper’s finding.\nWe also compare donation rates across match ratios in two ways:\n\nDirectly from the data — by calculating group means\n\nFrom the regression model — by examining the estimated coefficients\n\nBoth approaches provide estimates of how 2:1 and 3:1 match ratios compare to the 1:1 baseline.\n\n\nCode\n# Directly from data\ngave_11 = df_ratio[df_ratio['ratio'] == 1]['gave'].mean()\ngave_21 = df_ratio[df_ratio['ratio'] == 2]['gave'].mean()\ngave_31 = df_ratio[df_ratio['ratio'] == 3]['gave'].mean()\n\nprint(\"Direct from data:\")\nprint(f\"2:1 - 1:1 = {gave_21 - gave_11:.4f}\")\nprint(f\"3:1 - 1:1 = {gave_31 - gave_11:.4f}\")\nprint(f\"3:1 - 2:1 = {gave_31 - gave_21:.4f}\")\n\n# From regression coefficients\nb2 = model.params['ratio2']\nb3 = model.params['ratio3']\n\nprint(\"\\nFrom regression coefficients:\")\nprint(f\"2:1 - 1:1 = {b2:.4f}\")\nprint(f\"3:1 - 1:1 = {b3:.4f}\")\nprint(f\"3:1 - 2:1 = {b3 - b2:.4f}\")\n\nDirect from data: Response rate difference (2:1 - 1:1): 0.0019\nDirect from data: Response rate difference (3:1 - 1:1): 0.0020\nDirect from data: Response rate difference (3:1 - 2:1): 0.0001\nFrom regression: Response rate difference (2:1 - 1:1): 0.0019\nFrom regression: Response rate difference (3:1 - 1:1): 0.0020\nFrom regression: Response rate difference (3:1 - 2:1): 0.0001\n\nThese results are consistent whether calculated directly from the raw data or from the estimated regression coefficients.\nSuch small differences, combined with the non-significant p-values seen earlier, indicate that increasing the match ratio does not lead to a meaningful increase in the likelihood of donation.\n\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\nIn addition to increasing the likelihood of giving, does the treatment also lead to larger donation amounts among donors?\nWe conduct a t-test comparing the donation amounts (amount) between the treatment and control groups.\n\n\nT-test Code\n# T-test comparing donation amounts\namount_t = data[data['treatment'] == 1]['amount'].dropna()\namount_c = data[data['treatment'] == 0]['amount'].dropna()\n\nfrom scipy import stats\nt_stat, p_val = stats.ttest_ind(amount_t, amount_c, equal_var=False)\n\nprint(f\"T-statistic: {t_stat:.3f}\")\nprint(f\"P-value: {p_val:.4f}\")\nprint(f\"Mean difference: {amount_t.mean() - amount_c.mean():.3f}\")\n\nResults\n\nT-statistic: 1.918\nP-value: 0.0551\nMean difference: $0.154\n\n\nWhile the treatment group donated an average of $0.154 more than the control group, the difference is not statistically significant at the 5% level. The p-value of 0.0551 is just above the standard cutoff of 0.05.\nThis suggests a potential positive effect of treatment on donation amount, but the evidence is not strong enough to make a definitive conclusion.\nNow we restrict the sample to only those individuals who made a donation (amount &gt; 0) and examine whether assignment to the treatment group influenced the amount donated, conditional on giving.\nThis analysis estimates the conditional average treatment effect (CATE) on donation size.\n\n\nRegression Code\n# Filter: only donors\ndf_positive = data[(data['amount'] &gt; 0) & data['treatment'].notnull()]\n\n# Run regression\nX = sm.add_constant(df_positive['treatment'])\ny = df_positive['amount']\nmodel = sm.OLS(y, X).fit()\n\nprint(model.summary())\n\nResults\n\nIntercept (control group mean donation): $45.54\nTreatment coefficient: -1.668, p = 0.561\n\n\nAmong those who donated, individuals in the treatment group gave $1.67 less on average than those in the control group. However, this difference is not statistically significant (p = 0.561), and the confidence interval includes both negative and positive values (-7.31 to +3.97). This means that while the direction of the estimate is slightly negative, we have no evidence to conclude that the treatment caused people to give more or less, once they had already decided to donate.\nSo we do two bar chart to show the distribution directly. The histograms below display the distribution of donation amounts among individuals who made a donation, separately for the treatment and control groups. Each plot includes a red dashed line indicating the sample mean.\n\n\n\nDonation Amounts\n\n\nFrom the plots, we observe that:\n\nBoth groups are highly right-skewed: most donors give between $10–$50, but a small number give substantially more (some over $200).\nThe control group has a slightly higher mean donation ($45.54) compared to the treatment group ($43.87).\nThe two distributions are fairly similar in shape, with the treatment group having a marginally heavier tail but not substantially so.\n\nThis observation is consistent with our previous regression analysis (run on donors only), where the treatment group donated $1.67 less on average, but the difference was not statistically significant (p = 0.561).\nWhile the treatment had a positive effect on donation likelihood, it did not lead to higher donation amounts among those who chose to give. In fact, the treatment group donated slightly less on average.\nThis reinforces the interpretation that the matching grant offer may influence whether someone donates, but does not significantly affect how much they donate, once they decide to give.\n\n\nSimulation Experiment\nAs a reminder of how the t-statistic “works,” in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\n\nLaw of Large Numbers\nNext we want to simulate the cumulative average of differences in donation amounts between treatment and control groups.\nThis helps us visualize how a sample average stabilizes as the number of samples increases.\n\n\n\nCumulative Average of Donation Differences\n\n\n\n\n\nSimulation Code\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Filter for positive donations\ncontrol = data[(data['treatment'] == 0) & (data['amount'] &gt; 0)]['amount'].values\ntreatment = data[(data['treatment'] == 1) & (data['amount'] &gt; 0)]['amount'].values\n\n# Simulate draws\nnp.random.seed(42)\ndraws_control = np.random.choice(control, 100000, replace=True)\ndraws_treatment = np.random.choice(treatment, 10000, replace=True)\n\n# Calculate differences\ndifferences = draws_treatment - draws_control[:10000]\n\n# Compute cumulative average\ncumulative_avg = np.cumsum(differences) / np.arange(1, len(differences) + 1)\n\n# Plot\nplt.figure(figsize=(10, 5))\nplt.plot(cumulative_avg, label='Cumulative Average of Differences', color='steelblue')\nplt.axhline(np.mean(treatment) - np.mean(control), color='red', linestyle='--', label='True Mean Difference')\nplt.title('Cumulative Average of Donation Differences (Treatment - Control)')\nplt.xlabel('Number of Simulated Pairs')\nplt.ylabel('Cumulative Average Difference ($)')\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\nThis simulation illustrates how sample averages behave as we increase the number of observations. Initially, the cumulative average of differences is highly unstable, with large fluctuations. But as more pairs are sampled, the line converges toward the true mean difference, indicated by the red dashed line.\nThis is a direct demonstration of the Law of Large Numbers: as the number of samples increases, the sample mean gets closer to the population mean.\nIn this case, the simulation confirms that although there is considerable variation with small samples, the overall average difference in donations between the treatment and control groups stabilizes close to the true effect — which in our case is slightly negative.\n\n\nCentral Limit Theorem\nTo understand how sample size affects the precision and stability of estimated treatment effects, we simulate four sets of experiments with different sample sizes.\nFor each sample size (50, 200, 500, 1000), we:\n\nDraw n random samples from both the treatment and control distributions\nCompute the average donation difference (treatment - control)\nRepeat this process 1000 times\nPlot the distribution (histogram) of the 1000 average differences\n\n\n\n\nDifferent Sample Size Simulation\n\n\n\n\nSimulation Code\n\n# Filter: only positive donation amounts\ncontrol = data[(data['treatment'] == 0) & (data['amount'] &gt; 0)]['amount'].values\ntreatment = data[(data['treatment'] == 1) & (data['amount'] &gt; 0)]['amount'].values\n\nsample_sizes = [50, 200, 500, 1000]\nn_simulations = 1000\n\nplt.figure(figsize=(20, 4))\nfor i, size in enumerate(sample_sizes):\n    diffs = []\n    for _ in range(n_simulations):\n        c = np.random.choice(control, size, replace=True)\n        t = np.random.choice(treatment, size, replace=True)\n        diffs.append(np.mean(t) - np.mean(c))\n\n    plt.subplot(1, 4, i + 1)\n    plt.hist(diffs, bins=30, color='skyblue', edgecolor='black')\n    plt.axvline(x=0, color='red', linestyle='--', label='Zero Line')\n    plt.title(f'Sample size = {size}')\n    plt.xlabel('Mean Difference ($)')\n    plt.ylabel('Frequency')\n    plt.legend()\n\nplt.tight_layout()\nplt.show()\nAs sample size increases: - The distribution of estimated treatment effects becomes narrower and more concentrated - At small sample sizes (like 50), the distribution is wide, and zero is near the center, indicating a high degree of uncertainty - At larger sample sizes (like 1000), the distribution is tighter, and zero lies closer to the edge of the distribution, suggesting a more stable and possibly significant treatment effect\nThis simulation highlights how larger sample sizes reduce variance and help us better detect true effects.\nLarger samples lead to more precise and stable estimates of the treatment effect"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "My Projects",
    "section": "",
    "text": "Multinomial Logit Model\n\n\n\n\n\n\nCheng-Yuan Wu\n\n\nMay 27, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson Regression Examples\n\n\n\n\n\n\nCheng-Yuan Wu\n\n\nMay 27, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nA Replication of Karlan and List (2007)\n\n\n\n\n\n\nCheng-Yuan Wu\n\n\nMay 27, 2025\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog.html#introduction",
    "href": "blog.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nThe study found that announcing a matching grant significantly increased both the likelihood of donation and the average amount given. However, increasing the match ratio—such as from 1:1 to 3:1—did not yield any additional gains in giving. Interestingly, the effectiveness of matching grants also varied by political geography: donors in Republican-leaning (“red”) states were more responsive than those in Democratic-leaning (“blue”) states. These results challenge conventional fundraising wisdom and offer practical insights for designing more effective donation campaigns.\nThis project aims to replicate their findings."
  },
  {
    "objectID": "blog.html#data",
    "href": "blog.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nData Description\nThis dataset originates from the field experiment conducted by Karlan and List (2007), in which 50,083 prior donors were sent fundraising letters randomly assigned to different treatments. Each row represents one individual who received a solicitation letter.\n\n\nDataset Overview\n\nTotal observations: 50,083\nUnit of observation: Individual donor\nDesign: Randomized field experiment with control and treatment groups\n\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\nKey Variables Summary\n\n\n\n\n\n\n\n\nVariable\nMean\nDescription\n\n\n\n\ntreatment\n0.667\nReceived a treatment letter (matching/challenge)\n\n\ncontrol\n0.333\nReceived a standard control letter\n\n\ngave\n0.021\nDonated (binary: 1 if donated, 0 otherwise)\n\n\namount\n0.916\nDonation amount (including 0s for non-donors)\n\n\namountchange\n-52.67\nChange in amount given compared to previous donations\n\n\nhpa\n59.38\nHighest previous contribution\n\n\nltmedmra\n0.494\nIndicator for small prior donors (last gift &lt; $35)\n\n\nyear5\n0.509\nDonor for at least 5 years\n\n\ndormant\n0.523\nAlready donated in 2005\n\n\nfemale\n0.278\nFemale donor\n\n\ncouple\n0.092\nCouple donor\n\n\n\n\n\nTreatment Group Assignment\n\n\n\nVariable\nMean\nDescription\n\n\n\n\nratio2\n0.222\nReceived 2:1 matching treatment\n\n\nratio3\n0.222\nReceived 3:1 matching treatment\n\n\nsize25\n0.167\n$25,000 match threshold\n\n\nsize50\n0.167\n$50,000 match threshold\n\n\nsize100\n0.167\n$100,000 match threshold\n\n\nsizeno\n0.167\nUnstated match threshold\n\n\naskd1\n0.222\nSuggested amount = last donation\n\n\naskd2\n0.222\nSuggested amount = 1.25x last donation\n\n\naskd3\n0.222\nSuggested amount = 1.5x last donation\n\n\n\n\n\nPolitical and Geographic Context\n\n\n\n\n\n\n\n\nVariable\nMean\nDescription\n\n\n\n\nred0\n0.404\nLives in a Republican-leaning (red) state\n\n\nblue0\n0.596\nLives in a Democratic-leaning (blue) state\n\n\nredcty\n0.510\nLives in a red county\n\n\nperbush\n0.489\nState-level vote share for Bush (2004)\n\n\n\n\n\nZip Code Demographics\n\n\n\n\n\n\n\n\nVariable\nMean\nDescription\n\n\n\n\npwhite\n0.820\nProportion of white residents\n\n\npblack\n0.087\nProportion of Black residents\n\n\npage18_39\n0.322\nProportion aged 18–39\n\n\nmedian_hhincome\n$54,816\nMedian household income\n\n\npowner\n0.669\nProportion of homeowners\n\n\npsch_atlstba\n0.392\nProportion with at least a bachelor’s degree\n\n\n\n\n\n\n\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\n\n\nT-test vs. Linear Regression\nTo verify whether the random assignment was successful, we tested a few non-outcome variables to see if treatment and control groups differ significantly at the 95% confidence level. This helps us confirm whether any observed treatment effects later can be attributed to the intervention itself, not to pre-existing group differences.\nWe tested four variables: - mrm2 (months since last donation) - freq (number of prior donations) - amountchange (change in donation amount) - female (gender)\n\n\n\nHypothesis\nFor each variable, we test:\n\nNull hypothesis (H₀): There is no difference in means between treatment and control groups.\nAlternative hypothesis (H₁): There is a difference in means between the groups.\n\n\n\n\nT-test Results\n\n\n\n\n\n\n\n\n\n\n\nFeature\nMean\nStd\nT-statistic\nP-value\n95% Confidence Interval\n\n\n\n\nmrm2\n13.007\n12.081\n0.119\n0.905\n(-0.211, 0.238)\n\n\nfreq\n8.039\n11.394\n-0.111\n0.912\n(-0.224, 0.200)\n\n\namountchange\n-52.672\n1267.239\n0.527\n0.598\n(-17.216, 29.877)\n\n\nfemale\n0.278\n0.448\n-1.758\n0.079\n(-0.016, 0.001)\n\n\n\n\nAll p-values are above 0.05, so we fail to reject the null hypothesis. There is no statistically significant difference between the treatment and control groups for any of these variables, indicating that random assignment was successful.\n\n\n\n\nT-test Code\n# For 'mrm2' as an example\nimport pandas as pd\nimport numpy as np\nfrom scipy import stats\n\ndata = pd.read_stata(\"karlan_list_2007.dta\")\ndf_treatment = data[data['treatment'] == 1]['mrm2'].dropna()\ndf_control = data[data['treatment'] == 0]['mrm2'].dropna()\n\nt_stat, p_val = stats.ttest_ind(df_treatment, df_control, equal_var=True)\n\nprint(f\"T-statistic: {t_stat:.3f}\")\nprint(f\"P-value: {p_val:.4f}\")\n\nmean_diff = df_treatment.mean() - df_control.mean()\nn1, n2 = len(df_treatment), len(df_control)\ns1, s2 = df_treatment.std(ddof=1), df_control.std(ddof=1)\n\nsp = np.sqrt(((n1 - 1)*s1**2 + (n2 - 1)*s2**2) / (n1 + n2 - 2))\nse = sp * np.sqrt(1/n1 + 1/n2)\n\nt_crit = stats.t.ppf(0.975, df=n1 + n2 - 2)\nci_low = mean_diff - t_crit * se\nci_high = mean_diff + t_crit * se\n\nprint(f\"\\nMean Difference: {mean_diff:.3f}\")\nprint(f\"95% Confidence Interval: ({ci_low:.3f}, {ci_high:.3f})\")\n\n\nLinear Regression Validation\nTo confirm the t-test results, we also ran simple linear regressions for each variable using the treatment indicator as the independent variable. This allows us to check whether the coefficient on treatment (i.e., the mean difference between groups) matches the value from the t-test.\n\n\n\nRegression Model\nFor each variable (e.g., mrm2), we estimate the model: \\[\nY_i = \\beta_0 + \\beta_1 \\cdot \\text{treatment}_i + \\epsilon_i\n\\]\nWhere:\n\n\\(Y_i\\) is the outcome variable (e.g., mrm2, freq, etc.)\n\\(\\text{treatment}_i = 1\\) if the observation is in the treatment group, and \\(0\\) otherwise\n\n\\(\\beta_1\\) represents the difference in means between the treatment and control groups\n\n\nIf the coefficient of treatment is not statistically significant, it means there is no difference between the groups, which supports the randomization.\nThe p-value and 95% confidence interval should match the t-test results exactly.\n\n\n\nRegression Code\nimport statsmodels.api as sm\n\n# Prepare data\ndata = pd.read_stata(\"karlan_list_2007.dta\")\ndf_clean = data[['mrm2', 'treatment']].dropna()\n\n# Define X and y\nX = sm.add_constant(df_clean['treatment'])  # Adds intercept\ny = df_clean['mrm2']\n\n# Fit the model\nmodel = sm.OLS(y, X).fit()\n\n# Print regression result\nprint(model.summary())\nFor mrm2: The coefficient for treatment is 0.009, with a p-value of 0.940, which is exactly consistent with the t-test result. The 95% confidence interval also matches.\nSo the treatment group and the control group prove to be randomly selected and have no group differences."
  },
  {
    "objectID": "blog.html#experimental-results",
    "href": "blog.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\nThe bar chart below shows the proportion of people who donated in each group.\n\n\n\nDonation Rate by Group\n\n\n\n\nImpact of Treatment on Donation Behavior\nWe tested whether individuals who received a treatment letter (with a matching grant offer) were more likely to make a charitable donation compared to those who received a standard control letter.\n\n\nT-test Code\ndf_treatment = data[data['treatment'] == 1]['gave'].dropna()\ndf_control = data[data['treatment'] == 0]['gave'].dropna()\n\nt_stat, p_val = stats.ttest_ind(df_treatment, df_control, equal_var=True)\n\nprint(f\"T-statistic: {t_stat:.3f}\")\nprint(f\"P-value: {p_val:.4f}\")\n\nmean_diff = df_treatment.mean() - df_control.mean()\nn1, n2 = len(df_treatment), len(df_control)\ns1, s2 = df_treatment.std(ddof=1), df_control.std(ddof=1)\n\nsp = np.sqrt(((n1 - 1)*s1**2 + (n2 - 1)*s2**2) / (n1 + n2 - 2))\nse = sp * np.sqrt(1/n1 + 1/n2)\n\nt_crit = stats.t.ppf(0.975, df=n1 + n2 - 2)\nci_low = mean_diff - t_crit * se\nci_high = mean_diff + t_crit * se\n\nprint(f\"\\nMean Difference: {mean_diff:.3f}\")\nprint(f\"95% Confidence Interval: ({ci_low:.3f}, {ci_high:.3f})\")\nA t-test comparing the proportion of donors between the two groups revealed a statistically significant difference: individuals in the treatment group were more likely to donate. The mean difference in donation rates was 0.004, with a t-statistic of 3.101 and a p-value of 0.0019. The 95% confidence interval for the difference was (0.002, 0.007), which does not include zero — providing strong evidence that the treatment had a real effect.\nTo confirm this result, we ran a bivariate linear regression where the outcome variable was a binary indicator of whether a donation was made.\n\n\nRegression Code\ndf_clean = data[['gave', 'treatment']].dropna()\nX = sm.add_constant(df_clean['treatment']) \ny = df_clean['gave']\nmodel = sm.OLS(y, X).fit()\n\nprint(model.summary())\nThe coefficient on the treatment variable was 0.0042, with a p-value of 0.002, and the 95% confidence interval was also (0.002, 0.007). This perfectly aligns with the t-test results and confirms that the treatment increased the likelihood of giving.\nThis result supports the idea that including a matching grant offer in a donation appeal can meaningfully influence behavior. Even though the financial amount offered as a match was the same, simply presenting the opportunity to have one’s donation matched significantly increased the chance that someone would donate at all.\nIn behavioral terms, this suggests that people respond to cues of increased impact — like knowing their donation will be matched. It may enhance the perceived effectiveness or social validation of their gift. This finding highlights how small changes in how we ask for donations can significantly affect participation rates.\n\n\nProbit Regression Analysis\nTo better understand the impact of the treatment on donation behavior, we estimated a probit regression where the binary outcome variable was whether or not a donation was made (gave), and the explanatory variable was the assignment to the treatment group.\n\n\nCode\ndf_clean = data[['gave', 'treatment']].dropna()\n\nX = sm.add_constant(df_clean['treatment'])\ny = df_clean['gave']\n\nprobit_model = sm.Probit(y, X).fit()\nprint(probit_model.summary())\nThe estimated coefficient on the treatment variable is 0.0868, with a z-statistic of 3.113 and a p-value of 0.002, indicating that the effect is statistically significant at the 1% level. The 95% confidence interval for the coefficient is (0.032, 0.141), which excludes zero.\nThis result replicates Table 3, Column 1 of Karlan & List (2007), confirming that individuals in the treatment group were significantly more likely to make a charitable donation than those in the control group.\nThe positive and significant coefficient on treatment suggests that receiving a fundraising letter with a matching grant offer makes people more likely to donate, even after controlling for random assignment through a nonlinear probability model.\nIn simpler terms, this shows that human behavior is sensitive to perceived impact. When people are told that their donation will be matched by someone else, they are more motivated to act. This finding reinforces the behavioral insight that the way a request is framed — even without changing the actual cost — can meaningfully affect decision-making.\nThis has real-world implications for how nonprofits design their appeals: adding a matching offer not only increases donation amounts, but also boosts the likelihood that people will give at all.\n\n\nDifferences between Match Rates\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\nWe test whether higher match ratios (2:1, 3:1) increase the probability of donation compared to a baseline 1:1 match. The following T-tests compare donation rates between these match conditions.\n\n\nT-test Code\n\n# Filter for treatment group with valid ratio and donation outcome\ndf_ratio = data[(data['treatment'] == 1) & (data['ratio'].notnull()) & (data['gave'].notnull())]\n\n# Split by match ratio\ngave_11 = df_ratio[df_ratio['ratio'] == 1]['gave'].dropna()\ngave_21 = df_ratio[df_ratio['ratio'] == 2]['gave'].dropna()\ngave_31 = df_ratio[df_ratio['ratio'] == 3]['gave'].dropna()\n\n# Donation rates\nmean_11 = gave_11.mean()\nmean_21 = gave_21.mean()\nmean_31 = gave_31.mean()\n\nprint(\"Donation Rates:\")\nprint(f\"1:1  = {mean_11:.4f}\")\nprint(f\"2:1  = {mean_21:.4f}\")\nprint(f\"3:1  = {mean_31:.4f}\")\n\n# T-tests\nprint(\"\\nT-test Results:\")\nt_21, p_21 = stats.ttest_ind(gave_21, gave_11, equal_var=False)\nt_31, p_31 = stats.ttest_ind(gave_31, gave_11, equal_var=False)\nt_32, p_32 = stats.ttest_ind(gave_31, gave_21, equal_var=False)\n\nprint(f\"2:1 vs 1:1 → T = {t_21:.3f}, p = {p_21:.4f}\")\nprint(f\"3:1 vs 1:1 → T = {t_31:.3f}, p = {p_31:.4f}\")\nprint(f\"3:1 vs 2:1 → T = {t_32:.3f}, p = {p_32:.4f}\")\n\nT-test Results:\n\n2:1 vs 1:1 → T = 0.965, p = 0.3345\n3:1 vs 1:1 → T = 1.015, p = 0.3101\n3:1 vs 2:1 → T = 0.050, p = 0.9600\n\n\nAlthough the donation rates appear slightly higher for the 2:1 and 3:1 match offers (2.26% and 2.27%) compared to the 1:1 match (2.07%), the differences are not statistically significant. All p-values are far above the 0.05 threshold, meaning we cannot reject the null hypothesis of equal donation probabilities between match ratio groups.\nWe now assess the effect of different match ratios on donation behavior using a linear regression model. We create dummy variables for 2:1 and 3:1 match ratios, using 1:1 as the reference group.\n\n\nRegression Code\n\n# Filter treatment group and prepare ratio dummies\ndf_ratio = data[(data['treatment'] == 1) & (data['ratio'].isin([1,2,3])) & (data['gave'].notnull())]\ndf_ratio['ratio2'] = (df_ratio['ratio'] == 2).astype(int)\ndf_ratio['ratio3'] = (df_ratio['ratio'] == 3).astype(int)\n\n# Run regression\nX = sm.add_constant(df_ratio[['ratio2', 'ratio3']])\ny = df_ratio['gave']\nmodel = sm.OLS(y, X).fit()\nprint(model.summary())\n\nRegression Results Summary:\n\nIntercept (baseline 1:1 match): 0.0207\nCoefficient for 2:1 match: +0.0019, p = 0.338\nCoefficient for 3:1 match: +0.0020, p = 0.313\n\n\nThis regression confirms the earlier t-test findings: although donation rates appear slightly higher for 2:1 and 3:1 match ratios compared to 1:1, the differences are not statistically significant. Both p-values are above 0.3, and the confidence intervals include zero, suggesting no meaningful increase in donation probability from increasing the match ratio.\nThis matches the earlier t-test results and supports the paper’s finding.\nWe also compare donation rates across match ratios in two ways:\n\nDirectly from the data — by calculating group means\n\nFrom the regression model — by examining the estimated coefficients\n\nBoth approaches provide estimates of how 2:1 and 3:1 match ratios compare to the 1:1 baseline.\n\n\nCode\n# Directly from data\ngave_11 = df_ratio[df_ratio['ratio'] == 1]['gave'].mean()\ngave_21 = df_ratio[df_ratio['ratio'] == 2]['gave'].mean()\ngave_31 = df_ratio[df_ratio['ratio'] == 3]['gave'].mean()\n\nprint(\"Direct from data:\")\nprint(f\"2:1 - 1:1 = {gave_21 - gave_11:.4f}\")\nprint(f\"3:1 - 1:1 = {gave_31 - gave_11:.4f}\")\nprint(f\"3:1 - 2:1 = {gave_31 - gave_21:.4f}\")\n\n# From regression coefficients\nb2 = model.params['ratio2']\nb3 = model.params['ratio3']\n\nprint(\"\\nFrom regression coefficients:\")\nprint(f\"2:1 - 1:1 = {b2:.4f}\")\nprint(f\"3:1 - 1:1 = {b3:.4f}\")\nprint(f\"3:1 - 2:1 = {b3 - b2:.4f}\")\n\nDirect from data: Response rate difference (2:1 - 1:1): 0.0019\nDirect from data: Response rate difference (3:1 - 1:1): 0.0020\nDirect from data: Response rate difference (3:1 - 2:1): 0.0001\nFrom regression: Response rate difference (2:1 - 1:1): 0.0019\nFrom regression: Response rate difference (3:1 - 1:1): 0.0020\nFrom regression: Response rate difference (3:1 - 2:1): 0.0001\n\nThese results are consistent whether calculated directly from the raw data or from the estimated regression coefficients.\nSuch small differences, combined with the non-significant p-values seen earlier, indicate that increasing the match ratio does not lead to a meaningful increase in the likelihood of donation.\n\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\nIn addition to increasing the likelihood of giving, does the treatment also lead to larger donation amounts among donors?\nWe conduct a t-test comparing the donation amounts (amount) between the treatment and control groups.\n\n\nT-test Code\n# T-test comparing donation amounts\namount_t = data[data['treatment'] == 1]['amount'].dropna()\namount_c = data[data['treatment'] == 0]['amount'].dropna()\n\nfrom scipy import stats\nt_stat, p_val = stats.ttest_ind(amount_t, amount_c, equal_var=False)\n\nprint(f\"T-statistic: {t_stat:.3f}\")\nprint(f\"P-value: {p_val:.4f}\")\nprint(f\"Mean difference: {amount_t.mean() - amount_c.mean():.3f}\")\n\nResults\n\nT-statistic: 1.918\nP-value: 0.0551\nMean difference: $0.154\n\n\nWhile the treatment group donated an average of $0.154 more than the control group, the difference is not statistically significant at the 5% level. The p-value of 0.0551 is just above the standard cutoff of 0.05.\nThis suggests a potential positive effect of treatment on donation amount, but the evidence is not strong enough to make a definitive conclusion.\nNow we restrict the sample to only those individuals who made a donation (amount &gt; 0) and examine whether assignment to the treatment group influenced the amount donated, conditional on giving.\nThis analysis estimates the conditional average treatment effect (CATE) on donation size.\n\n\nRegression Code\n# Filter: only donors\ndf_positive = data[(data['amount'] &gt; 0) & data['treatment'].notnull()]\n\n# Run regression\nX = sm.add_constant(df_positive['treatment'])\ny = df_positive['amount']\nmodel = sm.OLS(y, X).fit()\n\nprint(model.summary())\n\nResults\n\nIntercept (control group mean donation): $45.54\nTreatment coefficient: -1.668, p = 0.561\n\n\nAmong those who donated, individuals in the treatment group gave $1.67 less on average than those in the control group. However, this difference is not statistically significant (p = 0.561), and the confidence interval includes both negative and positive values (-7.31 to +3.97). This means that while the direction of the estimate is slightly negative, we have no evidence to conclude that the treatment caused people to give more or less, once they had already decided to donate.\nSo we do two bar chart to show the distribution directly. The histograms below display the distribution of donation amounts among individuals who made a donation, separately for the treatment and control groups. Each plot includes a red dashed line indicating the sample mean.\n\n\n\nDonation Amounts\n\n\nFrom the plots, we observe that:\n\nBoth groups are highly right-skewed: most donors give between $10–$50, but a small number give substantially more (some over $200).\nThe control group has a slightly higher mean donation ($45.54) compared to the treatment group ($43.87).\nThe two distributions are fairly similar in shape, with the treatment group having a marginally heavier tail but not substantially so.\n\nThis observation is consistent with our previous regression analysis (run on donors only), where the treatment group donated $1.67 less on average, but the difference was not statistically significant (p = 0.561).\nWhile the treatment had a positive effect on donation likelihood, it did not lead to higher donation amounts among those who chose to give. In fact, the treatment group donated slightly less on average.\nThis reinforces the interpretation that the matching grant offer may influence whether someone donates, but does not significantly affect how much they donate, once they decide to give.\n\n\nSimulation Experiment\nAs a reminder of how the t-statistic “works,” in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\n\nLaw of Large Numbers\nNext we want to simulate the cumulative average of differences in donation amounts between treatment and control groups.\nThis helps us visualize how a sample average stabilizes as the number of samples increases.\n\n\n\nCumulative Average of Donation Differences\n\n\n\n\n\nSimulation Code\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Filter for positive donations\ncontrol = data[(data['treatment'] == 0) & (data['amount'] &gt; 0)]['amount'].values\ntreatment = data[(data['treatment'] == 1) & (data['amount'] &gt; 0)]['amount'].values\n\n# Simulate draws\nnp.random.seed(42)\ndraws_control = np.random.choice(control, 100000, replace=True)\ndraws_treatment = np.random.choice(treatment, 10000, replace=True)\n\n# Calculate differences\ndifferences = draws_treatment - draws_control[:10000]\n\n# Compute cumulative average\ncumulative_avg = np.cumsum(differences) / np.arange(1, len(differences) + 1)\n\n# Plot\nplt.figure(figsize=(10, 5))\nplt.plot(cumulative_avg, label='Cumulative Average of Differences', color='steelblue')\nplt.axhline(np.mean(treatment) - np.mean(control), color='red', linestyle='--', label='True Mean Difference')\nplt.title('Cumulative Average of Donation Differences (Treatment - Control)')\nplt.xlabel('Number of Simulated Pairs')\nplt.ylabel('Cumulative Average Difference ($)')\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\nThis simulation illustrates how sample averages behave as we increase the number of observations. Initially, the cumulative average of differences is highly unstable, with large fluctuations. But as more pairs are sampled, the line converges toward the true mean difference, indicated by the red dashed line.\nThis is a direct demonstration of the Law of Large Numbers: as the number of samples increases, the sample mean gets closer to the population mean.\nIn this case, the simulation confirms that although there is considerable variation with small samples, the overall average difference in donations between the treatment and control groups stabilizes close to the true effect — which in our case is slightly negative.\n\n\nCentral Limit Theorem\nTo understand how sample size affects the precision and stability of estimated treatment effects, we simulate four sets of experiments with different sample sizes.\nFor each sample size (50, 200, 500, 1000), we:\n\nDraw n random samples from both the treatment and control distributions\nCompute the average donation difference (treatment - control)\nRepeat this process 1000 times\nPlot the distribution (histogram) of the 1000 average differences\n\n\n\n\nDifferent Sample Size Simulation\n\n\n\n\nSimulation Code\n\n# Filter: only positive donation amounts\ncontrol = data[(data['treatment'] == 0) & (data['amount'] &gt; 0)]['amount'].values\ntreatment = data[(data['treatment'] == 1) & (data['amount'] &gt; 0)]['amount'].values\n\nsample_sizes = [50, 200, 500, 1000]\nn_simulations = 1000\n\nplt.figure(figsize=(20, 4))\nfor i, size in enumerate(sample_sizes):\n    diffs = []\n    for _ in range(n_simulations):\n        c = np.random.choice(control, size, replace=True)\n        t = np.random.choice(treatment, size, replace=True)\n        diffs.append(np.mean(t) - np.mean(c))\n\n    plt.subplot(1, 4, i + 1)\n    plt.hist(diffs, bins=30, color='skyblue', edgecolor='black')\n    plt.axvline(x=0, color='red', linestyle='--', label='Zero Line')\n    plt.title(f'Sample size = {size}')\n    plt.xlabel('Mean Difference ($)')\n    plt.ylabel('Frequency')\n    plt.legend()\n\nplt.tight_layout()\nplt.show()\nAs sample size increases: - The distribution of estimated treatment effects becomes narrower and more concentrated - At small sample sizes (like 50), the distribution is wide, and zero is near the center, indicating a high degree of uncertainty - At larger sample sizes (like 1000), the distribution is tighter, and zero lies closer to the edge of the distribution, suggesting a more stable and possibly significant treatment effect\nThis simulation highlights how larger sample sizes reduce variance and help us better detect true effects.\nLarger samples lead to more precise and stable estimates of the treatment effect"
  },
  {
    "objectID": "blog/project1/index.html",
    "href": "blog/project1/index.html",
    "title": "Project1",
    "section": "",
    "text": "Section 1: Data\nI clean some data"
  },
  {
    "objectID": "blog/A Replication of Karlan and List (2007)/A Replication of Karlan and List (2007).html",
    "href": "blog/A Replication of Karlan and List (2007)/A Replication of Karlan and List (2007).html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nThe study found that announcing a matching grant significantly increased both the likelihood of donation and the average amount given. However, increasing the match ratio—such as from 1:1 to 3:1—did not yield any additional gains in giving. Interestingly, the effectiveness of matching grants also varied by political geography: donors in Republican-leaning (“red”) states were more responsive than those in Democratic-leaning (“blue”) states. These results challenge conventional fundraising wisdom and offer practical insights for designing more effective donation campaigns.\nThis project aims to replicate their findings."
  },
  {
    "objectID": "blog/A Replication of Karlan and List (2007)/A Replication of Karlan and List (2007).html#introduction",
    "href": "blog/A Replication of Karlan and List (2007)/A Replication of Karlan and List (2007).html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nThe study found that announcing a matching grant significantly increased both the likelihood of donation and the average amount given. However, increasing the match ratio—such as from 1:1 to 3:1—did not yield any additional gains in giving. Interestingly, the effectiveness of matching grants also varied by political geography: donors in Republican-leaning (“red”) states were more responsive than those in Democratic-leaning (“blue”) states. These results challenge conventional fundraising wisdom and offer practical insights for designing more effective donation campaigns.\nThis project aims to replicate their findings."
  },
  {
    "objectID": "blog/A Replication of Karlan and List (2007)/A Replication of Karlan and List (2007).html#data",
    "href": "blog/A Replication of Karlan and List (2007)/A Replication of Karlan and List (2007).html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nData Description\nThis dataset originates from the field experiment conducted by Karlan and List (2007), in which 50,083 prior donors were sent fundraising letters randomly assigned to different treatments. Each row represents one individual who received a solicitation letter.\n\n\nDataset Overview\n\nTotal observations: 50,083\nUnit of observation: Individual donor\nDesign: Randomized field experiment with control and treatment groups\n\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\nKey Variables Summary\n\n\n\n\n\n\n\n\nVariable\nMean\nDescription\n\n\n\n\ntreatment\n0.667\nReceived a treatment letter (matching/challenge)\n\n\ncontrol\n0.333\nReceived a standard control letter\n\n\ngave\n0.021\nDonated (binary: 1 if donated, 0 otherwise)\n\n\namount\n0.916\nDonation amount (including 0s for non-donors)\n\n\namountchange\n-52.67\nChange in amount given compared to previous donations\n\n\nhpa\n59.38\nHighest previous contribution\n\n\nltmedmra\n0.494\nIndicator for small prior donors (last gift &lt; $35)\n\n\nyear5\n0.509\nDonor for at least 5 years\n\n\ndormant\n0.523\nAlready donated in 2005\n\n\nfemale\n0.278\nFemale donor\n\n\ncouple\n0.092\nCouple donor\n\n\n\n\n\nTreatment Group Assignment\n\n\n\nVariable\nMean\nDescription\n\n\n\n\nratio2\n0.222\nReceived 2:1 matching treatment\n\n\nratio3\n0.222\nReceived 3:1 matching treatment\n\n\nsize25\n0.167\n$25,000 match threshold\n\n\nsize50\n0.167\n$50,000 match threshold\n\n\nsize100\n0.167\n$100,000 match threshold\n\n\nsizeno\n0.167\nUnstated match threshold\n\n\naskd1\n0.222\nSuggested amount = last donation\n\n\naskd2\n0.222\nSuggested amount = 1.25x last donation\n\n\naskd3\n0.222\nSuggested amount = 1.5x last donation\n\n\n\n\n\nPolitical and Geographic Context\n\n\n\n\n\n\n\n\nVariable\nMean\nDescription\n\n\n\n\nred0\n0.404\nLives in a Republican-leaning (red) state\n\n\nblue0\n0.596\nLives in a Democratic-leaning (blue) state\n\n\nredcty\n0.510\nLives in a red county\n\n\nperbush\n0.489\nState-level vote share for Bush (2004)\n\n\n\n\n\nZip Code Demographics\n\n\n\n\n\n\n\n\nVariable\nMean\nDescription\n\n\n\n\npwhite\n0.820\nProportion of white residents\n\n\npblack\n0.087\nProportion of Black residents\n\n\npage18_39\n0.322\nProportion aged 18–39\n\n\nmedian_hhincome\n$54,816\nMedian household income\n\n\npowner\n0.669\nProportion of homeowners\n\n\npsch_atlstba\n0.392\nProportion with at least a bachelor’s degree\n\n\n\n\n\n\n\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\n\n\nT-test vs. Linear Regression\nTo verify whether the random assignment was successful, we tested a few non-outcome variables to see if treatment and control groups differ significantly at the 95% confidence level. This helps us confirm whether any observed treatment effects later can be attributed to the intervention itself, not to pre-existing group differences.\nWe tested four variables: - mrm2 (months since last donation) - freq (number of prior donations) - amountchange (change in donation amount) - female (gender)\n\n\n\nHypothesis\nFor each variable, we test:\n\nNull hypothesis (H₀): There is no difference in means between treatment and control groups.\nAlternative hypothesis (H₁): There is a difference in means between the groups.\n\n\n\n\nT-test Results\n\n\n\n\n\n\n\n\n\n\n\nFeature\nMean\nStd\nT-statistic\nP-value\n95% Confidence Interval\n\n\n\n\nmrm2\n13.007\n12.081\n0.119\n0.905\n(-0.211, 0.238)\n\n\nfreq\n8.039\n11.394\n-0.111\n0.912\n(-0.224, 0.200)\n\n\namountchange\n-52.672\n1267.239\n0.527\n0.598\n(-17.216, 29.877)\n\n\nfemale\n0.278\n0.448\n-1.758\n0.079\n(-0.016, 0.001)\n\n\n\n\nAll p-values are above 0.05, so we fail to reject the null hypothesis. There is no statistically significant difference between the treatment and control groups for any of these variables, indicating that random assignment was successful.\n\n\n\n\nT-test Code\n\n\nShow Code\n# For 'mrm2' as an example\nimport pandas as pd\nimport numpy as np\nfrom scipy import stats\n\ndata = pd.read_stata(\"karlan_list_2007.dta\")\ndf_treatment = data[data['treatment'] == 1]['mrm2'].dropna()\ndf_control = data[data['treatment'] == 0]['mrm2'].dropna()\n\nt_stat, p_val = stats.ttest_ind(df_treatment, df_control, equal_var=True)\n\nprint(f\"T-statistic: {t_stat:.3f}\")\nprint(f\"P-value: {p_val:.4f}\")\n\nmean_diff = df_treatment.mean() - df_control.mean()\nn1, n2 = len(df_treatment), len(df_control)\ns1, s2 = df_treatment.std(ddof=1), df_control.std(ddof=1)\n\nsp = np.sqrt(((n1 - 1)*s1**2 + (n2 - 1)*s2**2) / (n1 + n2 - 2))\nse = sp * np.sqrt(1/n1 + 1/n2)\n\nt_crit = stats.t.ppf(0.975, df=n1 + n2 - 2)\nci_low = mean_diff - t_crit * se\nci_high = mean_diff + t_crit * se\n\nprint(f\"\\nMean Difference: {mean_diff:.3f}\")\nprint(f\"95% Confidence Interval: ({ci_low:.3f}, {ci_high:.3f})\")\n\n\nT-statistic: 0.119\nP-value: 0.9049\n\nMean Difference: 0.014\n95% Confidence Interval: (-0.211, 0.238)\n\n\n\n\nLinear Regression Validation\nTo confirm the t-test results, we also ran simple linear regressions for each variable using the treatment indicator as the independent variable. This allows us to check whether the coefficient on treatment (i.e., the mean difference between groups) matches the value from the t-test.\n\n\n\nRegression Model\nFor each variable (e.g., mrm2), we estimate the model: \\[\nY_i = \\beta_0 + \\beta_1 \\cdot \\text{treatment}_i + \\epsilon_i\n\\]\nWhere:\n\n\\(Y_i\\) is the outcome variable (e.g., mrm2, freq, etc.)\n\\(\\text{treatment}_i = 1\\) if the observation is in the treatment group, and \\(0\\) otherwise\n\n\\(\\beta_1\\) represents the difference in means between the treatment and control groups\n\n\nIf the coefficient of treatment is not statistically significant, it means there is no difference between the groups, which supports the randomization.\nThe p-value and 95% confidence interval should match the t-test results exactly.\n\n\n\nRegression Code\n\n\nShow Code\nimport pandas as pd\nimport statsmodels.api as sm\n\n# Prepare data\ndata = pd.read_stata(\"karlan_list_2007.dta\")\ndf_clean = data[['mrm2', 'treatment']].dropna()\n\n# Define X and y\nX = sm.add_constant(df_clean['treatment'])  # Adds intercept\ny = df_clean['mrm2']\n\n# Fit the model\nmodel = sm.OLS(y, X).fit()\n\n# Print regression result\nprint(model.summary())\n\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   mrm2   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                 -0.000\nMethod:                 Least Squares   F-statistic:                   0.01428\nDate:                Fri, 25 Apr 2025   Prob (F-statistic):              0.905\nTime:                        13:41:14   Log-Likelihood:            -1.9585e+05\nNo. Observations:               50082   AIC:                         3.917e+05\nDf Residuals:                   50080   BIC:                         3.917e+05\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         12.9981      0.094    138.979      0.000      12.815      13.181\ntreatment      0.0137      0.115      0.119      0.905      -0.211       0.238\n==============================================================================\nOmnibus:                     8031.352   Durbin-Watson:                   2.004\nProb(Omnibus):                  0.000   Jarque-Bera (JB):            12471.135\nSkew:                           1.163   Prob(JB):                         0.00\nKurtosis:                       3.751   Cond. No.                         3.23\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nFor mrm2: The coefficient for treatment is 0.009, with a p-value of 0.940, which is exactly consistent with the t-test result. The 95% confidence interval also matches.\nSo the treatment group and the control group prove to be randomly selected and have no group differences."
  },
  {
    "objectID": "blog/A Replication of Karlan and List (2007)/A Replication of Karlan and List (2007).html#experimental-results",
    "href": "blog/A Replication of Karlan and List (2007)/A Replication of Karlan and List (2007).html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\nThe bar chart below shows the proportion of people who donated in each group.\n\n\n\nDonation Rate by Group\n\n\nWe tested whether individuals who received a treatment letter (with a matching grant offer) were more likely to make a charitable donation compared to those who received a standard control letter.\n\n\nT-test Code\n\n\nShow Code\ndf_treatment = data[data['treatment'] == 1]['gave'].dropna()\ndf_control = data[data['treatment'] == 0]['gave'].dropna()\n\nt_stat, p_val = stats.ttest_ind(df_treatment, df_control, equal_var=True)\n\nprint(f\"T-statistic: {t_stat:.3f}\")\nprint(f\"P-value: {p_val:.4f}\")\n\nmean_diff = df_treatment.mean() - df_control.mean()\nn1, n2 = len(df_treatment), len(df_control)\ns1, s2 = df_treatment.std(ddof=1), df_control.std(ddof=1)\n\nsp = np.sqrt(((n1 - 1)*s1**2 + (n2 - 1)*s2**2) / (n1 + n2 - 2))\nse = sp * np.sqrt(1/n1 + 1/n2)\n\nt_crit = stats.t.ppf(0.975, df=n1 + n2 - 2)\nci_low = mean_diff - t_crit * se\nci_high = mean_diff + t_crit * se\n\nprint(f\"\\nMean Difference: {mean_diff:.3f}\")\nprint(f\"95% Confidence Interval: ({ci_low:.3f}, {ci_high:.3f})\")\n\n\nT-statistic: 3.101\nP-value: 0.0019\n\nMean Difference: 0.004\n95% Confidence Interval: (0.002, 0.007)\n\n\nA t-test comparing the proportion of donors between the two groups revealed a statistically significant difference: individuals in the treatment group were more likely to donate. The mean difference in donation rates was 0.004, with a t-statistic of 3.101 and a p-value of 0.0019. The 95% confidence interval for the difference was (0.002, 0.007), which does not include zero — providing strong evidence that the treatment had a real effect.\nTo confirm this result, we ran a bivariate linear regression where the outcome variable was a binary indicator of whether a donation was made.\n\n\nRegression Code\n\n\nShow Code\ndf_clean = data[['gave', 'treatment']].dropna()\nX = sm.add_constant(df_clean['treatment']) \ny = df_clean['gave']\nmodel = sm.OLS(y, X).fit()\n\nprint(model.summary())\n\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   gave   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                  0.000\nMethod:                 Least Squares   F-statistic:                     9.618\nDate:                Fri, 25 Apr 2025   Prob (F-statistic):            0.00193\nTime:                        13:41:14   Log-Likelihood:                 26630.\nNo. Observations:               50083   AIC:                        -5.326e+04\nDf Residuals:                   50081   BIC:                        -5.324e+04\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          0.0179      0.001     16.225      0.000       0.016       0.020\ntreatment      0.0042      0.001      3.101      0.002       0.002       0.007\n==============================================================================\nOmnibus:                    59814.280   Durbin-Watson:                   2.005\nProb(Omnibus):                  0.000   Jarque-Bera (JB):          4317152.727\nSkew:                           6.740   Prob(JB):                         0.00\nKurtosis:                      46.440   Cond. No.                         3.23\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nThe coefficient on the treatment variable was 0.0042, with a p-value of 0.002, and the 95% confidence interval was also (0.002, 0.007). This perfectly aligns with the t-test results and confirms that the treatment increased the likelihood of giving.\nThis result supports the idea that including a matching grant offer in a donation appeal can meaningfully influence behavior. Even though the financial amount offered as a match was the same, simply presenting the opportunity to have one’s donation matched significantly increased the chance that someone would donate at all.\nIn behavioral terms, this suggests that people respond to cues of increased impact — like knowing their donation will be matched. It may enhance the perceived effectiveness or social validation of their gift. This finding highlights how small changes in how we ask for donations can significantly affect participation rates.\n\n\nProbit Regression Analysis\nTo better understand the impact of the treatment on donation behavior, we estimated a probit regression where the binary outcome variable was whether or not a donation was made (gave), and the explanatory variable was the assignment to the treatment group.\n\n\nCode\n\n\nShow Code\ndf_clean = data[['gave', 'treatment']].dropna()\n\nX = sm.add_constant(df_clean['treatment'])\ny = df_clean['gave']\n\nprobit_model = sm.Probit(y, X).fit()\nprint(probit_model.summary())\n\n\nOptimization terminated successfully.\n         Current function value: 0.100443\n         Iterations 7\n                          Probit Regression Results                           \n==============================================================================\nDep. Variable:                   gave   No. Observations:                50083\nModel:                         Probit   Df Residuals:                    50081\nMethod:                           MLE   Df Model:                            1\nDate:                Fri, 25 Apr 2025   Pseudo R-squ.:               0.0009783\nTime:                        13:41:15   Log-Likelihood:                -5030.5\nconverged:                       True   LL-Null:                       -5035.4\nCovariance Type:            nonrobust   LLR p-value:                  0.001696\n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         -2.1001      0.023    -90.073      0.000      -2.146      -2.054\ntreatment      0.0868      0.028      3.113      0.002       0.032       0.141\n==============================================================================\n\n\nThe estimated coefficient on the treatment variable is 0.0868, with a z-statistic of 3.113 and a p-value of 0.002, indicating that the effect is statistically significant at the 1% level. The 95% confidence interval for the coefficient is (0.032, 0.141), which excludes zero.\nThis result replicates Table 3, Column 1 of Karlan & List (2007), confirming that individuals in the treatment group were significantly more likely to make a charitable donation than those in the control group.\nThe positive and significant coefficient on treatment suggests that receiving a fundraising letter with a matching grant offer makes people more likely to donate, even after controlling for random assignment through a nonlinear probability model.\nIn simpler terms, this shows that human behavior is sensitive to perceived impact. When people are told that their donation will be matched by someone else, they are more motivated to act. This finding reinforces the behavioral insight that the way a request is framed — even without changing the actual cost — can meaningfully affect decision-making.\nThis has real-world implications for how nonprofits design their appeals: adding a matching offer not only increases donation amounts, but also boosts the likelihood that people will give at all.\n\n\nDifferences between Match Rates\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\nWe test whether higher match ratios (2:1, 3:1) increase the probability of donation compared to a baseline 1:1 match. The following T-tests compare donation rates between these match conditions.\n\n\nT-test Code\n\n\nShow Code\n# Filter for treatment group with valid ratio and donation outcome\ndf_ratio = data[(data['treatment'] == 1) & (data['ratio'].notnull()) & (data['gave'].notnull())]\n\n# Split by match ratio\ngave_11 = df_ratio[df_ratio['ratio'] == 1]['gave'].dropna()\ngave_21 = df_ratio[df_ratio['ratio'] == 2]['gave'].dropna()\ngave_31 = df_ratio[df_ratio['ratio'] == 3]['gave'].dropna()\n\n# Donation rates\nmean_11 = gave_11.mean()\nmean_21 = gave_21.mean()\nmean_31 = gave_31.mean()\n\nprint(\"Donation Rates:\")\nprint(f\"1:1  = {mean_11:.4f}\")\nprint(f\"2:1  = {mean_21:.4f}\")\nprint(f\"3:1  = {mean_31:.4f}\")\n\n# T-tests\nprint(\"\\nT-test Results:\")\nt_21, p_21 = stats.ttest_ind(gave_21, gave_11, equal_var=False)\nt_31, p_31 = stats.ttest_ind(gave_31, gave_11, equal_var=False)\nt_32, p_32 = stats.ttest_ind(gave_31, gave_21, equal_var=False)\n\nprint(f\"2:1 vs 1:1 → T = {t_21:.3f}, p = {p_21:.4f}\")\nprint(f\"3:1 vs 1:1 → T = {t_31:.3f}, p = {p_31:.4f}\")\nprint(f\"3:1 vs 2:1 → T = {t_32:.3f}, p = {p_32:.4f}\")\n\n\nDonation Rates:\n1:1  = 0.0207\n2:1  = 0.0226\n3:1  = 0.0227\n\nT-test Results:\n2:1 vs 1:1 → T = 0.965, p = 0.3345\n3:1 vs 1:1 → T = 1.015, p = 0.3101\n3:1 vs 2:1 → T = 0.050, p = 0.9600\n\n\n\nT-test Results:\n\n2:1 vs 1:1 → T = 0.965, p = 0.3345\n3:1 vs 1:1 → T = 1.015, p = 0.3101\n3:1 vs 2:1 → T = 0.050, p = 0.9600\n\n\nAlthough the donation rates appear slightly higher for the 2:1 and 3:1 match offers (2.26% and 2.27%) compared to the 1:1 match (2.07%), the differences are not statistically significant. All p-values are far above the 0.05 threshold, meaning we cannot reject the null hypothesis of equal donation probabilities between match ratio groups.\nWe now assess the effect of different match ratios on donation behavior using a linear regression model. We create dummy variables for 2:1 and 3:1 match ratios, using 1:1 as the reference group.\n\n\nRegression Code\n\n\nShow Code\n# Filter treatment group and prepare ratio dummies\ndf_ratio = data[(data['treatment'] == 1) & (data['ratio'].isin([1,2,3])) & (data['gave'].notnull())]\ndf_ratio['ratio2'] = (df_ratio['ratio'] == 2).astype(int)\ndf_ratio['ratio3'] = (df_ratio['ratio'] == 3).astype(int)\n\n# Run regression\nX = sm.add_constant(df_ratio[['ratio2', 'ratio3']])\ny = df_ratio['gave']\nmodel = sm.OLS(y, X).fit()\nprint(model.summary())\n\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   gave   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                 -0.000\nMethod:                 Least Squares   F-statistic:                    0.6454\nDate:                Fri, 25 Apr 2025   Prob (F-statistic):              0.524\nTime:                        13:41:15   Log-Likelihood:                 16688.\nNo. Observations:               33396   AIC:                        -3.337e+04\nDf Residuals:                   33393   BIC:                        -3.334e+04\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          0.0207      0.001     14.912      0.000       0.018       0.023\nratio2         0.0019      0.002      0.958      0.338      -0.002       0.006\nratio3         0.0020      0.002      1.008      0.313      -0.002       0.006\n==============================================================================\nOmnibus:                    38963.957   Durbin-Watson:                   1.995\nProb(Omnibus):                  0.000   Jarque-Bera (JB):          2506478.937\nSkew:                           6.511   Prob(JB):                         0.00\nKurtosis:                      43.394   Cond. No.                         3.73\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n/var/folders/4_/b_ln9nlx3j19x_2nmv9t3rd40000gn/T/ipykernel_51210/4272491186.py:3: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n/var/folders/4_/b_ln9nlx3j19x_2nmv9t3rd40000gn/T/ipykernel_51210/4272491186.py:4: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n\n\n\nRegression Results Summary:\n\nIntercept (baseline 1:1 match): 0.0207\nCoefficient for 2:1 match: +0.0019, p = 0.338\nCoefficient for 3:1 match: +0.0020, p = 0.313\n\n\nThis regression confirms the earlier t-test findings: although donation rates appear slightly higher for 2:1 and 3:1 match ratios compared to 1:1, the differences are not statistically significant. Both p-values are above 0.3, and the confidence intervals include zero, suggesting no meaningful increase in donation probability from increasing the match ratio.\nThis matches the earlier t-test results and supports the paper’s finding.\nWe also compare donation rates across match ratios in two ways:\n\nDirectly from the data — by calculating group means\n\nFrom the regression model — by examining the estimated coefficients\n\nBoth approaches provide estimates of how 2:1 and 3:1 match ratios compare to the 1:1 baseline.\n\n\nCode\n\n\nShow Code\n# Directly from data\ngave_11 = df_ratio[df_ratio['ratio'] == 1]['gave'].mean()\ngave_21 = df_ratio[df_ratio['ratio'] == 2]['gave'].mean()\ngave_31 = df_ratio[df_ratio['ratio'] == 3]['gave'].mean()\n\nprint(\"Direct from data:\")\nprint(f\"2:1 - 1:1 = {gave_21 - gave_11:.4f}\")\nprint(f\"3:1 - 1:1 = {gave_31 - gave_11:.4f}\")\nprint(f\"3:1 - 2:1 = {gave_31 - gave_21:.4f}\")\n\n# From regression coefficients\nb2 = model.params['ratio2']\nb3 = model.params['ratio3']\n\nprint(\"\\nFrom regression coefficients:\")\nprint(f\"2:1 - 1:1 = {b2:.4f}\")\nprint(f\"3:1 - 1:1 = {b3:.4f}\")\nprint(f\"3:1 - 2:1 = {b3 - b2:.4f}\")\n\n\nDirect from data:\n2:1 - 1:1 = 0.0019\n3:1 - 1:1 = 0.0020\n3:1 - 2:1 = 0.0001\n\nFrom regression coefficients:\n2:1 - 1:1 = 0.0019\n3:1 - 1:1 = 0.0020\n3:1 - 2:1 = 0.0001\n\n\n\nDirect from data: Response rate difference (2:1 - 1:1): 0.0019\nDirect from data: Response rate difference (3:1 - 1:1): 0.0020\nDirect from data: Response rate difference (3:1 - 2:1): 0.0001\nFrom regression: Response rate difference (2:1 - 1:1): 0.0019\nFrom regression: Response rate difference (3:1 - 1:1): 0.0020\nFrom regression: Response rate difference (3:1 - 2:1): 0.0001\n\nThese results are consistent whether calculated directly from the raw data or from the estimated regression coefficients.\nSuch small differences, combined with the non-significant p-values seen earlier, indicate that increasing the match ratio does not lead to a meaningful increase in the likelihood of donation.\n\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\nIn addition to increasing the likelihood of giving, does the treatment also lead to larger donation amounts among donors?\nWe conduct a t-test comparing the donation amounts (amount) between the treatment and control groups.\n\n\nT-test Code\n\n\nShow Code\n# T-test comparing donation amounts\namount_t = data[data['treatment'] == 1]['amount'].dropna()\namount_c = data[data['treatment'] == 0]['amount'].dropna()\n\nfrom scipy import stats\nt_stat, p_val = stats.ttest_ind(amount_t, amount_c, equal_var=False)\n\nprint(f\"T-statistic: {t_stat:.3f}\")\nprint(f\"P-value: {p_val:.4f}\")\nprint(f\"Mean difference: {amount_t.mean() - amount_c.mean():.3f}\")\n\n\nT-statistic: 1.918\nP-value: 0.0551\nMean difference: 0.154\n\n\n\nResults\n\nT-statistic: 1.918\nP-value: 0.0551\nMean difference: $0.154\n\n\nWhile the treatment group donated an average of $0.154 more than the control group, the difference is not statistically significant at the 5% level. The p-value of 0.0551 is just above the standard cutoff of 0.05.\nThis suggests a potential positive effect of treatment on donation amount, but the evidence is not strong enough to make a definitive conclusion.\nNow we restrict the sample to only those individuals who made a donation (amount &gt; 0) and examine whether assignment to the treatment group influenced the amount donated, conditional on giving.\nThis analysis estimates the conditional average treatment effect (CATE) on donation size.\n\n\nRegression Code\n\n\nShow Code\n# Filter: only donors\ndf_positive = data[(data['amount'] &gt; 0) & data['treatment'].notnull()]\n\n# Run regression\nX = sm.add_constant(df_positive['treatment'])\ny = df_positive['amount']\nmodel = sm.OLS(y, X).fit()\n\nprint(model.summary())\n\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                 amount   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                 -0.001\nMethod:                 Least Squares   F-statistic:                    0.3374\nDate:                Fri, 25 Apr 2025   Prob (F-statistic):              0.561\nTime:                        13:41:15   Log-Likelihood:                -5326.8\nNo. Observations:                1034   AIC:                         1.066e+04\nDf Residuals:                    1032   BIC:                         1.067e+04\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         45.5403      2.423     18.792      0.000      40.785      50.296\ntreatment     -1.6684      2.872     -0.581      0.561      -7.305       3.968\n==============================================================================\nOmnibus:                      587.258   Durbin-Watson:                   2.031\nProb(Omnibus):                  0.000   Jarque-Bera (JB):             5623.279\nSkew:                           2.464   Prob(JB):                         0.00\nKurtosis:                      13.307   Cond. No.                         3.49\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\nResults\n\nIntercept (control group mean donation): $45.54\nTreatment coefficient: -1.668, p = 0.561\n\n\nAmong those who donated, individuals in the treatment group gave $1.67 less on average than those in the control group. However, this difference is not statistically significant (p = 0.561), and the confidence interval includes both negative and positive values (-7.31 to +3.97). This means that while the direction of the estimate is slightly negative, we have no evidence to conclude that the treatment caused people to give more or less, once they had already decided to donate.\nSo we do two bar chart to show the distribution directly. The histograms below display the distribution of donation amounts among individuals who made a donation, separately for the treatment and control groups. Each plot includes a red dashed line indicating the sample mean.\n\n\n\nDonation Amounts\n\n\nFrom the plots, we observe that:\n\nBoth groups are highly right-skewed: most donors give between $10–$50, but a small number give substantially more (some over $200).\nThe control group has a slightly higher mean donation ($45.54) compared to the treatment group ($43.87).\nThe two distributions are fairly similar in shape, with the treatment group having a marginally heavier tail but not substantially so.\n\nThis observation is consistent with our previous regression analysis (run on donors only), where the treatment group donated $1.67 less on average, but the difference was not statistically significant (p = 0.561).\nWhile the treatment had a positive effect on donation likelihood, it did not lead to higher donation amounts among those who chose to give. In fact, the treatment group donated slightly less on average.\nThis reinforces the interpretation that the matching grant offer may influence whether someone donates, but does not significantly affect how much they donate, once they decide to give.\n\n\nSimulation Experiment\nAs a reminder of how the t-statistic “works,” in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\n\nLaw of Large Numbers\nNext we want to simulate the cumulative average of differences in donation amounts between treatment and control groups.\nThis helps us visualize how a sample average stabilizes as the number of samples increases.\n\n\n\nCumulative Average of Donation Differences\n\n\n\n\n\nSimulation Code\n\n\nShow Code\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Filter for positive donations\ncontrol = data[(data['treatment'] == 0) & (data['amount'] &gt; 0)]['amount'].values\ntreatment = data[(data['treatment'] == 1) & (data['amount'] &gt; 0)]['amount'].values\n\n# Simulate draws\nnp.random.seed(42)\ndraws_control = np.random.choice(control, 100000, replace=True)\ndraws_treatment = np.random.choice(treatment, 10000, replace=True)\n\n# Calculate differences\ndifferences = draws_treatment - draws_control[:10000]\n\n# Compute cumulative average\ncumulative_avg = np.cumsum(differences) / np.arange(1, len(differences) + 1)\n\n# Plot\nplt.figure(figsize=(10, 5))\nplt.plot(cumulative_avg, label='Cumulative Average of Differences', color='steelblue')\nplt.axhline(np.mean(treatment) - np.mean(control), color='red', linestyle='--', label='True Mean Difference')\nplt.title('Cumulative Average of Donation Differences (Treatment - Control)')\nplt.xlabel('Number of Simulated Pairs')\nplt.ylabel('Cumulative Average Difference ($)')\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nThis simulation illustrates how sample averages behave as we increase the number of observations. Initially, the cumulative average of differences is highly unstable, with large fluctuations. But as more pairs are sampled, the line converges toward the true mean difference, indicated by the red dashed line.\nThis is a direct demonstration of the Law of Large Numbers: as the number of samples increases, the sample mean gets closer to the population mean.\nIn this case, the simulation confirms that although there is considerable variation with small samples, the overall average difference in donations between the treatment and control groups stabilizes close to the true effect — which in our case is slightly negative.\n\n\nCentral Limit Theorem\nTo understand how sample size affects the precision and stability of estimated treatment effects, we simulate four sets of experiments with different sample sizes.\nFor each sample size (50, 200, 500, 1000), we:\n\nDraw n random samples from both the treatment and control distributions\nCompute the average donation difference (treatment - control)\nRepeat this process 1000 times\nPlot the distribution (histogram) of the 1000 average differences\n\n\n\n\nDifferent Sample Size Simulation\n\n\n\n\nSimulation Code\n\n\nShow Code\n# Filter: only positive donation amounts\ncontrol = data[(data['treatment'] == 0) & (data['amount'] &gt; 0)]['amount'].values\ntreatment = data[(data['treatment'] == 1) & (data['amount'] &gt; 0)]['amount'].values\n\nsample_sizes = [50, 200, 500, 1000]\nn_simulations = 1000\n\nplt.figure(figsize=(20, 4))\nfor i, size in enumerate(sample_sizes):\n    diffs = []\n    for _ in range(n_simulations):\n        c = np.random.choice(control, size, replace=True)\n        t = np.random.choice(treatment, size, replace=True)\n        diffs.append(np.mean(t) - np.mean(c))\n\n    plt.subplot(1, 4, i + 1)\n    plt.hist(diffs, bins=30, color='skyblue', edgecolor='black')\n    plt.axvline(x=0, color='red', linestyle='--', label='Zero Line')\n    plt.title(f'Sample size = {size}')\n    plt.xlabel('Mean Difference ($)')\n    plt.ylabel('Frequency')\n    plt.legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nAs sample size increases: - The distribution of estimated treatment effects becomes narrower and more concentrated - At small sample sizes (like 50), the distribution is wide, and zero is near the center, indicating a high degree of uncertainty - At larger sample sizes (like 1000), the distribution is tighter, and zero lies closer to the edge of the distribution, suggesting a more stable and possibly significant treatment effect\nThis simulation highlights how larger sample sizes reduce variance and help us better detect true effects.\nLarger samples lead to more precise and stable estimates of the treatment effect."
  },
  {
    "objectID": "test-1.html",
    "href": "test-1.html",
    "title": "Untitled",
    "section": "",
    "text": "▶ Click here to activate code\n\n\n\nShow Code\nprint(\"👋 Hello! This will run only after you click 'Run'!\")\n\n\n👋 Hello! This will run only after you click 'Run'!\n\n\n\n\n\n\n\n\nClick to show analysis"
  },
  {
    "objectID": "test-1.html#optional-加一顆-thebe-啟動按鈕",
    "href": "test-1.html#optional-加一顆-thebe-啟動按鈕",
    "title": "Untitled",
    "section": "💡 Optional: 加一顆 Thebe 啟動按鈕",
    "text": "💡 Optional: 加一顆 Thebe 啟動按鈕\n你可以在 .qmd 裡加一行：\n```markdown Activate Thebe ::: {.callout-note title=“Click to show analysis” collapse=true} :::"
  },
  {
    "objectID": "blog/project1/hw2_questions.html",
    "href": "blog/project1/hw2_questions.html",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\nThis dataset contains information on 1,500 mature engineering firms and is intended to help evaluate whether the use of Blueprinty software is associated with a higher number of awarded patents. The dataset includes four columns:\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\npatents\nNumber of patents awarded to the firm in the past five years (integer).\n\n\nregion\nThe geographic region where the firm is located (categorical).\n\n\nage\nThe age of the firm since incorporation, in years (float).\n\n\niscustomer\nBinary indicator of whether the firm uses Blueprinty software (0 = no, 1 = yes).\n\n\n\n\n\n\n\nTotal observations: 1,500 firms\n\nNo missing values\nData types:\n\npatents: int64\n\nregion: object (categorical)\n\nage: float64\n\niscustomer: int64 (binary)\n\n\nTo explore whether firms using Blueprinty’s software tend to have more patents, we compared the distribution of patent counts between customers and non-customers.\nThe histograms below show the number of patents awarded to firms, segmented by whether or not they are Blueprinty customers (iscustomer = 1 or 0):\n\n\n\nHistogram by iscustomer\n\n\nVisually, we observe that Blueprinty customers tend to have slightly more patents than non-customers. The distribution for customers is shifted to the right and shows a longer tail, indicating a higher frequency of firms with larger patent counts.\n\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nblueprinty = pd.read_csv('blueprinty.csv')\ng = sns.displot(\n    data=blueprinty,\n    x=\"patents\",\n    col=\"iscustomer\",  \n    kind=\"hist\",\n    bins=30,\n    kde=True,\n    height=4,\n    aspect=1.5,\n    facet_kws={'margin_titles': True}\n)\n\ng.set_axis_labels(\"Number of Patents\", \"Count\")\ng.set_titles(col_template=\"Customer: {col_name}\")\nplt.subplots_adjust(top=0.8)\nfor ax in g.axes.flatten():\n    ax.tick_params(labelbottom=True)\ng.fig.suptitle(\"Patent Distribution by Customer Status\")\nTo confirm this pattern numerically, we calculate the average number of patents for each group:\npatent_mean_bycustomer = blueprinty.groupby('iscustomer')['patents'].mean()\npatent_mean_bycustomer\nThe results show:\n\nNon-customers: 3.47 patents on average\nCustomers: 4.13 patents on average\n\nThis difference supports the notion that customer firms, on average, are more productive in securing patents.\nWhile this descriptive analysis does not establish a causal link, the pattern suggests a positive association between using Blueprinty and higher patent output.\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\nFrom the regional distribution, we observe that Blueprinty customers are most concentrated in the Northeast, where their count even exceeds that of non-customers. In contrast, in regions such as the Midwest, Northwest, and South, non-customer firms are significantly more numerous. This suggests that Blueprinty may have a stronger market presence or outreach in the Northeast region.\n\n\n\nRegion Distribution\n\n\nIn terms of firm age, Blueprinty customers tend to be younger. The majority of customer firms fall into the ≤20 and 21–30 age groups, while very few are in the 41–50 or &gt;50 categories. Although non-customer firms also peak in the 21–30 group, they are more evenly distributed across older age groups compared to customers.\n\n\n\nAge Distribution\n\n\nThese patterns imply that Blueprinty’s user base skews younger and more regionally concentrated, which could influence other observed differences, such as patent counts, and should be taken into account when interpreting any causal claims.\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\nThe likelihood function for a set of independent observations \\(Y_1\\), \\(Y_2\\), \\(\\dots\\), \\(Y_n\\), each following a Poisson distribution with mean , is given by:\n\\[\nL(\\lambda) = \\prod_{i=1}^{n} \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n\\]\nTaking the logarithm of the likelihood function gives the log-likelihood, which is more convenient to work with mathematically:\n\\[\n\\log L(\\lambda) = \\sum_{i=1}^{n} \\left( -\\lambda + Y_i \\log \\lambda - \\log Y_i! \\right)\n\\]\n\n\n\nimport numpy as np\nfrom scipy.special import gammaln \n\ndef poisson_loglikelihood(lam, Y):\n    if lam &lt;= 0:\n        return -np.inf\n    \n    Y = np.asarray(Y)\n    log_likelihood = np.sum(-lam + Y * np.log(lam) - gammaln(Y + 1))\n    return log_likelihood\n\nY = blueprinty['patents']\n\nlambda_values = np.linspace(0.1, 10, 200)\nloglikelihood_values = [poisson_loglikelihood(lam, Y) for lam in lambda_values]\n\nplt.figure(figsize=(8, 5))\nplt.plot(lambda_values, loglikelihood_values)\nplt.xlabel(\"λ (Lambda)\")\nplt.ylabel(\"Log-Likelihood\")\nplt.title(\"Poisson Log-Likelihood as a Function of λ\")\nplt.grid(True)\nplt.show()\n\n\n\nPoisson Model\n\n\nThe shape of the curve illustrates how the log-likelihood sharply decreases as \\(\\lambda\\) moves away from the mean in either direction. This provides intuition for why the mean is the most “likely” value under a Poisson process.\nTo find the maximum likelihood estimator (MLE) for \\(\\lambda\\) in a Poisson model, we start with the log-likelihood function:\n\\[\n\\log L(\\lambda) = \\sum_{i=1}^{n} \\left( -\\lambda + Y_i \\log \\lambda - \\log Y_i! \\right)\n\\]\nTaking the derivative of the log-likelihood with respect to \\(\\lambda\\):\n\\[\n\\frac{d}{d\\lambda} \\log L(\\lambda) = \\sum_{i=1}^{n} \\left( -1 + \\frac{Y_i}{\\lambda} \\right)\n\\]\nSetting the derivative equal to zero to find the maximum:\n\\[\n\\sum_{i=1}^{n} \\left( -1 + \\frac{Y_i}{\\lambda} \\right) = 0\n\\]\n\\[\n- n + \\frac{1}{\\lambda} \\sum_{i=1}^{n} Y_i = 0\n\\]\n\\[\n\\lambda = \\frac{1}{n} \\sum_{i=1}^{n} Y_i = \\bar{Y}\n\\]\nThe MLE of \\(\\lambda\\) is the sample mean of the observed values:\n\\[\n\\hat{\\lambda}_{\\text{MLE}} = \\bar{Y}\n\\]\nThis result makes intuitive sense because, in a Poisson distribution, the mean and the variance are both equal to \\(\\lambda\\). Therefore, using the sample mean as an estimate of \\(\\lambda\\) is not only mathematically valid, but also conceptually natural.\nfrom scipy.optimize import minimize_scalar\n\nobjective = lambda lam: -poisson_loglikelihood(lam, blueprinty['patents'])\n\nresult = minimize_scalar(objective, bounds=(0.1, 10), method='bounded')\nlambda_mle = result.x\nprint(\"Estimated λ (MLE):\", lambda_mle)\nTo obtain the maximum likelihood estimate (MLE) of \\(\\lambda\\) for the Poisson model, we used numerical optimization to maximize the log-likelihood function based on the observed number of patents.\nUsing Python’s scipy.optimize.minimize_scalar, we minimized the negative log-likelihood over a plausible range of \\(\\lambda\\) values:\n\nOptimization interval: \\(\\lambda\\) in [0.1, 10]\nOptimization method: ‘bounded’\n\nThe result of this optimization gives:\n\\[\n\\hat{\\lambda}_{\\text{MLE}} \\approx 3.685\n\\]\nThis estimate aligns closely with the sample mean of the patent counts, which is consistent with the theoretical property of the Poisson distribution: its mean equals the MLE for \\(\\lambda\\). This numerical confirmation reinforces the earlier analytical result that \\(\\hat{\\lambda} = \\bar{Y}\\).\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\nWe estimate a Poisson regression model to study how firm characteristics relate to the number of patents awarded. The dependent variable is the number of patents a firm received over the last five years. The key covariates include:\n\nage: Years since firm incorporation\n\nage_squared: To allow for non-linear age effects\n\nregion: Dummy-coded categorical variable for firm location\n\niscustomer: A binary indicator of whether the firm uses Blueprinty’s software\n\nWe model:\n\\[\nY_i \\sim \\text{Poisson}(\\lambda_i), \\quad \\lambda_i = \\exp(X_i' \\beta)\n\\]\nThis model assumes that each firm’s patent count \\(Y_i\\) follows a Poisson distribution with a rate \\(\\lambda_i\\) that depends on firm-level covariates.\nWe construct the design matrix ( X ) with an intercept and the variables described above. To estimate the model, we define a log-likelihood function and use scipy.optimize.minimize() to maximize it:\nfrom scipy.optimize import minimize\nfrom scipy.special import gammaln\ndef poisson_regression_loglikelihood(beta, Y, X):\n    eta = X @ beta\n    eta = np.clip(eta, -100, 100)\n    lambda_i = np.exp(eta)\n    log_lik = np.sum(-lambda_i + Y * eta - gammaln(Y + 1))\n    return log_lik\n\n# Build design matrix\nblueprinty['age_squared'] = blueprinty['age'] ** 2\nX = pd.concat([\n    pd.Series(1.0, index=blueprinty.index, name='intercept'),\n    blueprinty[['age', 'age_squared', 'iscustomer']],\n    pd.get_dummies(blueprinty['region'], drop_first=True)\n], axis=1)\nX_matrix = X.astype(float).values\nY = blueprinty['patents'].values\n\n# Maximize log-likelihood\ninit_beta = np.zeros(X_matrix.shape[1])\nresult = minimize(\n    fun=lambda beta: -poisson_regression_loglikelihood(beta, Y, X_matrix),\n    x0=init_beta,\n    method='BFGS'\n)\n\n# Extract estimates\nbeta_hat = result.x\nhessian_inv = result.hess_inv\nstandard_errors = np.sqrt(np.diag(hessian_inv))\nsummary_table = pd.DataFrame({\n    'Variable': X.columns,\n    'Coefficient (β)': beta_hat,\n    'Std. Error': standard_errors\n})\nsummary_table\nThe table below summarizes the coefficient estimates and standard errors obtained from the manually implemented Poisson regression using maximum likelihood estimation (MLE):\n\n\n\nVariable\nCoefficient (β)\nStd. Error\n\n\n\n\nintercept\n-0.510\n0.194\n\n\nage\n0.149\n0.015\n\n\nage_squared\n-0.003\n0.00029\n\n\niscustomer\n0.208\n0.029\n\n\nNortheast\n0.030\n0.033\n\n\nNorthwest\n-0.018\n0.025\n\n\nSouth\n0.057\n0.025\n\n\nSouthwest\n0.051\n0.039\n\n\n\n\nIntercept (−0.51): This represents the expected log-count of patents for a firm with age = 0 and all binary indicators equal to zero (i.e., not a customer and in the baseline region—likely Midwest). While not directly interpretable, it serves as the model’s baseline offset.\nAge (0.149) and Age Squared (−0.003): The combination of a positive linear age term and a small negative quadratic term suggests an inverted-U shaped relationship between firm age and patent output. Patent counts increase with age but at a decreasing rate, eventually leveling off or declining.\nIs Customer (0.208): Being a Blueprinty customer is associated with higher patent productivity. Since this is a log-linear model, we interpret the effect as:\n\\[\n\\exp(0.208) \\approx 1.23\n\\]\nThat is, Blueprinty customers have approximately 23% more expected patents than non-customers, controlling for other covariates.\nRegional Effects: Relative to the omitted region (likely Midwest), the region dummies show:\n\nNortheast: Slightly higher expected patent counts, though not strongly significant.\nNorthwest: Slightly lower than the baseline.\nSouth and Southwest: Positive coefficients, but again relatively modest in size.\n\n\n\nTo check results with sm.GLM() function:\nimport statsmodels.api as sm\n\nmodel = sm.GLM(Y, X, family=sm.families.Poisson())\nresult = model.fit()\n\nprint(result.summary())\nThe Poisson regression using statsmodels.GLM() confirms several key findings:\n\nAge has a positive and significant coefficient (0.1486), while age_squared is negative (−0.0030), indicating an inverted U-shaped relationship between firm age and patent output.\niscustomer (0.2076) is positive and statistically significant (p &lt; 0.001), suggesting that Blueprinty customers tend to receive more patents than non-customers, controlling for age and region.\nRegional dummy variables show small and mostly non-significant differences compared to the omitted region (likely Midwest).\n\nBecause this is a log-linear model, we interpret the iscustomer effect as:\n\\[\n\\exp(0.2076) \\approx 1.23\n\\]\nThis means Blueprinty customers are expected to have about 23% more patents than non-customers, holding other factors constant.\nOverall, the model captures important effects of age, customer status, and location on firm patent output. These results reinforce the idea that using Blueprinty’s software is associated with increased patenting success, particularly among firms of moderate age.\nTo estimate the average treatment effect of using Blueprinty’s software, we simulate patent predictions under two scenarios:\n\nAll firms are non-customers (iscustomer = 0)\nAll firms are customers (iscustomer = 1)\n\nWe use the fitted Poisson model to predict patent counts in both cases and compare the results.\nX_0 = X.copy()\nX_1 = X.copy()\nX_0['iscustomer'] = 0\nX_1['iscustomer'] = 1\n\ny_pred_0 = result.predict(X_0)\ny_pred_1 = result.predict(X_1)\n\naverage_effect = np.mean(y_pred_1 - y_pred_0)\nprint(\"Average expected increase in patents:\", average_effect)\nThe average firm is expected to receive approximately 0.79 more patents over five years if it adopts Blueprinty’s software, compared to if it does not—holding all else constant."
  },
  {
    "objectID": "blog/project1/hw2_questions.html#blueprinty-case-study",
    "href": "blog/project1/hw2_questions.html#blueprinty-case-study",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\nThis dataset contains information on 1,500 mature engineering firms and is intended to help evaluate whether the use of Blueprinty software is associated with a higher number of awarded patents. The dataset includes four columns:\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\npatents\nNumber of patents awarded to the firm in the past five years (integer).\n\n\nregion\nThe geographic region where the firm is located (categorical).\n\n\nage\nThe age of the firm since incorporation, in years (float).\n\n\niscustomer\nBinary indicator of whether the firm uses Blueprinty software (0 = no, 1 = yes).\n\n\n\n\n\n\n\nTotal observations: 1,500 firms\n\nNo missing values\nData types:\n\npatents: int64\n\nregion: object (categorical)\n\nage: float64\n\niscustomer: int64 (binary)\n\n\nTo explore whether firms using Blueprinty’s software tend to have more patents, we compared the distribution of patent counts between customers and non-customers.\nThe histograms below show the number of patents awarded to firms, segmented by whether or not they are Blueprinty customers (iscustomer = 1 or 0):\n\n\n\nHistogram by iscustomer\n\n\nVisually, we observe that Blueprinty customers tend to have slightly more patents than non-customers. The distribution for customers is shifted to the right and shows a longer tail, indicating a higher frequency of firms with larger patent counts.\n\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nblueprinty = pd.read_csv('blueprinty.csv')\ng = sns.displot(\n    data=blueprinty,\n    x=\"patents\",\n    col=\"iscustomer\",  \n    kind=\"hist\",\n    bins=30,\n    kde=True,\n    height=4,\n    aspect=1.5,\n    facet_kws={'margin_titles': True}\n)\n\ng.set_axis_labels(\"Number of Patents\", \"Count\")\ng.set_titles(col_template=\"Customer: {col_name}\")\nplt.subplots_adjust(top=0.8)\nfor ax in g.axes.flatten():\n    ax.tick_params(labelbottom=True)\ng.fig.suptitle(\"Patent Distribution by Customer Status\")\nTo confirm this pattern numerically, we calculate the average number of patents for each group:\npatent_mean_bycustomer = blueprinty.groupby('iscustomer')['patents'].mean()\npatent_mean_bycustomer\nThe results show:\n\nNon-customers: 3.47 patents on average\nCustomers: 4.13 patents on average\n\nThis difference supports the notion that customer firms, on average, are more productive in securing patents.\nWhile this descriptive analysis does not establish a causal link, the pattern suggests a positive association between using Blueprinty and higher patent output.\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\nFrom the regional distribution, we observe that Blueprinty customers are most concentrated in the Northeast, where their count even exceeds that of non-customers. In contrast, in regions such as the Midwest, Northwest, and South, non-customer firms are significantly more numerous. This suggests that Blueprinty may have a stronger market presence or outreach in the Northeast region.\n\n\n\nRegion Distribution\n\n\nIn terms of firm age, Blueprinty customers tend to be younger. The majority of customer firms fall into the ≤20 and 21–30 age groups, while very few are in the 41–50 or &gt;50 categories. Although non-customer firms also peak in the 21–30 group, they are more evenly distributed across older age groups compared to customers.\n\n\n\nAge Distribution\n\n\nThese patterns imply that Blueprinty’s user base skews younger and more regionally concentrated, which could influence other observed differences, such as patent counts, and should be taken into account when interpreting any causal claims.\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\nThe likelihood function for a set of independent observations \\(Y_1\\), \\(Y_2\\), \\(\\dots\\), \\(Y_n\\), each following a Poisson distribution with mean , is given by:\n\\[\nL(\\lambda) = \\prod_{i=1}^{n} \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n\\]\nTaking the logarithm of the likelihood function gives the log-likelihood, which is more convenient to work with mathematically:\n\\[\n\\log L(\\lambda) = \\sum_{i=1}^{n} \\left( -\\lambda + Y_i \\log \\lambda - \\log Y_i! \\right)\n\\]\n\n\n\nimport numpy as np\nfrom scipy.special import gammaln \n\ndef poisson_loglikelihood(lam, Y):\n    if lam &lt;= 0:\n        return -np.inf\n    \n    Y = np.asarray(Y)\n    log_likelihood = np.sum(-lam + Y * np.log(lam) - gammaln(Y + 1))\n    return log_likelihood\n\nY = blueprinty['patents']\n\nlambda_values = np.linspace(0.1, 10, 200)\nloglikelihood_values = [poisson_loglikelihood(lam, Y) for lam in lambda_values]\n\nplt.figure(figsize=(8, 5))\nplt.plot(lambda_values, loglikelihood_values)\nplt.xlabel(\"λ (Lambda)\")\nplt.ylabel(\"Log-Likelihood\")\nplt.title(\"Poisson Log-Likelihood as a Function of λ\")\nplt.grid(True)\nplt.show()\n\n\n\nPoisson Model\n\n\nThe shape of the curve illustrates how the log-likelihood sharply decreases as \\(\\lambda\\) moves away from the mean in either direction. This provides intuition for why the mean is the most “likely” value under a Poisson process.\nTo find the maximum likelihood estimator (MLE) for \\(\\lambda\\) in a Poisson model, we start with the log-likelihood function:\n\\[\n\\log L(\\lambda) = \\sum_{i=1}^{n} \\left( -\\lambda + Y_i \\log \\lambda - \\log Y_i! \\right)\n\\]\nTaking the derivative of the log-likelihood with respect to \\(\\lambda\\):\n\\[\n\\frac{d}{d\\lambda} \\log L(\\lambda) = \\sum_{i=1}^{n} \\left( -1 + \\frac{Y_i}{\\lambda} \\right)\n\\]\nSetting the derivative equal to zero to find the maximum:\n\\[\n\\sum_{i=1}^{n} \\left( -1 + \\frac{Y_i}{\\lambda} \\right) = 0\n\\]\n\\[\n- n + \\frac{1}{\\lambda} \\sum_{i=1}^{n} Y_i = 0\n\\]\n\\[\n\\lambda = \\frac{1}{n} \\sum_{i=1}^{n} Y_i = \\bar{Y}\n\\]\nThe MLE of \\(\\lambda\\) is the sample mean of the observed values:\n\\[\n\\hat{\\lambda}_{\\text{MLE}} = \\bar{Y}\n\\]\nThis result makes intuitive sense because, in a Poisson distribution, the mean and the variance are both equal to \\(\\lambda\\). Therefore, using the sample mean as an estimate of \\(\\lambda\\) is not only mathematically valid, but also conceptually natural.\nfrom scipy.optimize import minimize_scalar\n\nobjective = lambda lam: -poisson_loglikelihood(lam, blueprinty['patents'])\n\nresult = minimize_scalar(objective, bounds=(0.1, 10), method='bounded')\nlambda_mle = result.x\nprint(\"Estimated λ (MLE):\", lambda_mle)\nTo obtain the maximum likelihood estimate (MLE) of \\(\\lambda\\) for the Poisson model, we used numerical optimization to maximize the log-likelihood function based on the observed number of patents.\nUsing Python’s scipy.optimize.minimize_scalar, we minimized the negative log-likelihood over a plausible range of \\(\\lambda\\) values:\n\nOptimization interval: \\(\\lambda\\) in [0.1, 10]\nOptimization method: ‘bounded’\n\nThe result of this optimization gives:\n\\[\n\\hat{\\lambda}_{\\text{MLE}} \\approx 3.685\n\\]\nThis estimate aligns closely with the sample mean of the patent counts, which is consistent with the theoretical property of the Poisson distribution: its mean equals the MLE for \\(\\lambda\\). This numerical confirmation reinforces the earlier analytical result that \\(\\hat{\\lambda} = \\bar{Y}\\).\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\nWe estimate a Poisson regression model to study how firm characteristics relate to the number of patents awarded. The dependent variable is the number of patents a firm received over the last five years. The key covariates include:\n\nage: Years since firm incorporation\n\nage_squared: To allow for non-linear age effects\n\nregion: Dummy-coded categorical variable for firm location\n\niscustomer: A binary indicator of whether the firm uses Blueprinty’s software\n\nWe model:\n\\[\nY_i \\sim \\text{Poisson}(\\lambda_i), \\quad \\lambda_i = \\exp(X_i' \\beta)\n\\]\nThis model assumes that each firm’s patent count \\(Y_i\\) follows a Poisson distribution with a rate \\(\\lambda_i\\) that depends on firm-level covariates.\nWe construct the design matrix ( X ) with an intercept and the variables described above. To estimate the model, we define a log-likelihood function and use scipy.optimize.minimize() to maximize it:\nfrom scipy.optimize import minimize\nfrom scipy.special import gammaln\ndef poisson_regression_loglikelihood(beta, Y, X):\n    eta = X @ beta\n    eta = np.clip(eta, -100, 100)\n    lambda_i = np.exp(eta)\n    log_lik = np.sum(-lambda_i + Y * eta - gammaln(Y + 1))\n    return log_lik\n\n# Build design matrix\nblueprinty['age_squared'] = blueprinty['age'] ** 2\nX = pd.concat([\n    pd.Series(1.0, index=blueprinty.index, name='intercept'),\n    blueprinty[['age', 'age_squared', 'iscustomer']],\n    pd.get_dummies(blueprinty['region'], drop_first=True)\n], axis=1)\nX_matrix = X.astype(float).values\nY = blueprinty['patents'].values\n\n# Maximize log-likelihood\ninit_beta = np.zeros(X_matrix.shape[1])\nresult = minimize(\n    fun=lambda beta: -poisson_regression_loglikelihood(beta, Y, X_matrix),\n    x0=init_beta,\n    method='BFGS'\n)\n\n# Extract estimates\nbeta_hat = result.x\nhessian_inv = result.hess_inv\nstandard_errors = np.sqrt(np.diag(hessian_inv))\nsummary_table = pd.DataFrame({\n    'Variable': X.columns,\n    'Coefficient (β)': beta_hat,\n    'Std. Error': standard_errors\n})\nsummary_table\nThe table below summarizes the coefficient estimates and standard errors obtained from the manually implemented Poisson regression using maximum likelihood estimation (MLE):\n\n\n\nVariable\nCoefficient (β)\nStd. Error\n\n\n\n\nintercept\n-0.510\n0.194\n\n\nage\n0.149\n0.015\n\n\nage_squared\n-0.003\n0.00029\n\n\niscustomer\n0.208\n0.029\n\n\nNortheast\n0.030\n0.033\n\n\nNorthwest\n-0.018\n0.025\n\n\nSouth\n0.057\n0.025\n\n\nSouthwest\n0.051\n0.039\n\n\n\n\nIntercept (−0.51): This represents the expected log-count of patents for a firm with age = 0 and all binary indicators equal to zero (i.e., not a customer and in the baseline region—likely Midwest). While not directly interpretable, it serves as the model’s baseline offset.\nAge (0.149) and Age Squared (−0.003): The combination of a positive linear age term and a small negative quadratic term suggests an inverted-U shaped relationship between firm age and patent output. Patent counts increase with age but at a decreasing rate, eventually leveling off or declining.\nIs Customer (0.208): Being a Blueprinty customer is associated with higher patent productivity. Since this is a log-linear model, we interpret the effect as:\n\\[\n\\exp(0.208) \\approx 1.23\n\\]\nThat is, Blueprinty customers have approximately 23% more expected patents than non-customers, controlling for other covariates.\nRegional Effects: Relative to the omitted region (likely Midwest), the region dummies show:\n\nNortheast: Slightly higher expected patent counts, though not strongly significant.\nNorthwest: Slightly lower than the baseline.\nSouth and Southwest: Positive coefficients, but again relatively modest in size.\n\n\n\nTo check results with sm.GLM() function:\nimport statsmodels.api as sm\n\nmodel = sm.GLM(Y, X, family=sm.families.Poisson())\nresult = model.fit()\n\nprint(result.summary())\nThe Poisson regression using statsmodels.GLM() confirms several key findings:\n\nAge has a positive and significant coefficient (0.1486), while age_squared is negative (−0.0030), indicating an inverted U-shaped relationship between firm age and patent output.\niscustomer (0.2076) is positive and statistically significant (p &lt; 0.001), suggesting that Blueprinty customers tend to receive more patents than non-customers, controlling for age and region.\nRegional dummy variables show small and mostly non-significant differences compared to the omitted region (likely Midwest).\n\nBecause this is a log-linear model, we interpret the iscustomer effect as:\n\\[\n\\exp(0.2076) \\approx 1.23\n\\]\nThis means Blueprinty customers are expected to have about 23% more patents than non-customers, holding other factors constant.\nOverall, the model captures important effects of age, customer status, and location on firm patent output. These results reinforce the idea that using Blueprinty’s software is associated with increased patenting success, particularly among firms of moderate age.\nTo estimate the average treatment effect of using Blueprinty’s software, we simulate patent predictions under two scenarios:\n\nAll firms are non-customers (iscustomer = 0)\nAll firms are customers (iscustomer = 1)\n\nWe use the fitted Poisson model to predict patent counts in both cases and compare the results.\nX_0 = X.copy()\nX_1 = X.copy()\nX_0['iscustomer'] = 0\nX_1['iscustomer'] = 1\n\ny_pred_0 = result.predict(X_0)\ny_pred_1 = result.predict(X_1)\n\naverage_effect = np.mean(y_pred_1 - y_pred_0)\nprint(\"Average expected increase in patents:\", average_effect)\nThe average firm is expected to receive approximately 0.79 more patents over five years if it adopts Blueprinty’s software, compared to if it does not—holding all else constant."
  },
  {
    "objectID": "blog/project1/hw2_questions.html#airbnb-case-study",
    "href": "blog/project1/hw2_questions.html#airbnb-case-study",
    "title": "Poisson Regression Examples",
    "section": "AirBnB Case Study",
    "text": "AirBnB Case Study\n\nIntroduction\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City. The data include the following variables:\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n- `id` = unique ID number for each unit\n- `last_scraped` = date when information scraped\n- `host_since` = date when host first listed the unit on Airbnb\n- `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n- `room_type` = Entire home/apt., Private room, or Shared room\n- `bathrooms` = number of bathrooms\n- `bedrooms` = number of bedrooms\n- `price` = price per night (dollars)\n- `number_of_reviews` = number of reviews for the unit on Airbnb\n- `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n- `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n- `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n- `instant_bookable` = \"t\" if instantly bookable, \"f\" if not\n\n\n\n\n\nExploratory Data Analysis\nWe begin by exploring the distribution of key variables in the Airbnb dataset, focusing on room_type, number_of_reviews, and price.\n\nThe distribution of room types shows that the dataset is dominated by two categories: Entire home/apt and Private room, each with approximately 19,000–20,000 listings. Shared room listings are comparatively rare. \nThe number of reviews is highly right-skewed, with most listings receiving fewer than 50 reviews. A small number of listings receive over 100 reviews. \nThe price distribution is also strongly skewed to the right. While most listings are priced below $500 per night, a few extreme values exceed $1,000, with some outliers above $5,000 or even $10,000. \n\n\n\nMissing Data Overview\nWe also inspect the dataset for missing values. As shown below, some key review-related variables contain substantial missing data:\n\nreview_scores_cleanliness has ~10,195 missing entries\nreview_scores_location has ~10,254 missing entries\nreview_scores_value has ~10,256 missing entries\nbathrooms and bedrooms have 160 and 76 missing respectively\nOther important predictors such as price, room_type, and number_of_reviews have no missing values\n\nWe will consider dropping rows with missing values on relevant columns. We consider number of reviews a good proxy as the number of bookings, so next we will find relationships between reviews number and other conditions.\n\n\nPoisson Regression Results: Interpreting Coefficients\nWe fit a Poisson regression model to predict the number of reviews (as a proxy for number of bookings). The independent variables include price, room type, review cleanliness score, days since listed, and whether the listing is instantly bookable.\nBelow is the interpretation of each coefficient from the GLM output:\n\nIntercept (2.5122)\nThis is the baseline log-expected number of reviews when all numeric variables are 0 and categorical variables are at their reference level (e.g., room type = Entire home/apt, instant_bookable = False).\nExponentiating gives \\(e^{2.51} ≈ 12.3\\): the baseline expected number of reviews is around 12.\nPrice (−3.595e−05)\nA very small but significant negative coefficient.\nThis suggests that for each additional dollar in price, the expected number of reviews decreases slightly.\nAlthough small in magnitude, it’s statistically significant (p &lt; 0.001).\nReview Scores Cleanliness (+0.0455)\nThis is positive and highly significant.\nFor each 1-point increase in the cleanliness score, the expected number of reviews increases by approximately 4.7%: \\(\\exp(0.0455) ≈ 1.047\\)\nDays Listed (+5.085e−05)\nAlso positive and very significant.\nListings that have been active longer receive more reviews, which is expected.\nEach additional day listed increases expected reviews by ~0.005%.\nRoom Type: Private Room (−0.0192)\nCompared to Entire home/apt, private rooms have fewer reviews (−1.9%).\nThis effect is small but significant.\nRoom Type: Shared Room (−0.2450)\nShared rooms receive far fewer reviews than entire apartments.\n\\(\\exp(−0.245) ≈ 0.782\\)：about 21.8% fewer reviews than the baseline.\nInstant Bookable: True (+0.3574)\nThis variable has a strong positive effect.\n\\(\\exp(0.357) ≈ 1.43\\)：listings that allow instant booking receive about 43% more reviews than those that don’t.\n\ncols = ['number_of_reviews', 'price', 'room_type', 'review_scores_cleanliness', 'instant_bookable', 'days']\n\ndf = airbnb[cols].dropna()\ndf = pd.get_dummies(df, columns=['room_type', 'instant_bookable'], drop_first=True)\n\nX = df.drop(columns=['number_of_reviews'])\nX = sm.add_constant(X)\nX = X.astype(float)\ny = df['number_of_reviews']\n\npoisson_model = sm.GLM(y, X, family=sm.families.Poisson())\nresult = poisson_model.fit()\nprint(result.summary())\n\n\n\nSummary\n\nCleanliness, longer listing duration, and instant bookability are strong positive predictors of booking activity (as measured by number of reviews).\nHigher prices and shared rooms are associated with fewer reviews.\nAll predictors are statistically significant with p-values &lt; 0.001.\n\nThese results provide insight into what types of listings attract more bookings and help hosts optimize for more engagement."
  },
  {
    "objectID": "blog/project1/homework2.html",
    "href": "blog/project1/homework2.html",
    "title": "Cheng-Yuan Wu's Website",
    "section": "",
    "text": "Show Code\nimport pandas as pd\nimport numpy as np\n\n\n\n\nShow Code\nairbnb = pd.read_csv('airbnb.csv')\nblueprinty = pd.read_csv('blueprinty.csv')\n\n\n\n\nShow Code\nairbnb.head()\n\n\n\n\n\n\n\n\n\nUnnamed: 0\nid\ndays\nlast_scraped\nhost_since\nroom_type\nbathrooms\nbedrooms\nprice\nnumber_of_reviews\nreview_scores_cleanliness\nreview_scores_location\nreview_scores_value\ninstant_bookable\n\n\n\n\n0\n1\n2515\n3130\n4/2/2017\n9/6/2008\nPrivate room\n1.0\n1.0\n59\n150\n9.0\n9.0\n9.0\nf\n\n\n1\n2\n2595\n3127\n4/2/2017\n9/9/2008\nEntire home/apt\n1.0\n0.0\n230\n20\n9.0\n10.0\n9.0\nf\n\n\n2\n3\n3647\n3050\n4/2/2017\n11/25/2008\nPrivate room\n1.0\n1.0\n150\n0\nNaN\nNaN\nNaN\nf\n\n\n3\n4\n3831\n3038\n4/2/2017\n12/7/2008\nEntire home/apt\n1.0\n1.0\n89\n116\n9.0\n9.0\n9.0\nf\n\n\n4\n5\n4611\n3012\n4/2/2017\n1/2/2009\nPrivate room\nNaN\n1.0\n39\n93\n9.0\n8.0\n9.0\nt\n\n\n\n\n\n\n\n\n\nShow Code\nblueprinty.head()\n\n\n\n\n\n\n\n\n\npatents\nregion\nage\niscustomer\n\n\n\n\n0\n0\nMidwest\n32.5\n0\n\n\n1\n3\nSouthwest\n37.5\n0\n\n\n2\n4\nNorthwest\n27.0\n1\n\n\n3\n3\nNortheast\n24.5\n0\n\n\n4\n3\nSouthwest\n37.0\n0\n\n\n\n\n\n\n\n\n\nShow Code\nblueprinty.info()\nblueprinty\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1500 entries, 0 to 1499\nData columns (total 4 columns):\n #   Column      Non-Null Count  Dtype  \n---  ------      --------------  -----  \n 0   patents     1500 non-null   int64  \n 1   region      1500 non-null   object \n 2   age         1500 non-null   float64\n 3   iscustomer  1500 non-null   int64  \ndtypes: float64(1), int64(2), object(1)\nmemory usage: 47.0+ KB\n\n\n\n\n\n\n\n\n\npatents\nregion\nage\niscustomer\n\n\n\n\n0\n0\nMidwest\n32.5\n0\n\n\n1\n3\nSouthwest\n37.5\n0\n\n\n2\n4\nNorthwest\n27.0\n1\n\n\n3\n3\nNortheast\n24.5\n0\n\n\n4\n3\nSouthwest\n37.0\n0\n\n\n...\n...\n...\n...\n...\n\n\n1495\n2\nNortheast\n18.5\n1\n\n\n1496\n3\nSouthwest\n22.5\n0\n\n\n1497\n4\nSouthwest\n17.0\n0\n\n\n1498\n3\nSouth\n29.0\n0\n\n\n1499\n1\nSouth\n39.0\n0\n\n\n\n\n1500 rows × 4 columns\n\n\n\n\n\nShow Code\nairbnb.info()\nairbnb.describe()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 5 entries, 0 to 4\nData columns (total 14 columns):\n #   Column                     Non-Null Count  Dtype  \n---  ------                     --------------  -----  \n 0   Unnamed: 0                 5 non-null      int64  \n 1   id                         5 non-null      int64  \n 2   days                       5 non-null      int64  \n 3   last_scraped               5 non-null      object \n 4   host_since                 5 non-null      object \n 5   room_type                  5 non-null      object \n 6   bathrooms                  4 non-null      float64\n 7   bedrooms                   5 non-null      float64\n 8   price                      5 non-null      int64  \n 9   number_of_reviews          5 non-null      int64  \n 10  review_scores_cleanliness  4 non-null      float64\n 11  review_scores_location     4 non-null      float64\n 12  review_scores_value        4 non-null      float64\n 13  instant_bookable           5 non-null      object \ndtypes: float64(5), int64(5), object(4)\nmemory usage: 688.0+ bytes\n\n\n\n\n\n\n\n\n\nUnnamed: 0\nid\ndays\nbathrooms\nbedrooms\nprice\nnumber_of_reviews\nreview_scores_cleanliness\nreview_scores_location\nreview_scores_value\n\n\n\n\ncount\n5.000000\n5.000000\n5.000000\n4.0\n5.000000\n5.000000\n5.000000\n4.0\n4.000000\n4.0\n\n\nmean\n3.000000\n3439.800000\n3071.400000\n1.0\n0.800000\n113.400000\n75.800000\n9.0\n9.000000\n9.0\n\n\nstd\n1.581139\n885.525381\n53.914748\n0.0\n0.447214\n77.500323\n63.790281\n0.0\n0.816497\n0.0\n\n\nmin\n1.000000\n2515.000000\n3012.000000\n1.0\n0.000000\n39.000000\n0.000000\n9.0\n8.000000\n9.0\n\n\n25%\n2.000000\n2595.000000\n3038.000000\n1.0\n1.000000\n59.000000\n20.000000\n9.0\n8.750000\n9.0\n\n\n50%\n3.000000\n3647.000000\n3050.000000\n1.0\n1.000000\n89.000000\n93.000000\n9.0\n9.000000\n9.0\n\n\n75%\n4.000000\n3831.000000\n3127.000000\n1.0\n1.000000\n150.000000\n116.000000\n9.0\n9.250000\n9.0\n\n\nmax\n5.000000\n4611.000000\n3130.000000\n1.0\n1.000000\n230.000000\n150.000000\n9.0\n10.000000\n9.0\n\n\n\n\n\n\n\n\n\nShow Code\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.figure(figsize=(10, 6))\nplt.title('Patent Distribution')\nsns.histplot(blueprinty['patents'], bins=50, kde=True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nShow Code\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# 設定年齡分組（你可以依需要調整區間）\nblueprinty['age_group'] = pd.cut(blueprinty['age'], bins=[0, 20, 30, 40, 50, 100], \n                                 labels=['≤20', '21–30', '31–40', '41–50', '&gt;50'])\n\n# 畫出直方圖，按「是否為客戶」分 Facet，並用不同年齡分組顯示\ng = sns.displot(\n    data=blueprinty,\n    x=\"patents\",\n    col=\"region\",     \n    row=\"age_group\",         # 年齡分組\n    kind=\"hist\",\n    bins=30,\n    kde=True,\n    height=4,\n    aspect=1.5,\n    facet_kws={'margin_titles': True}\n)\n\ng.set_axis_labels(\"Number of Patents\", \"Count\")\ng.set_titles(col_template=\"Customer: {col_name}\")\ng.set_titles(col_template=\"Region: {col_name}\", row_template=\"Age Group: {row_name}\")\nplt.subplots_adjust(top=0.9)\nfor ax in g.axes.flatten():\n    ax.tick_params(labelbottom=True)\ng.fig.suptitle(\"Patent Distribution by Age Group and Region\", fontsize=16)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nShow Code\npatent_mean_bycustomer = blueprinty.groupby('iscustomer')['patents'].mean()\npatent_mean_bycustomer\n\n\niscustomer\n0    3.473013\n1    4.133056\nName: patents, dtype: float64\n\n\n\n\nShow Code\npatent_mean_byage = blueprinty.groupby('age_group')['patents'].mean()\npatent_mean_byage\n\n\n/var/folders/4_/b_ln9nlx3j19x_2nmv9t3rd40000gn/T/ipykernel_89268/1344393124.py:1: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  patent_mean_byage = blueprinty.groupby('age_group')['patents'].mean()\n\n\nage_group\n≤20      3.454277\n21–30    4.156695\n31–40    3.230769\n41–50    2.186047\n&gt;50           NaN\nName: patents, dtype: float64\n\n\n\n\nShow Code\n# 設定年齡分組（你可以依需要調整區間）\nblueprinty['age_group'] = pd.cut(blueprinty['age'], bins=[0, 20, 30, 40, 50, 100], \n                                 labels=['≤20', '21–30', '31–40', '41–50', '&gt;50'])\n\n# 畫出直方圖，按「是否為客戶」分 Facet，並用不同年齡分組顯示\ng = sns.displot(\n    data=blueprinty,\n    x=\"patents\",\n    col=\"iscustomer\",        # 是否為客戶：0 or 1\n    # row=\"age_group\",         # 年齡分組\n    kind=\"hist\",\n    bins=30,\n    kde=True,\n    height=4,\n    aspect=1.5,\n    facet_kws={'margin_titles': True}\n)\n\ng.set_axis_labels(\"Number of Patents\", \"Count\")\ng.set_titles(col_template=\"Customer: {col_name}\")\n# g.set_titles(col_template=\"Customer: {col_name}\", row_template=\"Age Group: {row_name}\")\nplt.subplots_adjust(top=0.8)\nfor ax in g.axes.flatten():\n    ax.tick_params(labelbottom=True)\ng.fig.suptitle(\"Patent Distribution by Customer Status\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nShow Code\nregion_counts = blueprinty.groupby(['iscustomer', 'region']).size().unstack()\nregion_counts\n\n\n\n\n\n\n\n\nregion\nMidwest\nNortheast\nNorthwest\nSouth\nSouthwest\n\n\niscustomer\n\n\n\n\n\n\n\n\n\n0\n187\n273\n158\n156\n245\n\n\n1\n37\n328\n29\n35\n52\n\n\n\n\n\n\n\n\n\nShow Code\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# 區域分布條形圖\nsns.countplot(data=blueprinty, x='region', hue='iscustomer',palette={0: \"lightgray\", 1: \"skyblue\"})\nplt.title('Region Distribution by Customer Status')\nplt.xticks(rotation=45)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nShow Code\nsns.countplot(data=blueprinty, x='age_group', hue='iscustomer',palette={0: \"lightgray\", 1: \"skyblue\"})\nplt.title('Age Distribution by Customer Status')\nplt.xticks(rotation=45)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nShow Code\nimport numpy as np\nfrom scipy.special import gammaln  # 更穩定地處理 log(Y!)\n\ndef poisson_loglikelihood(lam, Y):\n    \"\"\"\n    計算 Poisson 模型的 log-likelihood。\n    \n    參數:\n    lam: float, Poisson 分布的 λ 參數（事件的平均發生率）\n    Y: array-like, 一組觀察到的計數資料\n    \n    回傳:\n    總對數概似值（log-likelihood）\n    \"\"\"\n    if lam &lt;= 0:\n        return -np.inf  # λ 必須為正數，否則回傳負無限\n    \n    Y = np.asarray(Y)\n    log_likelihood = np.sum(-lam + Y * np.log(lam) - gammaln(Y + 1))\n    return log_likelihood\n\n\n\n\nShow Code\nY = blueprinty['patents']\n\nlambda_values = np.linspace(0.1, 10, 200)\nloglikelihood_values = [poisson_loglikelihood(lam, Y) for lam in lambda_values]\n\nplt.figure(figsize=(8, 5))\nplt.plot(lambda_values, loglikelihood_values)\nplt.xlabel(\"λ (Lambda)\")\nplt.ylabel(\"Log-Likelihood\")\nplt.title(\"Poisson Log-Likelihood as a Function of λ\")\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nShow Code\nfrom scipy.optimize import minimize_scalar\n\n# 目標函數要最大化，所以取 -loglikelihood 來做最小化\nobjective = lambda lam: -poisson_loglikelihood(lam, blueprinty['patents'])\n\nresult = minimize_scalar(objective, bounds=(0.1, 10), method='bounded')\nlambda_mle = result.x\nprint(\"Estimated λ (MLE):\", lambda_mle)\n\n\nEstimated λ (MLE): 3.6846662261327716\n\n\n\n\nShow Code\nimport numpy as np\nfrom scipy.special import gammaln\n\ndef poisson_regression_loglikelihood(beta, Y, X):\n    \"\"\"\n    計算 Poisson 回歸模型的 log-likelihood。\n    \n    參數：\n    - beta: 係數向量 (一維 array)，長度為 X 的欄數\n    - Y: 觀察值向量（每家公司有幾個專利）\n    - X: 共變量矩陣（每列代表一家公司的特徵）\n\n    回傳：\n    - 總對數概似值（log-likelihood）\n    \"\"\"\n    beta = np.asarray(beta)\n    Y = np.asarray(Y)\n    X = np.asarray(X)\n\n    # 計算每家公司自己的 lambda_i = exp(X_i · beta)\n    lambda_i = np.exp(X @ beta)\n\n    # 套用 Poisson log-likelihood 函數\n    log_lik = np.sum(-lambda_i + Y * np.log(lambda_i) - gammaln(Y + 1))\n\n    return log_lik\n\n\n\n\nShow Code\nfrom scipy.optimize import minimize\n\n# 建立 age_squared 欄位\nblueprinty['age_squared'] = blueprinty['age'] ** 2\n\n# 建立 region dummy variables（drop_first=True 是為了避免 multicollinearity）\nregion_dummies = pd.get_dummies(blueprinty['region'], drop_first=True)\n\n# 建立設計矩陣 X\nX = pd.concat([\n    pd.Series(1, index=blueprinty.index, name='intercept'),\n    blueprinty[['age', 'age_squared', 'iscustomer']],\n    region_dummies\n], axis=1)\nX = X.astype(float)\n# 目標變數 Y\nY = blueprinty['patents'].values\nX_matrix = X.values\n\n\n\n\nShow Code\nX_matrix\n\n\narray([[1.00000e+00, 3.25000e+01, 1.05625e+03, ..., 0.00000e+00,\n        0.00000e+00, 0.00000e+00],\n       [1.00000e+00, 3.75000e+01, 1.40625e+03, ..., 0.00000e+00,\n        0.00000e+00, 1.00000e+00],\n       [1.00000e+00, 2.70000e+01, 7.29000e+02, ..., 1.00000e+00,\n        0.00000e+00, 0.00000e+00],\n       ...,\n       [1.00000e+00, 1.70000e+01, 2.89000e+02, ..., 0.00000e+00,\n        0.00000e+00, 1.00000e+00],\n       [1.00000e+00, 2.90000e+01, 8.41000e+02, ..., 0.00000e+00,\n        1.00000e+00, 0.00000e+00],\n       [1.00000e+00, 3.90000e+01, 1.52100e+03, ..., 0.00000e+00,\n        1.00000e+00, 0.00000e+00]])\n\n\n\n\nShow Code\n# 初始值設定為 0 向量\ninit_beta = np.zeros(X_matrix.shape[1])\n\n# 改用明確傳參方式\nresult = minimize(\n    fun=lambda beta: -poisson_regression_loglikelihood(beta, Y, X_matrix),\n    x0=init_beta,\n    method='BFGS'\n)\n# 取出估計值與標準誤\nbeta_hat = result.x\nhessian_inv = result.hess_inv  # Hessian 反矩陣（共變異數矩陣）\nstandard_errors = np.sqrt(np.diag(hessian_inv))\n\n\n/var/folders/4_/b_ln9nlx3j19x_2nmv9t3rd40000gn/T/ipykernel_89268/3261295856.py:21: RuntimeWarning: overflow encountered in exp\n  lambda_i = np.exp(X @ beta)\n/var/folders/4_/b_ln9nlx3j19x_2nmv9t3rd40000gn/T/ipykernel_89268/3261295856.py:24: RuntimeWarning: invalid value encountered in multiply\n  log_lik = np.sum(-lambda_i + Y * np.log(lambda_i) - gammaln(Y + 1))\n/var/folders/4_/b_ln9nlx3j19x_2nmv9t3rd40000gn/T/ipykernel_89268/3261295856.py:24: RuntimeWarning: invalid value encountered in add\n  log_lik = np.sum(-lambda_i + Y * np.log(lambda_i) - gammaln(Y + 1))\n/Users/murphy_aloe/Library/Python/3.9/lib/python/site-packages/numpy/core/fromnumeric.py:88: RuntimeWarning: overflow encountered in reduce\n  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n/var/folders/4_/b_ln9nlx3j19x_2nmv9t3rd40000gn/T/ipykernel_89268/3261295856.py:21: RuntimeWarning: overflow encountered in exp\n  lambda_i = np.exp(X @ beta)\n/var/folders/4_/b_ln9nlx3j19x_2nmv9t3rd40000gn/T/ipykernel_89268/3261295856.py:24: RuntimeWarning: invalid value encountered in multiply\n  log_lik = np.sum(-lambda_i + Y * np.log(lambda_i) - gammaln(Y + 1))\n/var/folders/4_/b_ln9nlx3j19x_2nmv9t3rd40000gn/T/ipykernel_89268/3261295856.py:24: RuntimeWarning: invalid value encountered in add\n  log_lik = np.sum(-lambda_i + Y * np.log(lambda_i) - gammaln(Y + 1))\n/Users/murphy_aloe/Library/Python/3.9/lib/python/site-packages/numpy/core/fromnumeric.py:88: RuntimeWarning: overflow encountered in reduce\n  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n/var/folders/4_/b_ln9nlx3j19x_2nmv9t3rd40000gn/T/ipykernel_89268/3261295856.py:21: RuntimeWarning: overflow encountered in exp\n  lambda_i = np.exp(X @ beta)\n/var/folders/4_/b_ln9nlx3j19x_2nmv9t3rd40000gn/T/ipykernel_89268/3261295856.py:24: RuntimeWarning: invalid value encountered in multiply\n  log_lik = np.sum(-lambda_i + Y * np.log(lambda_i) - gammaln(Y + 1))\n/var/folders/4_/b_ln9nlx3j19x_2nmv9t3rd40000gn/T/ipykernel_89268/3261295856.py:24: RuntimeWarning: invalid value encountered in add\n  log_lik = np.sum(-lambda_i + Y * np.log(lambda_i) - gammaln(Y + 1))\n\n\n\n\nShow Code\n# 整理係數與標準誤\nsummary_table = pd.DataFrame({\n    'Variable': X.columns,\n    'Coefficient (β)': beta_hat,\n    'Std. Error': standard_errors\n})\nprint(summary_table)\n\n\n      Variable  Coefficient (β)  Std. Error\n0    intercept         1.480059         1.0\n1          age        38.016417         1.0\n2  age_squared      1033.539585         1.0\n3   iscustomer         0.553874         1.0\n4    Northeast         0.640979         1.0\n5    Northwest         0.164288         1.0\n6        South         0.181562         1.0\n7    Southwest         0.295497         1.0\n\n\n\n\nShow Code\n# Build design matrix\ndef poisson_regression_loglikelihood(beta, Y, X):\n    eta = X @ beta\n    eta = np.clip(eta, -100, 100)\n    lambda_i = np.exp(eta)\n\n    # 使用 where 保底防止 log(0)；同時限制 lambda_i 不為無限大\n    log_lik = np.sum(-lambda_i + Y * eta - gammaln(Y + 1))\n    return log_lik\n\nblueprinty['age_squared'] = blueprinty['age'] ** 2\nX = pd.concat([\n    pd.Series(1.0, index=blueprinty.index, name='intercept'),\n    blueprinty[['age', 'age_squared', 'iscustomer']],\n    pd.get_dummies(blueprinty['region'], drop_first=True)\n], axis=1)\nX_matrix = X.astype(float).values\nY = blueprinty['patents'].values\n\n# Maximize log-likelihood\ninit_beta = np.zeros(X_matrix.shape[1])\nresult = minimize(\n    fun=lambda beta: -poisson_regression_loglikelihood(beta, Y, X_matrix),\n    x0=init_beta,\n    method='BFGS'\n)\n\n# Extract estimates\nbeta_hat = result.x\nhessian_inv = result.hess_inv\nstandard_errors = np.sqrt(np.diag(hessian_inv))\nsummary_table = pd.DataFrame({\n    'Variable': X.columns,\n    'Coefficient (β)': beta_hat,\n    'Std. Error': standard_errors\n})\nsummary_table\n\n\n\n\n\n\n\n\n\nVariable\nCoefficient (β)\nStd. Error\n\n\n\n\n0\nintercept\n-0.509992\n0.193971\n\n\n1\nage\n0.148706\n0.015329\n\n\n2\nage_squared\n-0.002972\n0.000291\n\n\n3\niscustomer\n0.207609\n0.028616\n\n\n4\nNortheast\n0.029155\n0.032640\n\n\n5\nNorthwest\n-0.017578\n0.025004\n\n\n6\nSouth\n0.056565\n0.025004\n\n\n7\nSouthwest\n0.050567\n0.038745\n\n\n\n\n\n\n\n\n\nShow Code\nimport statsmodels.api as sm\n\nmodel = sm.GLM(Y, X, family=sm.families.Poisson())\nresult = model.fit()\n\n# 顯示完整結果\nprint(result.summary())\n\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:                      y   No. Observations:                 1500\nModel:                            GLM   Df Residuals:                     1492\nModel Family:                 Poisson   Df Model:                            7\nLink Function:                    Log   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -3258.1\nDate:                Fri, 02 May 2025   Deviance:                       2143.3\nTime:                        04:07:41   Pearson chi2:                 2.07e+03\nNo. Iterations:                     5   Pseudo R-squ. (CS):             0.1360\nCovariance Type:            nonrobust                                         \n===============================================================================\n                  coef    std err          z      P&gt;|z|      [0.025      0.975]\n-------------------------------------------------------------------------------\nintercept      -0.5089      0.183     -2.778      0.005      -0.868      -0.150\nage             0.1486      0.014     10.716      0.000       0.121       0.176\nage_squared    -0.0030      0.000    -11.513      0.000      -0.003      -0.002\niscustomer      0.2076      0.031      6.719      0.000       0.147       0.268\nNortheast       0.0292      0.044      0.669      0.504      -0.056       0.115\nNorthwest      -0.0176      0.054     -0.327      0.744      -0.123       0.088\nSouth           0.0566      0.053      1.074      0.283      -0.047       0.160\nSouthwest       0.0506      0.047      1.072      0.284      -0.042       0.143\n===============================================================================\n\n\n\n\nShow Code\nX.columns\n\n\nIndex(['intercept', 'age', 'age_squared', 'iscustomer', 'Northeast',\n       'Northwest', 'South', 'Southwest'],\n      dtype='object')\n\n\n\n\nShow Code\nX_0 = X.copy()\nX_1 = X.copy()\nX_0['iscustomer'] = 0\nX_1['iscustomer'] = 1\n\ny_pred_0 = result.predict(X_0)\ny_pred_1 = result.predict(X_1)\n\naverage_effect = np.mean(y_pred_1 - y_pred_0)\nprint(\"Average expected increase in patents:\", average_effect)\n\n\nAverage expected increase in patents: 0.7927680710452626\n\n\n\n\nShow Code\nairbnb.head()\n\n\n\n\n\n\n\n\n\nUnnamed: 0\nid\ndays\nlast_scraped\nhost_since\nroom_type\nbathrooms\nbedrooms\nprice\nnumber_of_reviews\nreview_scores_cleanliness\nreview_scores_location\nreview_scores_value\ninstant_bookable\n\n\n\n\n0\n1\n2515\n3130\n4/2/2017\n9/6/2008\nPrivate room\n1.0\n1.0\n59\n150\n9.0\n9.0\n9.0\nf\n\n\n1\n2\n2595\n3127\n4/2/2017\n9/9/2008\nEntire home/apt\n1.0\n0.0\n230\n20\n9.0\n10.0\n9.0\nf\n\n\n2\n3\n3647\n3050\n4/2/2017\n11/25/2008\nPrivate room\n1.0\n1.0\n150\n0\nNaN\nNaN\nNaN\nf\n\n\n3\n4\n3831\n3038\n4/2/2017\n12/7/2008\nEntire home/apt\n1.0\n1.0\n89\n116\n9.0\n9.0\n9.0\nf\n\n\n4\n5\n4611\n3012\n4/2/2017\n1/2/2009\nPrivate room\nNaN\n1.0\n39\n93\n9.0\n8.0\n9.0\nt"
  },
  {
    "objectID": "blog/project1/hw2_questions.html#missing-data-overview",
    "href": "blog/project1/hw2_questions.html#missing-data-overview",
    "title": "Poisson Regression Examples",
    "section": "Missing Data Overview",
    "text": "Missing Data Overview\nWe also inspect the dataset for missing values. As shown below, some key review-related variables contain substantial missing data: • review_scores_cleanliness has ~10,195 missing entries • review_scores_location has ~10,254 missing entries • review_scores_value has ~10,256 missing entries • bathrooms and bedrooms have 160 and 76 missing respectively • Other important predictors such as price, room_type, and number_of_reviews have no missing values\nWe will consider either dropping rows with missing values on relevant columns or imputing them, depending on the modeling strategy.\ntodo: Assume the number of reviews is a good proxy for the number of bookings. Perform some exploratory data analysis to get a feel for the data, handle or drop observations with missing values on relevant variables, build one or more models (e.g., a poisson regression model for the number of bookings as proxied by the number of reviews), and interpret model coefficients to describe variation in the number of reviews as a function of the variables provided."
  },
  {
    "objectID": "blog/Project3/hw3_questions.html",
    "href": "blog/Project3/hw3_questions.html",
    "title": "Multinomial Logit Model",
    "section": "",
    "text": "This blog expores two methods for estimating the MNL model: (1) via Maximum Likelihood, and (2) via a Bayesian approach using a Metropolis-Hastings MCMC algorithm."
  },
  {
    "objectID": "blog/Project3/hw3_questions.html#likelihood-for-the-multi-nomial-logit-mnl-model",
    "href": "blog/Project3/hw3_questions.html#likelihood-for-the-multi-nomial-logit-mnl-model",
    "title": "Multinomial Logit Model",
    "section": "1. Likelihood for the Multi-nomial Logit (MNL) Model",
    "text": "1. Likelihood for the Multi-nomial Logit (MNL) Model\nSuppose we have \\(i=1,\\ldots,n\\) consumers who each select exactly one product \\(j\\) from a set of \\(J\\) products. The outcome variable is the identity of the product chosen \\(y_i \\in \\{1, \\ldots, J\\}\\) or equivalently a vector of \\(J-1\\) zeros and \\(1\\) one, where the \\(1\\) indicates the selected product. For example, if the third product was chosen out of 3 products, then either \\(y=3\\) or \\(y=(0,0,1)\\) depending on how we want to represent it. Suppose also that we have a vector of data on each product \\(x_j\\) (eg, brand, price, etc.).\nWe model the consumer’s decision as the selection of the product that provides the most utility, and we’ll specify the utility function as a linear function of the product characteristics:\n\\[ U_{ij} = x_j'\\beta + \\epsilon_{ij} \\]\nwhere \\(\\epsilon_{ij}\\) is an i.i.d. extreme value error term.\nThe choice of the i.i.d. extreme value error term leads to a closed-form expression for the probability that consumer \\(i\\) chooses product \\(j\\):\n\\[ \\mathbb{P}_i(j) = \\frac{e^{x_j'\\beta}}{\\sum_{k=1}^Je^{x_k'\\beta}} \\]\nFor example, if there are 3 products, the probability that consumer \\(i\\) chooses product 3 is:\n\\[ \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{e^{x_1'\\beta} + e^{x_2'\\beta} + e^{x_3'\\beta}} \\]\nA clever way to write the individual likelihood function for consumer \\(i\\) is the product of the \\(J\\) probabilities, each raised to the power of an indicator variable (\\(\\delta_{ij}\\)) that indicates the chosen product:\n\\[ L_i(\\beta) = \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} = \\mathbb{P}_i(1)^{\\delta_{i1}} \\times \\ldots \\times \\mathbb{P}_i(J)^{\\delta_{iJ}}\\]\nNotice that if the consumer selected product \\(j=3\\), then \\(\\delta_{i3}=1\\) while \\(\\delta_{i1}=\\delta_{i2}=0\\) and the likelihood is:\n\\[ L_i(\\beta) = \\mathbb{P}_i(1)^0 \\times \\mathbb{P}_i(2)^0 \\times \\mathbb{P}_i(3)^1 = \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{\\sum_{k=1}^3e^{x_k'\\beta}} \\]\nThe joint likelihood (across all consumers) is the product of the \\(n\\) individual likelihoods:\n\\[ L_n(\\beta) = \\prod_{i=1}^n L_i(\\beta) = \\prod_{i=1}^n \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} \\]\nAnd the joint log-likelihood function is:\n\\[ \\ell_n(\\beta) = \\sum_{i=1}^n \\sum_{j=1}^J \\delta_{ij} \\log(\\mathbb{P}_i(j)) \\]"
  },
  {
    "objectID": "blog/Project3/hw3_questions.html#simulate-conjoint-data",
    "href": "blog/Project3/hw3_questions.html#simulate-conjoint-data",
    "title": "Multinomial Logit Model",
    "section": "2. Simulate Conjoint Data",
    "text": "2. Simulate Conjoint Data\nWe will simulate data from a conjoint experiment about video content streaming services. We elect to simulate 100 respondents, each completing 10 choice tasks, where they choose from three alternatives per task. For simplicity, there is not a “no choice” option; each simulated respondent must select one of the 3 alternatives.\nEach alternative is a hypothetical streaming offer consistent of three attributes: (1) brand is either Netflix, Amazon Prime, or Hulu; (2) ads can either be part of the experience, or it can be ad-free, and (3) price per month ranges from $4 to $32 in increments of $4.\nThe part-worths (ie, preference weights or beta parameters) for the attribute levels will be 1.0 for Netflix, 0.5 for Amazon Prime (with 0 for Hulu as the reference brand); -0.8 for included adverstisements (0 for ad-free); and -0.1*price so that utility to consumer \\(i\\) for hypothethical streaming service \\(j\\) is\n\\[\nu_{ij} = (1 \\times Netflix_j) + (0.5 \\times Prime_j) + (-0.8*Ads_j) - 0.1\\times Price_j + \\varepsilon_{ij}\n\\]\nwhere the variables are binary indicators and \\(\\varepsilon\\) is Type 1 Extreme Value (ie, Gumble) distributed.\nThe following code provides the simulation of the conjoint data.\n\n\n\n\n\n\nNote\n\n\n\n\n\nimport pandas as pd\nimport numpy as np\n\n# Set seed for reproducibility\nnp.random.seed(123)\n\n# Define attributes\nbrands = [\"N\", \"P\", \"H\"]  # Netflix, Prime, Hulu\nads = [\"Yes\", \"No\"]\nprices = np.arange(8, 33, 4)  # 8, 12, ..., 32\n\n# Create all possible profiles (full factorial)\nprofiles = pd.DataFrame([\n    (b, a, p) for b in brands for a in ads for p in prices\n], columns=[\"brand\", \"ad\", \"price\"])\nm = len(profiles)\n\n# Define true part-worth utilities\nb_util = {\"N\": 1.0, \"P\": 0.5, \"H\": 0.0}\na_util = {\"Yes\": -0.8, \"No\": 0.0}\ndef p_util(p): return -0.1 * p\n\n# Parameters\nn_peeps = 100\nn_tasks = 10\nn_alts = 3\n\n# Function to simulate one respondent\ndef sim_one(id):\n    all_tasks = []\n\n    for t in range(1, n_tasks + 1):\n        # Randomly sample 3 alternatives\n        sampled = profiles.sample(n=n_alts, replace=False).copy()\n        sampled[\"resp\"] = id\n        sampled[\"task\"] = t\n\n        # Deterministic utility\n        sampled[\"v\"] = (\n            sampled[\"brand\"].map(b_util) +\n            sampled[\"ad\"].map(a_util) +\n            sampled[\"price\"].apply(p_util)\n        ).round(10)\n\n        # Gumbel noise (Type I extreme value)\n        eps = -np.log(-np.log(np.random.uniform(size=n_alts)))\n        sampled[\"e\"] = eps\n        sampled[\"u\"] = sampled[\"v\"] + sampled[\"e\"]\n\n        # Choice indicator\n        sampled[\"choice\"] = (sampled[\"u\"] == sampled[\"u\"].max()).astype(int)\n\n        all_tasks.append(sampled)\n\n    return pd.concat(all_tasks, ignore_index=True)\n\n# Simulate all respondents\nconjoint_data = pd.concat([sim_one(i) for i in range(1, n_peeps + 1)], ignore_index=True)\n\n# Keep only observable columns\nconjoint_data = conjoint_data[[\"resp\", \"task\", \"brand\", \"ad\", \"price\", \"choice\"]]\n\n# Preview\nprint(conjoint_data.head())"
  },
  {
    "objectID": "blog/Project3/hw3_questions.html#preparing-the-data-for-estimation",
    "href": "blog/Project3/hw3_questions.html#preparing-the-data-for-estimation",
    "title": "Multinomial Logit Model",
    "section": "3. Preparing the Data for Estimation",
    "text": "3. Preparing the Data for Estimation\nThe “hard part” of the MNL likelihood function is organizing the data, as we need to keep track of 3 dimensions (consumer \\(i\\), covariate \\(k\\), and product \\(j\\)) instead of the typical 2 dimensions for cross-sectional regression models (consumer \\(i\\) and covariate \\(k\\)). The fact that each task for each respondent has the same number of alternatives (3) helps. In addition, we need to convert the categorical variables for brand and ads into binary variables.\nTo prepare the data for estimating a Multinomial Logit (MNL) model, we followed these key steps:\n\nStep 1: Convert Categorical Variables to Dummy Variables\nWe transformed the categorical variables brand and ad into binary (dummy) variables. This is necessary because the MNL model requires all covariates to be numerical.\n\nFor the brand variable, which includes values “N” (Netflix), “P” (Prime), and “H” (Hulu), we created two dummy variables:\n\nbrand_N = 1 if the brand is Netflix, 0 otherwise\n\nbrand_P = 1 if the brand is Prime, 0 otherwise\nHulu is treated as the reference category and therefore not included as a dummy.\n\nFor the ad variable, which indicates whether the plan includes advertisements, we created:\n\nad_Yes = 1 if ads are present, 0 otherwise\nAd-free (No) is treated as the reference category.\n\n\n\n\nStep 2: Drop Original Categorical Columns\nAfter creating dummy variables, we dropped the original brand and ad columns to avoid redundancy.\n\n\nStep 3: Verify Data Structure\nThe dataset is already in “long” format, where each row represents one product alternative within a choice task for a specific respondent. This format is appropriate for estimating an MNL model.\n\n\nFinal Columns\nAfter processing, each row in the dataset includes:\n\nresp: respondent ID\n\ntask: task number\n\nchoice: binary indicator (1 if the option was chosen, 0 otherwise)\n\nprice: monthly price of the streaming option\n\nbrand_N, brand_P: brand dummies\n\nad_Yes: ad dummy\n\nThis processed dataset is now ready for model estimation using either a statistical package (e.g., statsmodels.MNLogit) or a custom likelihood function.\nThe following code provides the simulation of the conjoint data.\n\n\n\n\n\n\nNote\n\n\n\n\n\nimport pandas as pd\nimport numpy as np\n\ndf = pd.read_csv('conjoint_data.csv')\nbrand_dummies = pd.get_dummies(df['brand'], prefix='brand')\nbrand_dummies.drop('brand_H', axis=1, inplace=True) \nad_dummies = pd.get_dummies(df['ad'], prefix='ad')\nad_dummies.drop('ad_No', axis=1, inplace=True)\ndf_processed = pd.concat([df, brand_dummies, ad_dummies], axis=1)\ndf_processed.drop(columns=['brand', 'ad'], inplace=True)\ndf_processed.head()"
  },
  {
    "objectID": "blog/Project3/hw3_questions.html#estimation-via-maximum-likelihood",
    "href": "blog/Project3/hw3_questions.html#estimation-via-maximum-likelihood",
    "title": "Multinomial Logit Model",
    "section": "4. Estimation via Maximum Likelihood",
    "text": "4. Estimation via Maximum Likelihood\nWe estimate the parameters of the Multinomial Logit (MNL) model using maximum likelihood estimation (MLE). The model includes four explanatory variables:\n\nbrand_N: Indicator for Netflix\nbrand_P: Indicator for Amazon Prime (Hulu is the reference category)\nad_Yes: Indicator for whether the plan includes advertisements (ad-free is the reference)\nprice: Monthly subscription price in dollars\n\n\nLog-Likelihood Function\nWe define the individual utility of alternative ( j ) for consumer ( i ) as:\n\\(U_{ij} = x_{ij}' \\beta + \\varepsilon_{ij}\\)\nUnder the assumption that ( _{ij} ) follows an i.i.d. Type I Extreme Value distribution, the choice probability is:\n\\(P_{ij} = \\frac{e^{x_{ij}'\\beta}}{\\sum_{k \\in J} e^{x_{ik}'\\beta}}\\)\nThe log-likelihood for all choices is:\n\\(\\ell(\\beta) = \\sum_{i=1}^n \\sum_{j \\in J} y_{ij} \\log(P_{ij})\\)\nWe implemented this function in Python and used scipy.optimize.minimize() with the BFGS method to find the maximum likelihood estimates (MLEs).\n\n\nResults\nThe following table reports the parameter estimates, standard errors, and 95% confidence intervals:\n\n\n\nParameter\nEstimate\nStd. Error\n95% CI Lower\n95% CI Upper\n\n\n\n\nbrand_N\n0.9412\n0.1173\n0.7113\n1.1711\n\n\nbrand_P\n0.5016\n0.1213\n0.2638\n0.7394\n\n\nad_Yes\n-0.7320\n0.0887\n-0.9059\n-0.5581\n\n\nprice\n-0.0995\n0.0063\n-0.1119\n-0.0870\n\n\n\n\n\nInterpretation\n\nConsumers prefer Netflix and Amazon Prime over Hulu (baseline), with Netflix having the strongest positive effect on utility.\nThe presence of ads significantly decreases the utility of an alternative.\nPrice has a negative coefficient, as expected—higher prices reduce utility and choice probability.\n\nThese results are consistent with rational consumer behavior and the true data-generating process used in the simulation.\nThe following code provides the simulation of the conjoint data.\n\n\n\n\n\n\nNote\n\n\n\n\n\nfrom scipy.optimize import minimize\nimport numpy as np\nimport pandas as pd\nimport math\nvector_exp = np.vectorize(math.exp)\nX_cols = ['brand_N', 'brand_P', 'ad_Yes', 'price']\nX = df_processed[X_cols].to_numpy()\ny = df_processed['choice'].values\ntasks = df_processed.groupby(['resp', 'task']).ngroup().values\nn_tasks = len(np.unique(tasks))\n\ndef make_neg_log_likelihood(X, y, tasks, n_tasks):\n    def neg_log_likelihood(beta):\n        beta = np.asarray(beta)  \n        utilities = X @ beta\n        exp_util = vector_exp(utilities)\n\n        task_sums = np.zeros(n_tasks)\n        for t in range(n_tasks):\n            task_sums[t] = np.sum(exp_util[tasks == t])\n\n        prob = exp_util / task_sums[tasks]  \n        log_likelihood = np.sum(y * np.log(prob + 1e-12))\n        return -log_likelihood\n    return neg_log_likelihood\n\nneg_ll = make_neg_log_likelihood(X, y, tasks, n_tasks)\ninit_params = np.zeros(X.shape[1])\nresult = minimize(neg_ll, init_params, method='BFGS')\nbeta_hat = result.x\nhessian = result.hess_inv\nse = np.sqrt(np.diag(hessian))\n\nz = 1.96\nconf_int = np.vstack([\n    beta_hat - z * se,\n    beta_hat + z * se\n]).T\n\nprint(\"Parameter Estimates and 95% Confidence Intervals:\")\nfor name, b, s, (low, high) in zip(X_cols, beta_hat, se, conf_int):\n    print(f\"{name:&gt;10}: {b:.4f} (SE={s:.4f})  95% CI: [{low:.4f}, {high:.4f}]\")"
  },
  {
    "objectID": "blog/Project3/hw3_questions.html#estimation-via-bayesian-methods",
    "href": "blog/Project3/hw3_questions.html#estimation-via-bayesian-methods",
    "title": "Multinomial Logit Model",
    "section": "5. Estimation via Bayesian Methods",
    "text": "5. Estimation via Bayesian Methods\nIn addition to maximum likelihood estimation, we conducted a Bayesian analysis of the Multinomial Logit (MNL) model using the Metropolis-Hastings MCMC algorithm. This approach allows us to obtain the full posterior distributions of the model parameters, rather than just point estimates.\n\nModel Parameters\nWe estimated four parameters corresponding to:\n\nbrand_N: Indicator for Netflix (vs. Hulu as baseline)\nbrand_P: Indicator for Amazon Prime\nad_Yes: Indicator for advertisements present (vs. ad-free)\nprice: Monthly subscription price\n\n\n\nPriors\nWe placed weakly informative normal priors on the parameters:\n\n\\(\\beta_{\\text{brand}} \\sim \\mathcal{N}(0, 5)\\)\n\\(\\beta_{\\text{ads}} \\sim \\mathcal{N}(0, 5)\\)\n\\(\\beta_{\\text{price}} \\sim \\mathcal{N}(0, 1)\\)\n\n\n\nPosterior and Likelihood\nRather than computing the full posterior density directly, we worked in log-space for numerical stability:\n\\[\n\\log p(\\beta \\mid \\text{data}) = \\log \\text{likelihood}(\\beta) + \\log \\text{prior}(\\beta)\n\\]\nWe reused the log-likelihood function from the MLE section and added the log-prior.\n\n\nProposal Distribution\nWe used a multivariate normal proposal distribution with independent dimensions, corresponding to:\n\n\\(\\mathcal{N}(0, 0.05)\\) for the three binary covariates\n\\(\\mathcal{N}(0, 0.005)\\) for the price coefficient\n\nThis ensures smaller step sizes for the price dimension, which typically has smaller scale variation.\n\n\nSampling Process\nWe ran the Metropolis-Hastings algorithm for 11,000 iterations, discarding the first 1,000 as burn-in and retaining the last 10,000 samples for inference.\nDue to compatibility issues with np.exp, we used a vectorized version of math.exp to safely compute exponentials during likelihood evaluation.\n\n\nResults\nThe table below shows the posterior means, standard deviations, and 95% credible intervals for each parameter:\n\n\n\nParameter\nMean\nSD\n2.5%\n97.5%\n\n\n\n\nbrand_N\n0.946\n0.112\n0.732\n1.159\n\n\nbrand_P\n0.503\n0.110\n0.298\n0.721\n\n\nad_Yes\n-0.735\n0.088\n-0.914\n-0.568\n\n\nprice\n-0.100\n0.006\n-0.112\n-0.087\n\n\n\n\n\nInterpretation\n\nNetflix and Prime are both preferred over Hulu, with Netflix having a stronger positive effect.\nPresence of advertisements has a strong negative effect on utility.\nPrice negatively impacts utility, with a tight credible interval around the true value.\n\nThese posterior estimates closely match the true part-worths used in the data simulation and validate the model’s ability to recover meaningful consumer preferences.\n\n\n\n\n\n\nNote\n\n\n\n\n\nimport math\nvector_exp = np.vectorize(math.exp)\nX_cols = ['brand_N', 'brand_P', 'ad_Yes', 'price']\nX = df_processed[X_cols].to_numpy()\ny = df_processed['choice'].values\ntasks = df_processed.groupby(['resp', 'task']).ngroup().values\nn_tasks = len(np.unique(tasks))\n\ndef log_likelihood(beta):\n    beta = np.asarray(beta)\n    utilities = X @ beta\n    exp_util = vector_exp(utilities)\n    task_sum = np.bincount(tasks, weights=exp_util)\n    prob = exp_util / task_sum[tasks]\n    return np.sum(y * np.log(prob + 1e-12))\n\ndef log_prior(beta):\n    b1, b2, b3, b4 = beta\n    prior1 = -0.5 * (b1 ** 2) / 5\n    prior2 = -0.5 * (b2 ** 2) / 5\n    prior3 = -0.5 * (b3 ** 2) / 5\n    prior4 = -0.5 * (b4 ** 2) / 1\n    return prior1 + prior2 + prior3 + prior4\n\n# log-posterior = log-likelihood + log-prior\ndef log_posterior(beta):\n    return log_likelihood(beta) + log_prior(beta)\n\nn_iter = 11000\nburn_in = 1000\nsamples = np.zeros((n_iter, 4))\nbeta_current = np.zeros(4)\nlog_post_current = log_posterior(beta_current)\n\nproposal_std = np.array([0.05, 0.05, 0.05, 0.005])\nfor i in range(n_iter):\n    proposal = beta_current + np.random.normal(0, proposal_std)\n    log_post_proposal = log_posterior(proposal)\n    log_accept_ratio = log_post_proposal - log_post_current\n    if np.log(np.random.rand()) &lt; log_accept_ratio:\n        beta_current = proposal\n        log_post_current = log_post_proposal\n    samples[i] = beta_current\n\nposterior_samples = samples[burn_in:]\nposterior_summary = pd.DataFrame({\n    'mean': posterior_samples.mean(axis=0),\n    'sd': posterior_samples.std(axis=0),\n    '2.5%': np.percentile(posterior_samples, 2.5, axis=0),\n    '97.5%': np.percentile(posterior_samples, 97.5, axis=0)\n}, index=X_cols)\n\nprint(\"Posterior Summary from MCMC:\")\nprint(posterior_summary)\n\n\n\nTo assess the convergence and quality of the posterior samples obtained from our Metropolis-Hastings MCMC algorithm, we visually inspected the results for the price coefficient, denoted as \\(\\beta_{\\text{price}}\\).\n\n\nTrace Plot\nThe trace plot below displays the sampled values of \\(\\beta_{\\text{price}}\\) across 10,000 post–burn-in iterations:\n\n\n\nTrace Plot for β_price\n\n\nThe trace appears to mix well and explore the parameter space without strong trends or sticking points. This suggests that the Markov chain has reached its stationary distribution and provides valid posterior samples for inference.\n\n\n\nPosterior Distribution\nThe posterior distribution for \\(\\beta_{\\text{price}}\\) is shown in the histogram and kernel density plot below:\n\n\n\nPosterior Distribution for β_price\n\n\nThe distribution is approximately Gaussian, centered near -0.10, and relatively narrow, indicating high certainty around the estimate. This posterior matches the true data-generating value and the MLE estimate quite closely.\n\n\n\nPosterior Summary vs. MLE Comparison\n\n\n\n\n\n\n\n\n\n\nParameter\nMLE Estimate\nMLE CI (95%)\nMCMC Mean\nMCMC CI (95%)\n\n\n\n\nbrand_N\n0.9412\n[0.7113, 1.1711]\n0.9459\n[0.7320, 1.1589]\n\n\nbrand_P\n0.5016\n[0.2638, 0.7394]\n0.5028\n[0.2980, 0.7210]\n\n\nad_Yes\n-0.7320\n[-0.9059, -0.5581]\n-0.7351\n[-0.9141, -0.5678]\n\n\nprice\n-0.0995\n[-0.1119, -0.0870]\n-0.0996\n[-0.1122, -0.0874]\n\n\n\n\n\n\nInterpretation\n\nAgreement: The MCMC posterior means closely match the MLE point estimates, and the credible intervals are also highly consistent with the MLE confidence intervals.\nUncertainty: Posterior standard deviations and intervals confirm that the price coefficient is estimated with high precision.\nValidation: These results support the conclusion that both the frequentist and Bayesian approaches recover the underlying preference structure in the simulated data well.\n\nThis analysis confirms that our MCMC sampler is functioning correctly and provides a robust Bayesian inference framework for discrete choice models.\n\n\n\n\n\n\nNote\n\n\n\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nbeta_price_samples = posterior_samples[:, 3]\n\nsns.set(style=\"whitegrid\", context=\"notebook\")\n\n# 🎨 Trace plot\nplt.figure(figsize=(10, 4))\nplt.plot(beta_price_samples, color=\"steelblue\", alpha=0.8)\nplt.title(\"Trace Plot for β_price\", fontsize=14)\nplt.xlabel(\"Iteration\", fontsize=12)\nplt.ylabel(\"Value\", fontsize=12)\nplt.tight_layout()\nplt.show()\n\n# 🎨 Histogram + KDE\nplt.figure(figsize=(8, 4))\nsns.histplot(beta_price_samples, bins=40, kde=True, color=\"skyblue\")\nplt.title(\"Posterior Distribution for β_price\", fontsize=14)\nplt.xlabel(\"Value\", fontsize=12)\nplt.ylabel(\"Density\", fontsize=12)\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "blog/Project3/hw3_questions.html#discussion",
    "href": "blog/Project3/hw3_questions.html#discussion",
    "title": "Multinomial Logit Model",
    "section": "6. Discussion",
    "text": "6. Discussion\nSuppose we had not simulated the data and were instead analyzing real-world responses. What could we infer from the parameter estimates alone?\n\nRelative Brand Preferences: The estimated parameters indicate that: \\[\n\\beta_{\\text{Netflix}} &gt; \\beta_{\\text{Prime}} &gt; \\beta_{\\text{Hulu}} = 0\n\\] This tells us that, on average, respondents prefer Netflix over Amazon Prime, and both are preferred over Hulu (which was used as the baseline). A higher \\(\\beta\\) value means a brand contributes more to the utility and is more likely to be chosen, all else equal.\nNegative Price Coefficient: The estimate for \\(\\beta_{\\text{price}}\\) is negative, which makes intuitive sense in consumer behavior. A higher monthly subscription price decreases the overall utility of a streaming plan and therefore reduces the likelihood of being chosen. This aligns with economic theory and real-world expectations.\nInterpretability Without Simulation: Even if we didn’t know the “true” values of the parameters (as we do in simulation), these signs and relative magnitudes allow us to interpret meaningful consumer preferences from real choice data.\n\n\nIn real-world conjoint analysis, we often observe heterogeneity across respondents — different people value price and features differently. A single set of fixed parameters (()) may fail to capture this variation.\nTo address this, we move from a fixed-parameter MNL model to a hierarchical (multi-level) model, also known as a random-parameters logit model.\n\nChanges Required to Simulate Hierarchical Data\nTo simulate data from such a model, instead of assigning the same \\(\\beta\\) vector to every respondent, we assume: \\[\n\\beta_i \\sim \\mathcal{N}(\\mu, \\Sigma)\n\\] Where: - \\(\\beta_i\\) is the preference vector for individual \\(i\\) - \\(\\mu\\) is the population mean - \\(\\Sigma\\) is the covariance matrix capturing between-individual variance\nEach respondent draws their own (_i), which is then used to generate their choices.\n\n\nEstimating Hierarchical Models\nTo estimate this model, we must:\n\nUse Hierarchical Bayesian methods such as Gibbs sampling or Hamiltonian Monte Carlo (e.g., via Stan)\nOr use frequentist methods like simulated maximum likelihood with random draws from ((, )) (e.g., mixed logit)\n\nThese approaches allow us to recover individual-level preference distributions, which are more realistic for personalized targeting and segmentation."
  },
  {
    "objectID": "blog/Poisson Regression Examples/hw2_questions.html",
    "href": "blog/Poisson Regression Examples/hw2_questions.html",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\nThis dataset contains information on 1,500 mature engineering firms and is intended to help evaluate whether the use of Blueprinty software is associated with a higher number of awarded patents. The dataset includes four columns:\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\npatents\nNumber of patents awarded to the firm in the past five years (integer).\n\n\nregion\nThe geographic region where the firm is located (categorical).\n\n\nage\nThe age of the firm since incorporation, in years (float).\n\n\niscustomer\nBinary indicator of whether the firm uses Blueprinty software (0 = no, 1 = yes).\n\n\n\n\n\n\n\nTotal observations: 1,500 firms\n\nNo missing values\nData types:\n\npatents: int64\n\nregion: object (categorical)\n\nage: float64\n\niscustomer: int64 (binary)\n\n\nTo explore whether firms using Blueprinty’s software tend to have more patents, we compared the distribution of patent counts between customers and non-customers.\nThe histograms below show the number of patents awarded to firms, segmented by whether or not they are Blueprinty customers (iscustomer = 1 or 0):\n\n\n\nHistogram by iscustomer\n\n\nVisually, we observe that Blueprinty customers tend to have slightly more patents than non-customers. The distribution for customers is shifted to the right and shows a longer tail, indicating a higher frequency of firms with larger patent counts.\n\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nblueprinty = pd.read_csv('blueprinty.csv')\ng = sns.displot(\n    data=blueprinty,\n    x=\"patents\",\n    col=\"iscustomer\",  \n    kind=\"hist\",\n    bins=30,\n    kde=True,\n    height=4,\n    aspect=1.5,\n    facet_kws={'margin_titles': True}\n)\n\ng.set_axis_labels(\"Number of Patents\", \"Count\")\ng.set_titles(col_template=\"Customer: {col_name}\")\nplt.subplots_adjust(top=0.8)\nfor ax in g.axes.flatten():\n    ax.tick_params(labelbottom=True)\ng.fig.suptitle(\"Patent Distribution by Customer Status\")\nTo confirm this pattern numerically, we calculate the average number of patents for each group:\npatent_mean_bycustomer = blueprinty.groupby('iscustomer')['patents'].mean()\npatent_mean_bycustomer\nThe results show:\n\nNon-customers: 3.47 patents on average\nCustomers: 4.13 patents on average\n\nThis difference supports the notion that customer firms, on average, are more productive in securing patents.\nWhile this descriptive analysis does not establish a causal link, the pattern suggests a positive association between using Blueprinty and higher patent output.\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\nFrom the regional distribution, we observe that Blueprinty customers are most concentrated in the Northeast, where their count even exceeds that of non-customers. In contrast, in regions such as the Midwest, Northwest, and South, non-customer firms are significantly more numerous. This suggests that Blueprinty may have a stronger market presence or outreach in the Northeast region.\n\n\n\nRegion Distribution\n\n\nIn terms of firm age, Blueprinty customers tend to be younger. The majority of customer firms fall into the ≤20 and 21–30 age groups, while very few are in the 41–50 or &gt;50 categories. Although non-customer firms also peak in the 21–30 group, they are more evenly distributed across older age groups compared to customers.\n\n\n\nAge Distribution\n\n\nThese patterns imply that Blueprinty’s user base skews younger and more regionally concentrated, which could influence other observed differences, such as patent counts, and should be taken into account when interpreting any causal claims.\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\nThe likelihood function for a set of independent observations \\(Y_1\\), \\(Y_2\\), \\(\\dots\\), \\(Y_n\\), each following a Poisson distribution with mean , is given by:\n\\[\nL(\\lambda) = \\prod_{i=1}^{n} \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n\\]\nTaking the logarithm of the likelihood function gives the log-likelihood, which is more convenient to work with mathematically:\n\\[\n\\log L(\\lambda) = \\sum_{i=1}^{n} \\left( -\\lambda + Y_i \\log \\lambda - \\log Y_i! \\right)\n\\]\n\n\n\nimport numpy as np\nfrom scipy.special import gammaln \n\ndef poisson_loglikelihood(lam, Y):\n    if lam &lt;= 0:\n        return -np.inf\n    \n    Y = np.asarray(Y)\n    log_likelihood = np.sum(-lam + Y * np.log(lam) - gammaln(Y + 1))\n    return log_likelihood\n\nY = blueprinty['patents']\n\nlambda_values = np.linspace(0.1, 10, 200)\nloglikelihood_values = [poisson_loglikelihood(lam, Y) for lam in lambda_values]\n\nplt.figure(figsize=(8, 5))\nplt.plot(lambda_values, loglikelihood_values)\nplt.xlabel(\"λ (Lambda)\")\nplt.ylabel(\"Log-Likelihood\")\nplt.title(\"Poisson Log-Likelihood as a Function of λ\")\nplt.grid(True)\nplt.show()\n\n\n\nPoisson Model\n\n\nThe shape of the curve illustrates how the log-likelihood sharply decreases as \\(\\lambda\\) moves away from the mean in either direction. This provides intuition for why the mean is the most “likely” value under a Poisson process.\nTo find the maximum likelihood estimator (MLE) for \\(\\lambda\\) in a Poisson model, we start with the log-likelihood function:\n\\[\n\\log L(\\lambda) = \\sum_{i=1}^{n} \\left( -\\lambda + Y_i \\log \\lambda - \\log Y_i! \\right)\n\\]\nTaking the derivative of the log-likelihood with respect to \\(\\lambda\\):\n\\[\n\\frac{d}{d\\lambda} \\log L(\\lambda) = \\sum_{i=1}^{n} \\left( -1 + \\frac{Y_i}{\\lambda} \\right)\n\\]\nSetting the derivative equal to zero to find the maximum:\n\\[\n\\sum_{i=1}^{n} \\left( -1 + \\frac{Y_i}{\\lambda} \\right) = 0\n\\]\n\\[\n- n + \\frac{1}{\\lambda} \\sum_{i=1}^{n} Y_i = 0\n\\]\n\\[\n\\lambda = \\frac{1}{n} \\sum_{i=1}^{n} Y_i = \\bar{Y}\n\\]\nThe MLE of \\(\\lambda\\) is the sample mean of the observed values:\n\\[\n\\hat{\\lambda}_{\\text{MLE}} = \\bar{Y}\n\\]\nThis result makes intuitive sense because, in a Poisson distribution, the mean and the variance are both equal to \\(\\lambda\\). Therefore, using the sample mean as an estimate of \\(\\lambda\\) is not only mathematically valid, but also conceptually natural.\nfrom scipy.optimize import minimize_scalar\n\nobjective = lambda lam: -poisson_loglikelihood(lam, blueprinty['patents'])\n\nresult = minimize_scalar(objective, bounds=(0.1, 10), method='bounded')\nlambda_mle = result.x\nprint(\"Estimated λ (MLE):\", lambda_mle)\nTo obtain the maximum likelihood estimate (MLE) of \\(\\lambda\\) for the Poisson model, we used numerical optimization to maximize the log-likelihood function based on the observed number of patents.\nUsing Python’s scipy.optimize.minimize_scalar, we minimized the negative log-likelihood over a plausible range of \\(\\lambda\\) values:\n\nOptimization interval: \\(\\lambda\\) in [0.1, 10]\nOptimization method: ‘bounded’\n\nThe result of this optimization gives:\n\\[\n\\hat{\\lambda}_{\\text{MLE}} \\approx 3.685\n\\]\nThis estimate aligns closely with the sample mean of the patent counts, which is consistent with the theoretical property of the Poisson distribution: its mean equals the MLE for \\(\\lambda\\). This numerical confirmation reinforces the earlier analytical result that \\(\\hat{\\lambda} = \\bar{Y}\\).\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\nWe estimate a Poisson regression model to study how firm characteristics relate to the number of patents awarded. The dependent variable is the number of patents a firm received over the last five years. The key covariates include:\n\nage: Years since firm incorporation\n\nage_squared: To allow for non-linear age effects\n\nregion: Dummy-coded categorical variable for firm location\n\niscustomer: A binary indicator of whether the firm uses Blueprinty’s software\n\nWe model:\n\\[\nY_i \\sim \\text{Poisson}(\\lambda_i), \\quad \\lambda_i = \\exp(X_i' \\beta)\n\\]\nThis model assumes that each firm’s patent count \\(Y_i\\) follows a Poisson distribution with a rate \\(\\lambda_i\\) that depends on firm-level covariates.\nWe construct the design matrix ( X ) with an intercept and the variables described above. To estimate the model, we define a log-likelihood function and use scipy.optimize.minimize() to maximize it:\nfrom scipy.optimize import minimize\nfrom scipy.special import gammaln\ndef poisson_regression_loglikelihood(beta, Y, X):\n    eta = X @ beta\n    eta = np.clip(eta, -100, 100)\n    lambda_i = np.exp(eta)\n    log_lik = np.sum(-lambda_i + Y * eta - gammaln(Y + 1))\n    return log_lik\n\n# Build design matrix\nblueprinty['age_squared'] = blueprinty['age'] ** 2\nX = pd.concat([\n    pd.Series(1.0, index=blueprinty.index, name='intercept'),\n    blueprinty[['age', 'age_squared', 'iscustomer']],\n    pd.get_dummies(blueprinty['region'], drop_first=True)\n], axis=1)\nX_matrix = X.astype(float).values\nY = blueprinty['patents'].values\n\n# Maximize log-likelihood\ninit_beta = np.zeros(X_matrix.shape[1])\nresult = minimize(\n    fun=lambda beta: -poisson_regression_loglikelihood(beta, Y, X_matrix),\n    x0=init_beta,\n    method='BFGS'\n)\n\n# Extract estimates\nbeta_hat = result.x\nhessian_inv = result.hess_inv\nstandard_errors = np.sqrt(np.diag(hessian_inv))\nsummary_table = pd.DataFrame({\n    'Variable': X.columns,\n    'Coefficient (β)': beta_hat,\n    'Std. Error': standard_errors\n})\nsummary_table\nThe table below summarizes the coefficient estimates and standard errors obtained from the manually implemented Poisson regression using maximum likelihood estimation (MLE):\n\n\n\nVariable\nCoefficient (β)\nStd. Error\n\n\n\n\nintercept\n-0.510\n0.194\n\n\nage\n0.149\n0.015\n\n\nage_squared\n-0.003\n0.00029\n\n\niscustomer\n0.208\n0.029\n\n\nNortheast\n0.030\n0.033\n\n\nNorthwest\n-0.018\n0.025\n\n\nSouth\n0.057\n0.025\n\n\nSouthwest\n0.051\n0.039\n\n\n\n\nIntercept (−0.51): This represents the expected log-count of patents for a firm with age = 0 and all binary indicators equal to zero (i.e., not a customer and in the baseline region—likely Midwest). While not directly interpretable, it serves as the model’s baseline offset.\nAge (0.149) and Age Squared (−0.003): The combination of a positive linear age term and a small negative quadratic term suggests an inverted-U shaped relationship between firm age and patent output. Patent counts increase with age but at a decreasing rate, eventually leveling off or declining.\nIs Customer (0.208): Being a Blueprinty customer is associated with higher patent productivity. Since this is a log-linear model, we interpret the effect as:\n\\[\n\\exp(0.208) \\approx 1.23\n\\]\nThat is, Blueprinty customers have approximately 23% more expected patents than non-customers, controlling for other covariates.\nRegional Effects: Relative to the omitted region (likely Midwest), the region dummies show:\n\nNortheast: Slightly higher expected patent counts, though not strongly significant.\nNorthwest: Slightly lower than the baseline.\nSouth and Southwest: Positive coefficients, but again relatively modest in size.\n\n\n\nTo check results with sm.GLM() function:\nimport statsmodels.api as sm\n\nmodel = sm.GLM(Y, X, family=sm.families.Poisson())\nresult = model.fit()\n\nprint(result.summary())\nThe Poisson regression using statsmodels.GLM() confirms several key findings:\n\nAge has a positive and significant coefficient (0.1486), while age_squared is negative (−0.0030), indicating an inverted U-shaped relationship between firm age and patent output.\niscustomer (0.2076) is positive and statistically significant (p &lt; 0.001), suggesting that Blueprinty customers tend to receive more patents than non-customers, controlling for age and region.\nRegional dummy variables show small and mostly non-significant differences compared to the omitted region (likely Midwest).\n\nBecause this is a log-linear model, we interpret the iscustomer effect as:\n\\[\n\\exp(0.2076) \\approx 1.23\n\\]\nThis means Blueprinty customers are expected to have about 23% more patents than non-customers, holding other factors constant.\nOverall, the model captures important effects of age, customer status, and location on firm patent output. These results reinforce the idea that using Blueprinty’s software is associated with increased patenting success, particularly among firms of moderate age.\nTo estimate the average treatment effect of using Blueprinty’s software, we simulate patent predictions under two scenarios:\n\nAll firms are non-customers (iscustomer = 0)\nAll firms are customers (iscustomer = 1)\n\nWe use the fitted Poisson model to predict patent counts in both cases and compare the results.\nX_0 = X.copy()\nX_1 = X.copy()\nX_0['iscustomer'] = 0\nX_1['iscustomer'] = 1\n\ny_pred_0 = result.predict(X_0)\ny_pred_1 = result.predict(X_1)\n\naverage_effect = np.mean(y_pred_1 - y_pred_0)\nprint(\"Average expected increase in patents:\", average_effect)\nThe average firm is expected to receive approximately 0.79 more patents over five years if it adopts Blueprinty’s software, compared to if it does not—holding all else constant."
  },
  {
    "objectID": "blog/Poisson Regression Examples/hw2_questions.html#blueprinty-case-study",
    "href": "blog/Poisson Regression Examples/hw2_questions.html#blueprinty-case-study",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\nThis dataset contains information on 1,500 mature engineering firms and is intended to help evaluate whether the use of Blueprinty software is associated with a higher number of awarded patents. The dataset includes four columns:\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\npatents\nNumber of patents awarded to the firm in the past five years (integer).\n\n\nregion\nThe geographic region where the firm is located (categorical).\n\n\nage\nThe age of the firm since incorporation, in years (float).\n\n\niscustomer\nBinary indicator of whether the firm uses Blueprinty software (0 = no, 1 = yes).\n\n\n\n\n\n\n\nTotal observations: 1,500 firms\n\nNo missing values\nData types:\n\npatents: int64\n\nregion: object (categorical)\n\nage: float64\n\niscustomer: int64 (binary)\n\n\nTo explore whether firms using Blueprinty’s software tend to have more patents, we compared the distribution of patent counts between customers and non-customers.\nThe histograms below show the number of patents awarded to firms, segmented by whether or not they are Blueprinty customers (iscustomer = 1 or 0):\n\n\n\nHistogram by iscustomer\n\n\nVisually, we observe that Blueprinty customers tend to have slightly more patents than non-customers. The distribution for customers is shifted to the right and shows a longer tail, indicating a higher frequency of firms with larger patent counts.\n\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nblueprinty = pd.read_csv('blueprinty.csv')\ng = sns.displot(\n    data=blueprinty,\n    x=\"patents\",\n    col=\"iscustomer\",  \n    kind=\"hist\",\n    bins=30,\n    kde=True,\n    height=4,\n    aspect=1.5,\n    facet_kws={'margin_titles': True}\n)\n\ng.set_axis_labels(\"Number of Patents\", \"Count\")\ng.set_titles(col_template=\"Customer: {col_name}\")\nplt.subplots_adjust(top=0.8)\nfor ax in g.axes.flatten():\n    ax.tick_params(labelbottom=True)\ng.fig.suptitle(\"Patent Distribution by Customer Status\")\nTo confirm this pattern numerically, we calculate the average number of patents for each group:\npatent_mean_bycustomer = blueprinty.groupby('iscustomer')['patents'].mean()\npatent_mean_bycustomer\nThe results show:\n\nNon-customers: 3.47 patents on average\nCustomers: 4.13 patents on average\n\nThis difference supports the notion that customer firms, on average, are more productive in securing patents.\nWhile this descriptive analysis does not establish a causal link, the pattern suggests a positive association between using Blueprinty and higher patent output.\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\nFrom the regional distribution, we observe that Blueprinty customers are most concentrated in the Northeast, where their count even exceeds that of non-customers. In contrast, in regions such as the Midwest, Northwest, and South, non-customer firms are significantly more numerous. This suggests that Blueprinty may have a stronger market presence or outreach in the Northeast region.\n\n\n\nRegion Distribution\n\n\nIn terms of firm age, Blueprinty customers tend to be younger. The majority of customer firms fall into the ≤20 and 21–30 age groups, while very few are in the 41–50 or &gt;50 categories. Although non-customer firms also peak in the 21–30 group, they are more evenly distributed across older age groups compared to customers.\n\n\n\nAge Distribution\n\n\nThese patterns imply that Blueprinty’s user base skews younger and more regionally concentrated, which could influence other observed differences, such as patent counts, and should be taken into account when interpreting any causal claims.\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\nThe likelihood function for a set of independent observations \\(Y_1\\), \\(Y_2\\), \\(\\dots\\), \\(Y_n\\), each following a Poisson distribution with mean , is given by:\n\\[\nL(\\lambda) = \\prod_{i=1}^{n} \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n\\]\nTaking the logarithm of the likelihood function gives the log-likelihood, which is more convenient to work with mathematically:\n\\[\n\\log L(\\lambda) = \\sum_{i=1}^{n} \\left( -\\lambda + Y_i \\log \\lambda - \\log Y_i! \\right)\n\\]\n\n\n\nimport numpy as np\nfrom scipy.special import gammaln \n\ndef poisson_loglikelihood(lam, Y):\n    if lam &lt;= 0:\n        return -np.inf\n    \n    Y = np.asarray(Y)\n    log_likelihood = np.sum(-lam + Y * np.log(lam) - gammaln(Y + 1))\n    return log_likelihood\n\nY = blueprinty['patents']\n\nlambda_values = np.linspace(0.1, 10, 200)\nloglikelihood_values = [poisson_loglikelihood(lam, Y) for lam in lambda_values]\n\nplt.figure(figsize=(8, 5))\nplt.plot(lambda_values, loglikelihood_values)\nplt.xlabel(\"λ (Lambda)\")\nplt.ylabel(\"Log-Likelihood\")\nplt.title(\"Poisson Log-Likelihood as a Function of λ\")\nplt.grid(True)\nplt.show()\n\n\n\nPoisson Model\n\n\nThe shape of the curve illustrates how the log-likelihood sharply decreases as \\(\\lambda\\) moves away from the mean in either direction. This provides intuition for why the mean is the most “likely” value under a Poisson process.\nTo find the maximum likelihood estimator (MLE) for \\(\\lambda\\) in a Poisson model, we start with the log-likelihood function:\n\\[\n\\log L(\\lambda) = \\sum_{i=1}^{n} \\left( -\\lambda + Y_i \\log \\lambda - \\log Y_i! \\right)\n\\]\nTaking the derivative of the log-likelihood with respect to \\(\\lambda\\):\n\\[\n\\frac{d}{d\\lambda} \\log L(\\lambda) = \\sum_{i=1}^{n} \\left( -1 + \\frac{Y_i}{\\lambda} \\right)\n\\]\nSetting the derivative equal to zero to find the maximum:\n\\[\n\\sum_{i=1}^{n} \\left( -1 + \\frac{Y_i}{\\lambda} \\right) = 0\n\\]\n\\[\n- n + \\frac{1}{\\lambda} \\sum_{i=1}^{n} Y_i = 0\n\\]\n\\[\n\\lambda = \\frac{1}{n} \\sum_{i=1}^{n} Y_i = \\bar{Y}\n\\]\nThe MLE of \\(\\lambda\\) is the sample mean of the observed values:\n\\[\n\\hat{\\lambda}_{\\text{MLE}} = \\bar{Y}\n\\]\nThis result makes intuitive sense because, in a Poisson distribution, the mean and the variance are both equal to \\(\\lambda\\). Therefore, using the sample mean as an estimate of \\(\\lambda\\) is not only mathematically valid, but also conceptually natural.\nfrom scipy.optimize import minimize_scalar\n\nobjective = lambda lam: -poisson_loglikelihood(lam, blueprinty['patents'])\n\nresult = minimize_scalar(objective, bounds=(0.1, 10), method='bounded')\nlambda_mle = result.x\nprint(\"Estimated λ (MLE):\", lambda_mle)\nTo obtain the maximum likelihood estimate (MLE) of \\(\\lambda\\) for the Poisson model, we used numerical optimization to maximize the log-likelihood function based on the observed number of patents.\nUsing Python’s scipy.optimize.minimize_scalar, we minimized the negative log-likelihood over a plausible range of \\(\\lambda\\) values:\n\nOptimization interval: \\(\\lambda\\) in [0.1, 10]\nOptimization method: ‘bounded’\n\nThe result of this optimization gives:\n\\[\n\\hat{\\lambda}_{\\text{MLE}} \\approx 3.685\n\\]\nThis estimate aligns closely with the sample mean of the patent counts, which is consistent with the theoretical property of the Poisson distribution: its mean equals the MLE for \\(\\lambda\\). This numerical confirmation reinforces the earlier analytical result that \\(\\hat{\\lambda} = \\bar{Y}\\).\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\nWe estimate a Poisson regression model to study how firm characteristics relate to the number of patents awarded. The dependent variable is the number of patents a firm received over the last five years. The key covariates include:\n\nage: Years since firm incorporation\n\nage_squared: To allow for non-linear age effects\n\nregion: Dummy-coded categorical variable for firm location\n\niscustomer: A binary indicator of whether the firm uses Blueprinty’s software\n\nWe model:\n\\[\nY_i \\sim \\text{Poisson}(\\lambda_i), \\quad \\lambda_i = \\exp(X_i' \\beta)\n\\]\nThis model assumes that each firm’s patent count \\(Y_i\\) follows a Poisson distribution with a rate \\(\\lambda_i\\) that depends on firm-level covariates.\nWe construct the design matrix ( X ) with an intercept and the variables described above. To estimate the model, we define a log-likelihood function and use scipy.optimize.minimize() to maximize it:\nfrom scipy.optimize import minimize\nfrom scipy.special import gammaln\ndef poisson_regression_loglikelihood(beta, Y, X):\n    eta = X @ beta\n    eta = np.clip(eta, -100, 100)\n    lambda_i = np.exp(eta)\n    log_lik = np.sum(-lambda_i + Y * eta - gammaln(Y + 1))\n    return log_lik\n\n# Build design matrix\nblueprinty['age_squared'] = blueprinty['age'] ** 2\nX = pd.concat([\n    pd.Series(1.0, index=blueprinty.index, name='intercept'),\n    blueprinty[['age', 'age_squared', 'iscustomer']],\n    pd.get_dummies(blueprinty['region'], drop_first=True)\n], axis=1)\nX_matrix = X.astype(float).values\nY = blueprinty['patents'].values\n\n# Maximize log-likelihood\ninit_beta = np.zeros(X_matrix.shape[1])\nresult = minimize(\n    fun=lambda beta: -poisson_regression_loglikelihood(beta, Y, X_matrix),\n    x0=init_beta,\n    method='BFGS'\n)\n\n# Extract estimates\nbeta_hat = result.x\nhessian_inv = result.hess_inv\nstandard_errors = np.sqrt(np.diag(hessian_inv))\nsummary_table = pd.DataFrame({\n    'Variable': X.columns,\n    'Coefficient (β)': beta_hat,\n    'Std. Error': standard_errors\n})\nsummary_table\nThe table below summarizes the coefficient estimates and standard errors obtained from the manually implemented Poisson regression using maximum likelihood estimation (MLE):\n\n\n\nVariable\nCoefficient (β)\nStd. Error\n\n\n\n\nintercept\n-0.510\n0.194\n\n\nage\n0.149\n0.015\n\n\nage_squared\n-0.003\n0.00029\n\n\niscustomer\n0.208\n0.029\n\n\nNortheast\n0.030\n0.033\n\n\nNorthwest\n-0.018\n0.025\n\n\nSouth\n0.057\n0.025\n\n\nSouthwest\n0.051\n0.039\n\n\n\n\nIntercept (−0.51): This represents the expected log-count of patents for a firm with age = 0 and all binary indicators equal to zero (i.e., not a customer and in the baseline region—likely Midwest). While not directly interpretable, it serves as the model’s baseline offset.\nAge (0.149) and Age Squared (−0.003): The combination of a positive linear age term and a small negative quadratic term suggests an inverted-U shaped relationship between firm age and patent output. Patent counts increase with age but at a decreasing rate, eventually leveling off or declining.\nIs Customer (0.208): Being a Blueprinty customer is associated with higher patent productivity. Since this is a log-linear model, we interpret the effect as:\n\\[\n\\exp(0.208) \\approx 1.23\n\\]\nThat is, Blueprinty customers have approximately 23% more expected patents than non-customers, controlling for other covariates.\nRegional Effects: Relative to the omitted region (likely Midwest), the region dummies show:\n\nNortheast: Slightly higher expected patent counts, though not strongly significant.\nNorthwest: Slightly lower than the baseline.\nSouth and Southwest: Positive coefficients, but again relatively modest in size.\n\n\n\nTo check results with sm.GLM() function:\nimport statsmodels.api as sm\n\nmodel = sm.GLM(Y, X, family=sm.families.Poisson())\nresult = model.fit()\n\nprint(result.summary())\nThe Poisson regression using statsmodels.GLM() confirms several key findings:\n\nAge has a positive and significant coefficient (0.1486), while age_squared is negative (−0.0030), indicating an inverted U-shaped relationship between firm age and patent output.\niscustomer (0.2076) is positive and statistically significant (p &lt; 0.001), suggesting that Blueprinty customers tend to receive more patents than non-customers, controlling for age and region.\nRegional dummy variables show small and mostly non-significant differences compared to the omitted region (likely Midwest).\n\nBecause this is a log-linear model, we interpret the iscustomer effect as:\n\\[\n\\exp(0.2076) \\approx 1.23\n\\]\nThis means Blueprinty customers are expected to have about 23% more patents than non-customers, holding other factors constant.\nOverall, the model captures important effects of age, customer status, and location on firm patent output. These results reinforce the idea that using Blueprinty’s software is associated with increased patenting success, particularly among firms of moderate age.\nTo estimate the average treatment effect of using Blueprinty’s software, we simulate patent predictions under two scenarios:\n\nAll firms are non-customers (iscustomer = 0)\nAll firms are customers (iscustomer = 1)\n\nWe use the fitted Poisson model to predict patent counts in both cases and compare the results.\nX_0 = X.copy()\nX_1 = X.copy()\nX_0['iscustomer'] = 0\nX_1['iscustomer'] = 1\n\ny_pred_0 = result.predict(X_0)\ny_pred_1 = result.predict(X_1)\n\naverage_effect = np.mean(y_pred_1 - y_pred_0)\nprint(\"Average expected increase in patents:\", average_effect)\nThe average firm is expected to receive approximately 0.79 more patents over five years if it adopts Blueprinty’s software, compared to if it does not—holding all else constant."
  },
  {
    "objectID": "blog/Poisson Regression Examples/hw2_questions.html#airbnb-case-study",
    "href": "blog/Poisson Regression Examples/hw2_questions.html#airbnb-case-study",
    "title": "Poisson Regression Examples",
    "section": "AirBnB Case Study",
    "text": "AirBnB Case Study\n\nIntroduction\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City. The data include the following variables:\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n- `id` = unique ID number for each unit\n- `last_scraped` = date when information scraped\n- `host_since` = date when host first listed the unit on Airbnb\n- `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n- `room_type` = Entire home/apt., Private room, or Shared room\n- `bathrooms` = number of bathrooms\n- `bedrooms` = number of bedrooms\n- `price` = price per night (dollars)\n- `number_of_reviews` = number of reviews for the unit on Airbnb\n- `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n- `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n- `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n- `instant_bookable` = \"t\" if instantly bookable, \"f\" if not\n\n\n\n\n\nExploratory Data Analysis\nWe begin by exploring the distribution of key variables in the Airbnb dataset, focusing on room_type, number_of_reviews, and price.\n\nThe distribution of room types shows that the dataset is dominated by two categories: Entire home/apt and Private room, each with approximately 19,000–20,000 listings. Shared room listings are comparatively rare. \nThe number of reviews is highly right-skewed, with most listings receiving fewer than 50 reviews. A small number of listings receive over 100 reviews. \nThe price distribution is also strongly skewed to the right. While most listings are priced below $500 per night, a few extreme values exceed $1,000, with some outliers above $5,000 or even $10,000. \n\n\n\nMissing Data Overview\nWe also inspect the dataset for missing values. As shown below, some key review-related variables contain substantial missing data:\n\nreview_scores_cleanliness has ~10,195 missing entries\nreview_scores_location has ~10,254 missing entries\nreview_scores_value has ~10,256 missing entries\nbathrooms and bedrooms have 160 and 76 missing respectively\nOther important predictors such as price, room_type, and number_of_reviews have no missing values\n\nWe will consider dropping rows with missing values on relevant columns. We consider number of reviews a good proxy as the number of bookings, so next we will find relationships between reviews number and other conditions.\n\n\nPoisson Regression Results: Interpreting Coefficients\nWe fit a Poisson regression model to predict the number of reviews (as a proxy for number of bookings). The independent variables include price, room type, review cleanliness score, days since listed, and whether the listing is instantly bookable.\nBelow is the interpretation of each coefficient from the GLM output:\n\nIntercept (2.5122)\nThis is the baseline log-expected number of reviews when all numeric variables are 0 and categorical variables are at their reference level (e.g., room type = Entire home/apt, instant_bookable = False).\nExponentiating gives \\(e^{2.51} ≈ 12.3\\): the baseline expected number of reviews is around 12.\nPrice (−3.595e−05)\nA very small but significant negative coefficient.\nThis suggests that for each additional dollar in price, the expected number of reviews decreases slightly.\nAlthough small in magnitude, it’s statistically significant (p &lt; 0.001).\nReview Scores Cleanliness (+0.0455)\nThis is positive and highly significant.\nFor each 1-point increase in the cleanliness score, the expected number of reviews increases by approximately 4.7%: \\(\\exp(0.0455) ≈ 1.047\\)\nDays Listed (+5.085e−05)\nAlso positive and very significant.\nListings that have been active longer receive more reviews, which is expected.\nEach additional day listed increases expected reviews by ~0.005%.\nRoom Type: Private Room (−0.0192)\nCompared to Entire home/apt, private rooms have fewer reviews (−1.9%).\nThis effect is small but significant.\nRoom Type: Shared Room (−0.2450)\nShared rooms receive far fewer reviews than entire apartments.\n\\(\\exp(−0.245) ≈ 0.782\\)：about 21.8% fewer reviews than the baseline.\nInstant Bookable: True (+0.3574)\nThis variable has a strong positive effect.\n\\(\\exp(0.357) ≈ 1.43\\)：listings that allow instant booking receive about 43% more reviews than those that don’t.\n\ncols = ['number_of_reviews', 'price', 'room_type', 'review_scores_cleanliness', 'instant_bookable', 'days']\n\ndf = airbnb[cols].dropna()\ndf = pd.get_dummies(df, columns=['room_type', 'instant_bookable'], drop_first=True)\n\nX = df.drop(columns=['number_of_reviews'])\nX = sm.add_constant(X)\nX = X.astype(float)\ny = df['number_of_reviews']\n\npoisson_model = sm.GLM(y, X, family=sm.families.Poisson())\nresult = poisson_model.fit()\nprint(result.summary())\n\n\n\nSummary\n\nCleanliness, longer listing duration, and instant bookability are strong positive predictors of booking activity (as measured by number of reviews).\nHigher prices and shared rooms are associated with fewer reviews.\nAll predictors are statistically significant with p-values &lt; 0.001.\n\nThese results provide insight into what types of listings attract more bookings and help hosts optimize for more engagement."
  },
  {
    "objectID": "homework2.html",
    "href": "homework2.html",
    "title": "Cheng-Yuan Wu's Website",
    "section": "",
    "text": "Show Code\nimport pandas as pd\nimport numpy as np\n\n\n\n\nShow Code\nairbnb = pd.read_csv('airbnb.csv')\nblueprinty = pd.read_csv('blueprinty.csv')\n\n\n\n\nShow Code\nairbnb.head()\n\n\n\n\n\n\n\n\n\nUnnamed: 0\nid\ndays\nlast_scraped\nhost_since\nroom_type\nbathrooms\nbedrooms\nprice\nnumber_of_reviews\nreview_scores_cleanliness\nreview_scores_location\nreview_scores_value\ninstant_bookable\n\n\n\n\n0\n1\n2515\n3130\n4/2/2017\n9/6/2008\nPrivate room\n1.0\n1.0\n59\n150\n9.0\n9.0\n9.0\nf\n\n\n1\n2\n2595\n3127\n4/2/2017\n9/9/2008\nEntire home/apt\n1.0\n0.0\n230\n20\n9.0\n10.0\n9.0\nf\n\n\n2\n3\n3647\n3050\n4/2/2017\n11/25/2008\nPrivate room\n1.0\n1.0\n150\n0\nNaN\nNaN\nNaN\nf\n\n\n3\n4\n3831\n3038\n4/2/2017\n12/7/2008\nEntire home/apt\n1.0\n1.0\n89\n116\n9.0\n9.0\n9.0\nf\n\n\n4\n5\n4611\n3012\n4/2/2017\n1/2/2009\nPrivate room\nNaN\n1.0\n39\n93\n9.0\n8.0\n9.0\nt\n\n\n\n\n\n\n\n\n\nShow Code\nblueprinty.head()\n\n\n\n\n\n\n\n\n\npatents\nregion\nage\niscustomer\n\n\n\n\n0\n0\nMidwest\n32.5\n0\n\n\n1\n3\nSouthwest\n37.5\n0\n\n\n2\n4\nNorthwest\n27.0\n1\n\n\n3\n3\nNortheast\n24.5\n0\n\n\n4\n3\nSouthwest\n37.0\n0\n\n\n\n\n\n\n\n\n\nShow Code\nblueprinty.info()\nblueprinty\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1500 entries, 0 to 1499\nData columns (total 4 columns):\n #   Column      Non-Null Count  Dtype  \n---  ------      --------------  -----  \n 0   patents     1500 non-null   int64  \n 1   region      1500 non-null   object \n 2   age         1500 non-null   float64\n 3   iscustomer  1500 non-null   int64  \ndtypes: float64(1), int64(2), object(1)\nmemory usage: 47.0+ KB\n\n\n\n\n\n\n\n\n\npatents\nregion\nage\niscustomer\n\n\n\n\n0\n0\nMidwest\n32.5\n0\n\n\n1\n3\nSouthwest\n37.5\n0\n\n\n2\n4\nNorthwest\n27.0\n1\n\n\n3\n3\nNortheast\n24.5\n0\n\n\n4\n3\nSouthwest\n37.0\n0\n\n\n...\n...\n...\n...\n...\n\n\n1495\n2\nNortheast\n18.5\n1\n\n\n1496\n3\nSouthwest\n22.5\n0\n\n\n1497\n4\nSouthwest\n17.0\n0\n\n\n1498\n3\nSouth\n29.0\n0\n\n\n1499\n1\nSouth\n39.0\n0\n\n\n\n\n1500 rows × 4 columns\n\n\n\n\n\nShow Code\nairbnb.info()\nairbnb.describe()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 5 entries, 0 to 4\nData columns (total 14 columns):\n #   Column                     Non-Null Count  Dtype  \n---  ------                     --------------  -----  \n 0   Unnamed: 0                 5 non-null      int64  \n 1   id                         5 non-null      int64  \n 2   days                       5 non-null      int64  \n 3   last_scraped               5 non-null      object \n 4   host_since                 5 non-null      object \n 5   room_type                  5 non-null      object \n 6   bathrooms                  4 non-null      float64\n 7   bedrooms                   5 non-null      float64\n 8   price                      5 non-null      int64  \n 9   number_of_reviews          5 non-null      int64  \n 10  review_scores_cleanliness  4 non-null      float64\n 11  review_scores_location     4 non-null      float64\n 12  review_scores_value        4 non-null      float64\n 13  instant_bookable           5 non-null      object \ndtypes: float64(5), int64(5), object(4)\nmemory usage: 688.0+ bytes\n\n\n\n\n\n\n\n\n\nUnnamed: 0\nid\ndays\nbathrooms\nbedrooms\nprice\nnumber_of_reviews\nreview_scores_cleanliness\nreview_scores_location\nreview_scores_value\n\n\n\n\ncount\n5.000000\n5.000000\n5.000000\n4.0\n5.000000\n5.000000\n5.000000\n4.0\n4.000000\n4.0\n\n\nmean\n3.000000\n3439.800000\n3071.400000\n1.0\n0.800000\n113.400000\n75.800000\n9.0\n9.000000\n9.0\n\n\nstd\n1.581139\n885.525381\n53.914748\n0.0\n0.447214\n77.500323\n63.790281\n0.0\n0.816497\n0.0\n\n\nmin\n1.000000\n2515.000000\n3012.000000\n1.0\n0.000000\n39.000000\n0.000000\n9.0\n8.000000\n9.0\n\n\n25%\n2.000000\n2595.000000\n3038.000000\n1.0\n1.000000\n59.000000\n20.000000\n9.0\n8.750000\n9.0\n\n\n50%\n3.000000\n3647.000000\n3050.000000\n1.0\n1.000000\n89.000000\n93.000000\n9.0\n9.000000\n9.0\n\n\n75%\n4.000000\n3831.000000\n3127.000000\n1.0\n1.000000\n150.000000\n116.000000\n9.0\n9.250000\n9.0\n\n\nmax\n5.000000\n4611.000000\n3130.000000\n1.0\n1.000000\n230.000000\n150.000000\n9.0\n10.000000\n9.0\n\n\n\n\n\n\n\n\n\nShow Code\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.figure(figsize=(10, 6))\nplt.title('Patent Distribution')\nsns.histplot(blueprinty['patents'], bins=50, kde=True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nShow Code\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# 設定年齡分組（你可以依需要調整區間）\nblueprinty['age_group'] = pd.cut(blueprinty['age'], bins=[0, 20, 30, 40, 50, 100], \n                                 labels=['≤20', '21–30', '31–40', '41–50', '&gt;50'])\n\n# 畫出直方圖，按「是否為客戶」分 Facet，並用不同年齡分組顯示\ng = sns.displot(\n    data=blueprinty,\n    x=\"patents\",\n    col=\"region\",     \n    row=\"age_group\",         # 年齡分組\n    kind=\"hist\",\n    bins=30,\n    kde=True,\n    height=4,\n    aspect=1.5,\n    facet_kws={'margin_titles': True}\n)\n\ng.set_axis_labels(\"Number of Patents\", \"Count\")\ng.set_titles(col_template=\"Customer: {col_name}\")\ng.set_titles(col_template=\"Region: {col_name}\", row_template=\"Age Group: {row_name}\")\nplt.subplots_adjust(top=0.9)\nfor ax in g.axes.flatten():\n    ax.tick_params(labelbottom=True)\ng.fig.suptitle(\"Patent Distribution by Age Group and Region\", fontsize=16)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nShow Code\npatent_mean_bycustomer = blueprinty.groupby('iscustomer')['patents'].mean()\npatent_mean_bycustomer\n\n\niscustomer\n0    3.473013\n1    4.133056\nName: patents, dtype: float64\n\n\n\n\nShow Code\npatent_mean_byage = blueprinty.groupby('age_group')['patents'].mean()\npatent_mean_byage\n\n\n/var/folders/4_/b_ln9nlx3j19x_2nmv9t3rd40000gn/T/ipykernel_89268/1344393124.py:1: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  patent_mean_byage = blueprinty.groupby('age_group')['patents'].mean()\n\n\nage_group\n≤20      3.454277\n21–30    4.156695\n31–40    3.230769\n41–50    2.186047\n&gt;50           NaN\nName: patents, dtype: float64\n\n\n\n\nShow Code\n# 設定年齡分組（你可以依需要調整區間）\nblueprinty['age_group'] = pd.cut(blueprinty['age'], bins=[0, 20, 30, 40, 50, 100], \n                                 labels=['≤20', '21–30', '31–40', '41–50', '&gt;50'])\n\n# 畫出直方圖，按「是否為客戶」分 Facet，並用不同年齡分組顯示\ng = sns.displot(\n    data=blueprinty,\n    x=\"patents\",\n    col=\"iscustomer\",        # 是否為客戶：0 or 1\n    # row=\"age_group\",         # 年齡分組\n    kind=\"hist\",\n    bins=30,\n    kde=True,\n    height=4,\n    aspect=1.5,\n    facet_kws={'margin_titles': True}\n)\n\ng.set_axis_labels(\"Number of Patents\", \"Count\")\ng.set_titles(col_template=\"Customer: {col_name}\")\n# g.set_titles(col_template=\"Customer: {col_name}\", row_template=\"Age Group: {row_name}\")\nplt.subplots_adjust(top=0.8)\nfor ax in g.axes.flatten():\n    ax.tick_params(labelbottom=True)\ng.fig.suptitle(\"Patent Distribution by Customer Status\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nShow Code\nregion_counts = blueprinty.groupby(['iscustomer', 'region']).size().unstack()\nregion_counts\n\n\n\n\n\n\n\n\nregion\nMidwest\nNortheast\nNorthwest\nSouth\nSouthwest\n\n\niscustomer\n\n\n\n\n\n\n\n\n\n0\n187\n273\n158\n156\n245\n\n\n1\n37\n328\n29\n35\n52\n\n\n\n\n\n\n\n\n\nShow Code\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# 區域分布條形圖\nsns.countplot(data=blueprinty, x='region', hue='iscustomer',palette={0: \"lightgray\", 1: \"skyblue\"})\nplt.title('Region Distribution by Customer Status')\nplt.xticks(rotation=45)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nShow Code\nsns.countplot(data=blueprinty, x='age_group', hue='iscustomer',palette={0: \"lightgray\", 1: \"skyblue\"})\nplt.title('Age Distribution by Customer Status')\nplt.xticks(rotation=45)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nShow Code\nimport numpy as np\nfrom scipy.special import gammaln  # 更穩定地處理 log(Y!)\n\ndef poisson_loglikelihood(lam, Y):\n    \"\"\"\n    計算 Poisson 模型的 log-likelihood。\n    \n    參數:\n    lam: float, Poisson 分布的 λ 參數（事件的平均發生率）\n    Y: array-like, 一組觀察到的計數資料\n    \n    回傳:\n    總對數概似值（log-likelihood）\n    \"\"\"\n    if lam &lt;= 0:\n        return -np.inf  # λ 必須為正數，否則回傳負無限\n    \n    Y = np.asarray(Y)\n    log_likelihood = np.sum(-lam + Y * np.log(lam) - gammaln(Y + 1))\n    return log_likelihood\n\n\n\n\nShow Code\nY = blueprinty['patents']\n\nlambda_values = np.linspace(0.1, 10, 200)\nloglikelihood_values = [poisson_loglikelihood(lam, Y) for lam in lambda_values]\n\nplt.figure(figsize=(8, 5))\nplt.plot(lambda_values, loglikelihood_values)\nplt.xlabel(\"λ (Lambda)\")\nplt.ylabel(\"Log-Likelihood\")\nplt.title(\"Poisson Log-Likelihood as a Function of λ\")\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nShow Code\nfrom scipy.optimize import minimize_scalar\n\n# 目標函數要最大化，所以取 -loglikelihood 來做最小化\nobjective = lambda lam: -poisson_loglikelihood(lam, blueprinty['patents'])\n\nresult = minimize_scalar(objective, bounds=(0.1, 10), method='bounded')\nlambda_mle = result.x\nprint(\"Estimated λ (MLE):\", lambda_mle)\n\n\nEstimated λ (MLE): 3.6846662261327716\n\n\n\n\nShow Code\nimport numpy as np\nfrom scipy.special import gammaln\n\ndef poisson_regression_loglikelihood(beta, Y, X):\n    \"\"\"\n    計算 Poisson 回歸模型的 log-likelihood。\n    \n    參數：\n    - beta: 係數向量 (一維 array)，長度為 X 的欄數\n    - Y: 觀察值向量（每家公司有幾個專利）\n    - X: 共變量矩陣（每列代表一家公司的特徵）\n\n    回傳：\n    - 總對數概似值（log-likelihood）\n    \"\"\"\n    beta = np.asarray(beta)\n    Y = np.asarray(Y)\n    X = np.asarray(X)\n\n    # 計算每家公司自己的 lambda_i = exp(X_i · beta)\n    lambda_i = np.exp(X @ beta)\n\n    # 套用 Poisson log-likelihood 函數\n    log_lik = np.sum(-lambda_i + Y * np.log(lambda_i) - gammaln(Y + 1))\n\n    return log_lik\n\n\n\n\nShow Code\nfrom scipy.optimize import minimize\n\n# 建立 age_squared 欄位\nblueprinty['age_squared'] = blueprinty['age'] ** 2\n\n# 建立 region dummy variables（drop_first=True 是為了避免 multicollinearity）\nregion_dummies = pd.get_dummies(blueprinty['region'], drop_first=True)\n\n# 建立設計矩陣 X\nX = pd.concat([\n    pd.Series(1, index=blueprinty.index, name='intercept'),\n    blueprinty[['age', 'age_squared', 'iscustomer']],\n    region_dummies\n], axis=1)\nX = X.astype(float)\n# 目標變數 Y\nY = blueprinty['patents'].values\nX_matrix = X.values\n\n\n\n\nShow Code\nX_matrix\n\n\narray([[1.00000e+00, 3.25000e+01, 1.05625e+03, ..., 0.00000e+00,\n        0.00000e+00, 0.00000e+00],\n       [1.00000e+00, 3.75000e+01, 1.40625e+03, ..., 0.00000e+00,\n        0.00000e+00, 1.00000e+00],\n       [1.00000e+00, 2.70000e+01, 7.29000e+02, ..., 1.00000e+00,\n        0.00000e+00, 0.00000e+00],\n       ...,\n       [1.00000e+00, 1.70000e+01, 2.89000e+02, ..., 0.00000e+00,\n        0.00000e+00, 1.00000e+00],\n       [1.00000e+00, 2.90000e+01, 8.41000e+02, ..., 0.00000e+00,\n        1.00000e+00, 0.00000e+00],\n       [1.00000e+00, 3.90000e+01, 1.52100e+03, ..., 0.00000e+00,\n        1.00000e+00, 0.00000e+00]])\n\n\n\n\nShow Code\n# 初始值設定為 0 向量\ninit_beta = np.zeros(X_matrix.shape[1])\n\n# 改用明確傳參方式\nresult = minimize(\n    fun=lambda beta: -poisson_regression_loglikelihood(beta, Y, X_matrix),\n    x0=init_beta,\n    method='BFGS'\n)\n# 取出估計值與標準誤\nbeta_hat = result.x\nhessian_inv = result.hess_inv  # Hessian 反矩陣（共變異數矩陣）\nstandard_errors = np.sqrt(np.diag(hessian_inv))\n\n\n/var/folders/4_/b_ln9nlx3j19x_2nmv9t3rd40000gn/T/ipykernel_89268/3261295856.py:21: RuntimeWarning: overflow encountered in exp\n  lambda_i = np.exp(X @ beta)\n/var/folders/4_/b_ln9nlx3j19x_2nmv9t3rd40000gn/T/ipykernel_89268/3261295856.py:24: RuntimeWarning: invalid value encountered in multiply\n  log_lik = np.sum(-lambda_i + Y * np.log(lambda_i) - gammaln(Y + 1))\n/var/folders/4_/b_ln9nlx3j19x_2nmv9t3rd40000gn/T/ipykernel_89268/3261295856.py:24: RuntimeWarning: invalid value encountered in add\n  log_lik = np.sum(-lambda_i + Y * np.log(lambda_i) - gammaln(Y + 1))\n/Users/murphy_aloe/Library/Python/3.9/lib/python/site-packages/numpy/core/fromnumeric.py:88: RuntimeWarning: overflow encountered in reduce\n  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n/var/folders/4_/b_ln9nlx3j19x_2nmv9t3rd40000gn/T/ipykernel_89268/3261295856.py:21: RuntimeWarning: overflow encountered in exp\n  lambda_i = np.exp(X @ beta)\n/var/folders/4_/b_ln9nlx3j19x_2nmv9t3rd40000gn/T/ipykernel_89268/3261295856.py:24: RuntimeWarning: invalid value encountered in multiply\n  log_lik = np.sum(-lambda_i + Y * np.log(lambda_i) - gammaln(Y + 1))\n/var/folders/4_/b_ln9nlx3j19x_2nmv9t3rd40000gn/T/ipykernel_89268/3261295856.py:24: RuntimeWarning: invalid value encountered in add\n  log_lik = np.sum(-lambda_i + Y * np.log(lambda_i) - gammaln(Y + 1))\n/Users/murphy_aloe/Library/Python/3.9/lib/python/site-packages/numpy/core/fromnumeric.py:88: RuntimeWarning: overflow encountered in reduce\n  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n/var/folders/4_/b_ln9nlx3j19x_2nmv9t3rd40000gn/T/ipykernel_89268/3261295856.py:21: RuntimeWarning: overflow encountered in exp\n  lambda_i = np.exp(X @ beta)\n/var/folders/4_/b_ln9nlx3j19x_2nmv9t3rd40000gn/T/ipykernel_89268/3261295856.py:24: RuntimeWarning: invalid value encountered in multiply\n  log_lik = np.sum(-lambda_i + Y * np.log(lambda_i) - gammaln(Y + 1))\n/var/folders/4_/b_ln9nlx3j19x_2nmv9t3rd40000gn/T/ipykernel_89268/3261295856.py:24: RuntimeWarning: invalid value encountered in add\n  log_lik = np.sum(-lambda_i + Y * np.log(lambda_i) - gammaln(Y + 1))\n\n\n\n\nShow Code\n# 整理係數與標準誤\nsummary_table = pd.DataFrame({\n    'Variable': X.columns,\n    'Coefficient (β)': beta_hat,\n    'Std. Error': standard_errors\n})\nprint(summary_table)\n\n\n      Variable  Coefficient (β)  Std. Error\n0    intercept         1.480059         1.0\n1          age        38.016417         1.0\n2  age_squared      1033.539585         1.0\n3   iscustomer         0.553874         1.0\n4    Northeast         0.640979         1.0\n5    Northwest         0.164288         1.0\n6        South         0.181562         1.0\n7    Southwest         0.295497         1.0\n\n\n\n\nShow Code\n# Build design matrix\ndef poisson_regression_loglikelihood(beta, Y, X):\n    eta = X @ beta\n    eta = np.clip(eta, -100, 100)\n    lambda_i = np.exp(eta)\n\n    # 使用 where 保底防止 log(0)；同時限制 lambda_i 不為無限大\n    log_lik = np.sum(-lambda_i + Y * eta - gammaln(Y + 1))\n    return log_lik\n\nblueprinty['age_squared'] = blueprinty['age'] ** 2\nX = pd.concat([\n    pd.Series(1.0, index=blueprinty.index, name='intercept'),\n    blueprinty[['age', 'age_squared', 'iscustomer']],\n    pd.get_dummies(blueprinty['region'], drop_first=True)\n], axis=1)\nX_matrix = X.astype(float).values\nY = blueprinty['patents'].values\n\n# Maximize log-likelihood\ninit_beta = np.zeros(X_matrix.shape[1])\nresult = minimize(\n    fun=lambda beta: -poisson_regression_loglikelihood(beta, Y, X_matrix),\n    x0=init_beta,\n    method='BFGS'\n)\n\n# Extract estimates\nbeta_hat = result.x\nhessian_inv = result.hess_inv\nstandard_errors = np.sqrt(np.diag(hessian_inv))\nsummary_table = pd.DataFrame({\n    'Variable': X.columns,\n    'Coefficient (β)': beta_hat,\n    'Std. Error': standard_errors\n})\nsummary_table\n\n\n\n\n\n\n\n\n\nVariable\nCoefficient (β)\nStd. Error\n\n\n\n\n0\nintercept\n-0.509992\n0.193971\n\n\n1\nage\n0.148706\n0.015329\n\n\n2\nage_squared\n-0.002972\n0.000291\n\n\n3\niscustomer\n0.207609\n0.028616\n\n\n4\nNortheast\n0.029155\n0.032640\n\n\n5\nNorthwest\n-0.017578\n0.025004\n\n\n6\nSouth\n0.056565\n0.025004\n\n\n7\nSouthwest\n0.050567\n0.038745\n\n\n\n\n\n\n\n\n\nShow Code\nimport statsmodels.api as sm\n\nmodel = sm.GLM(Y, X, family=sm.families.Poisson())\nresult = model.fit()\n\n# 顯示完整結果\nprint(result.summary())\n\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:                      y   No. Observations:                 1500\nModel:                            GLM   Df Residuals:                     1492\nModel Family:                 Poisson   Df Model:                            7\nLink Function:                    Log   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -3258.1\nDate:                Fri, 02 May 2025   Deviance:                       2143.3\nTime:                        04:07:41   Pearson chi2:                 2.07e+03\nNo. Iterations:                     5   Pseudo R-squ. (CS):             0.1360\nCovariance Type:            nonrobust                                         \n===============================================================================\n                  coef    std err          z      P&gt;|z|      [0.025      0.975]\n-------------------------------------------------------------------------------\nintercept      -0.5089      0.183     -2.778      0.005      -0.868      -0.150\nage             0.1486      0.014     10.716      0.000       0.121       0.176\nage_squared    -0.0030      0.000    -11.513      0.000      -0.003      -0.002\niscustomer      0.2076      0.031      6.719      0.000       0.147       0.268\nNortheast       0.0292      0.044      0.669      0.504      -0.056       0.115\nNorthwest      -0.0176      0.054     -0.327      0.744      -0.123       0.088\nSouth           0.0566      0.053      1.074      0.283      -0.047       0.160\nSouthwest       0.0506      0.047      1.072      0.284      -0.042       0.143\n===============================================================================\n\n\n\n\nShow Code\nX.columns\n\n\nIndex(['intercept', 'age', 'age_squared', 'iscustomer', 'Northeast',\n       'Northwest', 'South', 'Southwest'],\n      dtype='object')\n\n\n\n\nShow Code\nX_0 = X.copy()\nX_1 = X.copy()\nX_0['iscustomer'] = 0\nX_1['iscustomer'] = 1\n\ny_pred_0 = result.predict(X_0)\ny_pred_1 = result.predict(X_1)\n\naverage_effect = np.mean(y_pred_1 - y_pred_0)\nprint(\"Average expected increase in patents:\", average_effect)\n\n\nAverage expected increase in patents: 0.7927680710452626\n\n\n\n\nShow Code\nairbnb.head()\n\n\n\n\n\n\n\n\n\nUnnamed: 0\nid\ndays\nlast_scraped\nhost_since\nroom_type\nbathrooms\nbedrooms\nprice\nnumber_of_reviews\nreview_scores_cleanliness\nreview_scores_location\nreview_scores_value\ninstant_bookable\n\n\n\n\n0\n1\n2515\n3130\n4/2/2017\n9/6/2008\nPrivate room\n1.0\n1.0\n59\n150\n9.0\n9.0\n9.0\nf\n\n\n1\n2\n2595\n3127\n4/2/2017\n9/9/2008\nEntire home/apt\n1.0\n0.0\n230\n20\n9.0\n10.0\n9.0\nf\n\n\n2\n3\n3647\n3050\n4/2/2017\n11/25/2008\nPrivate room\n1.0\n1.0\n150\n0\nNaN\nNaN\nNaN\nf\n\n\n3\n4\n3831\n3038\n4/2/2017\n12/7/2008\nEntire home/apt\n1.0\n1.0\n89\n116\n9.0\n9.0\n9.0\nf\n\n\n4\n5\n4611\n3012\n4/2/2017\n1/2/2009\nPrivate room\nNaN\n1.0\n39\n93\n9.0\n8.0\n9.0\nt"
  }
]